{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"RNN_Captioning.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"tags":["pdf-title"],"id":"9wOCLT-N-Idy","colab_type":"text"},"source":["# Image Captioning with RNNs\n","In this exercise you will implement a vanilla recurrent neural networks and use them it to train a model that can generate novel captions for images."]},{"cell_type":"code","metadata":{"id":"c6BVfDao-e6U","colab_type":"code","outputId":"b124b82c-1273-437a-c15e-8418e7572c42","executionInfo":{"status":"ok","timestamp":1579609068250,"user_tz":-540,"elapsed":16844,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Du8Ev4l_-ldd","colab_type":"code","outputId":"d0c09ea4-119f-407a-8f3f-93ae9ca410a3","executionInfo":{"status":"ok","timestamp":1579609071265,"user_tz":-540,"elapsed":1959,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/drive/My Drive/Colab Notebooks/cs231n/assignment3"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/cs231n/assignment3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xn-XN0P3-zYs","colab_type":"code","outputId":"39dc432f-8b23-4462-e69e-cd69c48776ac","executionInfo":{"status":"ok","timestamp":1579609177413,"user_tz":-540,"elapsed":98912,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["pip install -r requirements.txt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting absl-py==0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/6b/ba04a9fe6aefa56adafa6b9e0557b959e423c49950527139cb8651b0480b/absl-py-0.2.0.tar.gz (82kB)\n","\u001b[K     |████████████████████████████████| 92kB 3.4MB/s \n","\u001b[?25hCollecting aiocoap==0.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/f6/d839e4b14258d76e74a39810829c13f8dd31de2bfe0915579b2a609d1bbe/aiocoap-0.3.tar.gz (78kB)\n","\u001b[K     |████████████████████████████████| 81kB 13.1MB/s \n","\u001b[?25hCollecting aiohttp==3.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/fb/33dd1b634c1aa837afce8aa259a83d6b294581442d727f13a3baf39f608c/aiohttp-3.2.0-cp36-cp36m-manylinux1_x86_64.whl (760kB)\n","\u001b[K     |████████████████████████████████| 768kB 62.3MB/s \n","\u001b[?25hCollecting appnope==0.1.0\n","  Downloading https://files.pythonhosted.org/packages/87/a9/7985e6a53402f294c8f0e8eff3151a83f1fb901fa92909bb3ff29b4d22af/appnope-0.1.0-py2.py3-none-any.whl\n","Collecting astor==0.6.2\n","  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\n","Collecting async-timeout==3.0.0\n","  Downloading https://files.pythonhosted.org/packages/96/0f/e6357458c87fb4ed8f3df215773f3caad40968f10e05552cbd8bd28415e4/async_timeout-3.0.0-py3-none-any.whl\n","Collecting attrs==18.1.0\n","  Downloading https://files.pythonhosted.org/packages/41/59/cedf87e91ed541be7957c501a92102f9cc6363c623a7666d69d51c78ac5b/attrs-18.1.0-py2.py3-none-any.whl\n","Collecting backports-abc==0.4\n","  Downloading https://files.pythonhosted.org/packages/f5/5e/57e1afdc63d8c37496b2f6d9cb0ddfc4a3d55c949074debeab7594c19b54/backports_abc-0.4-py2.py3-none-any.whl\n","Collecting backports.ssl-match-hostname==3.5.0.1\n","  Downloading https://files.pythonhosted.org/packages/76/21/2dc61178a2038a5cb35d14b61467c6ac632791ed05131dda72c20e7b9e23/backports.ssl_match_hostname-3.5.0.1.tar.gz\n","Collecting bleach==1.5.0\n","  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n","Collecting certifi==2015.11.20.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/21/86903664789d010c7693523aa44cd6f96f9d60c7bc813761ff3db5fa8aad/certifi-2015.11.20.1-py2.py3-none-any.whl (368kB)\n","\u001b[K     |████████████████████████████████| 378kB 66.3MB/s \n","\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (3.0.4)\n","Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (0.10.0)\n","Collecting Cython==0.25.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/b5/cdc587086ee51e073da0a794984da29f1a8418d93728c249f0caa85094ea/Cython-0.25.2-cp36-cp36m-manylinux1_x86_64.whl (7.3MB)\n","\u001b[K     |████████████████████████████████| 7.3MB 46.2MB/s \n","\u001b[?25hCollecting decorator==4.0.6\n","  Downloading https://files.pythonhosted.org/packages/33/de/f87b10da4cebbea3f33386cc73e495a2a4a2ddfafaae8d58ac80b4d9494b/decorator-4.0.6-py2.py3-none-any.whl\n","Requirement already satisfied: future==0.16.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (0.16.0)\n","Collecting gast==0.2.0\n","  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n","Collecting gnureadline==6.3.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/ee/2c3f568b0a74974791ac590ec742ef6133e2fbd287a074ba72a53fa5e97c/gnureadline-6.3.3.tar.gz (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 49.6MB/s \n","\u001b[?25hCollecting grpcio==1.11.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/b8/00e703183b7ae5e02f161dafacdfa8edbd7234cb7434aef00f126a3a511e/grpcio-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (8.8MB)\n","\u001b[K     |████████████████████████████████| 8.8MB 51.7MB/s \n","\u001b[?25hCollecting h5py==2.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/f9/a449c76cab5d310fc0f7cf56ccb7d531b8abe21dd6395312a5f9e9c330ac/h5py-2.7.0-cp36-cp36m-manylinux1_x86_64.whl (4.8MB)\n","\u001b[K     |████████████████████████████████| 4.8MB 50.7MB/s \n","\u001b[?25hCollecting html5lib==0.9999999\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n","\u001b[K     |████████████████████████████████| 890kB 57.1MB/s \n","\u001b[?25hCollecting idna==2.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/cc/6dd9a3869f15c2edfab863b992838277279ce92663d334df9ecf5106f5c6/idna-2.6-py2.py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n","\u001b[?25hCollecting idna-ssl==1.0.1\n","  Downloading https://files.pythonhosted.org/packages/c4/3b/facf5a5009e577e7764e68a2af5ee25c63f41c78277260c2c42b8cfabf2e/idna-ssl-1.0.1.tar.gz\n","Collecting ipykernel==4.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/87/8800c72ff09e6c8792f33217f3dbedee393fd08c8aaa4a0f04dcac4bf47a/ipykernel-4.2.2-py2.py3-none-any.whl (91kB)\n","\u001b[K     |████████████████████████████████| 92kB 14.6MB/s \n","\u001b[?25hCollecting ipython==4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/73/258505e845c8370a1636f815aa68bc596a7a1871731484ed4e8d5d207709/ipython-4.0.1-py3-none-any.whl (730kB)\n","\u001b[K     |████████████████████████████████| 737kB 53.8MB/s \n","\u001b[?25hCollecting ipython-genutils==0.1.0\n","  Downloading https://files.pythonhosted.org/packages/6a/b3/4ce580020d6be73770d55515a65aec9ed5f3ea95f09e00300e53fc264218/ipython_genutils-0.1.0-py2.py3-none-any.whl\n","Collecting ipywidgets==4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/67/fd0239a344fd17a0e00009bbc117e68a74261e835a9a16195f1fdd5736a1/ipywidgets-4.1.1-py2.py3-none-any.whl (117kB)\n","\u001b[K     |████████████████████████████████| 122kB 61.4MB/s \n","\u001b[?25hCollecting Jinja2==2.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/a1/c56bc4d99dc2663514a8481511e80eba8994133ae75eebdadfc91a5597d9/Jinja2-2.8-py2.py3-none-any.whl (263kB)\n","\u001b[K     |████████████████████████████████| 266kB 65.9MB/s \n","\u001b[?25hCollecting jsonschema==2.5.1\n","  Downloading https://files.pythonhosted.org/packages/bd/cc/5388547ea3504bd8cbf99ba2ae7a3231598f54038e9b228cbd174f8ec6a1/jsonschema-2.5.1-py2.py3-none-any.whl\n","Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 30)) (1.0.0)\n","Collecting jupyter-client==4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/d5/418205a8c83cd34dc1e83b852936a6fe02e4ba3bdc420dc12a0a0ca7c3d8/jupyter_client-4.1.1-py2.py3-none-any.whl (70kB)\n","\u001b[K     |████████████████████████████████| 71kB 12.1MB/s \n","\u001b[?25hCollecting jupyter-console==4.0.3\n","  Downloading https://files.pythonhosted.org/packages/7c/c2/5e865900de40f7bf80b5cfbf9a79532f540a6679c7ded8f793f99cf546fa/jupyter_console-4.0.3-py2.py3-none-any.whl\n","Collecting jupyter-core==4.0.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/45/182ca59bf976794b910c03138692e2f73785a6e475ea17553f9988332595/jupyter_core-4.0.6-py2.py3-none-any.whl (74kB)\n","\u001b[K     |████████████████████████████████| 81kB 12.6MB/s \n","\u001b[?25hCollecting Markdown==2.6.11\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n","\u001b[K     |████████████████████████████████| 81kB 13.7MB/s \n","\u001b[?25hCollecting MarkupSafe==0.23\n","  Downloading https://files.pythonhosted.org/packages/c0/41/bae1254e0396c0cc8cf1751cb7d9afc90a602353695af5952530482c963f/MarkupSafe-0.23.tar.gz\n","Collecting matplotlib==2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/28/86488fe20a146241ece4570af94b49689ab80b66a1609256a8469a0bd1b0/matplotlib-2.0.0-1-cp36-cp36m-manylinux1_x86_64.whl (14.7MB)\n","\u001b[K     |████████████████████████████████| 14.7MB 209kB/s \n","\u001b[?25hCollecting mistune==0.7.1\n","  Downloading https://files.pythonhosted.org/packages/60/42/658ffae2278a95bd25b6f08f2a999155020c9bdd6df10a1c84157fcbcf3b/mistune-0.7.1-py2.py3-none-any.whl\n","Collecting multidict==4.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/30/508a22a28dfb50cf9079cd9d0cf9b0d7dbae5afdf9823977351cbd548897/multidict-4.3.1-cp36-cp36m-manylinux1_x86_64.whl (476kB)\n","\u001b[K     |████████████████████████████████| 481kB 55.6MB/s \n","\u001b[?25hCollecting nbconvert==4.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/98/39e08d99ba3a514203973d2d1c9098f1db5e95652951670962bb6f8e7514/nbconvert-4.1.0-py2.py3-none-any.whl (281kB)\n","\u001b[K     |████████████████████████████████| 286kB 45.8MB/s \n","\u001b[?25hCollecting nbformat==4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/2d/83180bdaa7c8aa1ff2308b928607d66ae907fb7061cbe59dc1e6243b69db/nbformat-4.0.1-py2.py3-none-any.whl (138kB)\n","\u001b[K     |████████████████████████████████| 143kB 43.2MB/s \n","\u001b[?25hCollecting nltk==3.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/ce/cba8bf82c8ab538d444ea4ab6f4eb1d80340c7b737d7a8d1f08b429fccae/nltk-3.2.2.tar.gz (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 62.6MB/s \n","\u001b[?25hCollecting notebook==4.0.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/af/87fbc159182390c35d5c04faaf30c10e3f972bcc0567ea0a99cf1eaf2c9f/notebook-4.0.6-py2.py3-none-any.whl (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 41.5MB/s \n","\u001b[?25hCollecting numexpr==2.6.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/5c/996afde0914a86e519bb1e716f6bf521dbbe1f3f23b579cb91b0f3b509a2/numexpr-2.6.5-cp36-cp36m-manylinux1_x86_64.whl (166kB)\n","\u001b[K     |████████████████████████████████| 174kB 72.0MB/s \n","\u001b[?25hCollecting numpy==1.13.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/a7/e3e6bd9d595125e1abbe162e323fd2d06f6f6683185294b79cd2cdb190d5/numpy-1.13.3-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n","\u001b[K     |████████████████████████████████| 17.0MB 127kB/s \n","\u001b[?25hCollecting path.py==8.1.2\n","  Downloading https://files.pythonhosted.org/packages/cc/2c/969f346ac0fdf1a5cecb746362f750c08f5c03258b1ed3ae5f31d776946a/path.py-8.1.2-py2.py3-none-any.whl\n","Collecting pexpect==4.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/50/3aa199018e996bcf963a81160083fd30557c3d4dab72eb1e9cce3afb69e7/pexpect-4.0.1.tar.gz (143kB)\n","\u001b[K     |████████████████████████████████| 153kB 75.3MB/s \n","\u001b[?25hCollecting pickleshare==0.5\n","  Downloading https://files.pythonhosted.org/packages/58/47/8b219a446eb351e1cb966cc10662bf8fa90a7710a9c61c2e81a585661ebb/pickleshare-0.5.tar.gz\n","Collecting Pillow==5.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/4b/8b54ab9d37b93998c81b364557dff9f61972c0f650efa0ceaf470b392740/Pillow-5.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K     |████████████████████████████████| 2.0MB 51.7MB/s \n","\u001b[?25hCollecting protobuf==3.5.2.post1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/ad/ecd865eb1ba1ff7f6bd6bcb731a89d55bc0450ced8d457ed2d167c7b8d5f/protobuf-3.5.2.post1-cp36-cp36m-manylinux1_x86_64.whl (6.4MB)\n","\u001b[K     |████████████████████████████████| 6.4MB 28.8MB/s \n","\u001b[?25hCollecting ptyprocess==0.5\n","  Downloading https://files.pythonhosted.org/packages/21/4e/590ee28cfe596e190cf5409ea0c29cb00dd61b22568571960bddeb8bac9c/ptyprocess-0.5.tar.gz\n","Collecting Pygments==2.0.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/e3/cadb43a197476ec0adef73292ea7ea188f2b0531188eebc150905d3ed78c/Pygments-2.0.2-py3-none-any.whl (672kB)\n","\u001b[K     |████████████████████████████████| 675kB 52.0MB/s \n","\u001b[?25hCollecting pyparsing==2.0.7\n","  Downloading https://files.pythonhosted.org/packages/ac/de/44b122749e143cf87db83edb2d3d239da772e1c3ae4cbeb55964a1bfabdd/pyparsing-2.0.7-py2.py3-none-any.whl\n","Collecting python-dateutil==2.4.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/75/666cd70de6a70cc7c6560429340ee7ef08196c93f552428983a808423755/python_dateutil-2.4.2-py2.py3-none-any.whl (188kB)\n","\u001b[K     |████████████████████████████████| 194kB 77.6MB/s \n","\u001b[?25hCollecting pytz==2015.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/28/973f0382c803b21734cd7e97e0590928148ee21b1cbe8f7fed8b506204fb/pytz-2015.7-py2.py3-none-any.whl (476kB)\n","\u001b[K     |████████████████████████████████| 481kB 63.0MB/s \n","\u001b[?25hCollecting pyzmq==15.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/bb/e2d939287eeebba9c686d54c2d83590cea37028daea888abde225c5de6d2/pyzmq-15.1.0.tar.gz (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 50.8MB/s \n","\u001b[?25hCollecting qtconsole==4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/63/29ba2f12cd4195eef706375adc00573d99cd381a500eeb9d026232850881/qtconsole-4.1.1-py2.py3-none-any.whl (98kB)\n","\u001b[K     |████████████████████████████████| 102kB 16.3MB/s \n","\u001b[?25hCollecting scipy==0.19.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/7b/415fd5bb215f28b423d32dc98126f700ebe7f1efa53e65377ed6ed55df99/scipy-0.19.0-cp36-cp36m-manylinux1_x86_64.whl (48.2MB)\n","\u001b[K     |████████████████████████████████| 48.2MB 63kB/s \n","\u001b[?25hRequirement already satisfied: simplegeneric==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 58)) (0.8.1)\n","Collecting singledispatch==3.4.0.3\n","  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n","Collecting sites==0.0.1\n","  Downloading https://files.pythonhosted.org/packages/62/49/f6bb1c3b88bd9b4bb5499c22547d8d1c0d91a05ab6281885ff521a0c64c7/sites-0.0.1-py3-none-any.whl\n","Collecting six==1.10.0\n","  Downloading https://files.pythonhosted.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b44607ab535be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 62)) (1.1.0)\n","Collecting terminado==0.5\n","  Downloading https://files.pythonhosted.org/packages/9b/fd/8dfb64877a020511461233230c442ed629bdf845792afc3d5f5f07989598/terminado-0.5.tar.gz\n","Collecting tornado==4.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/29/e64c97013e97d42d93b3d5997234a6f17455f3744847a7c16289289f8fa6/tornado-4.3.tar.gz (450kB)\n","\u001b[K     |████████████████████████████████| 460kB 46.5MB/s \n","\u001b[?25hCollecting traitlets==4.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/9b/4a47994434b0f9f044c0cb44ebd5402a7cef4e02ff0662f7f5a2eeb6d484/traitlets-4.0.0-py2.py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 10.3MB/s \n","\u001b[?25hCollecting Werkzeug==0.14.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n","\u001b[K     |████████████████████████████████| 327kB 62.2MB/s \n","\u001b[?25hCollecting yarl==1.2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/da/aec3974c0de3f7cde440bc5f984145cdc2c3ac2060944b1233e392e2e6db/yarl-1.2.4-cp36-cp36m-manylinux1_x86_64.whl (252kB)\n","\u001b[K     |████████████████████████████████| 256kB 56.8MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf==3.5.2.post1->-r requirements.txt (line 49)) (42.0.2)\n","Building wheels for collected packages: absl-py, aiocoap, backports.ssl-match-hostname, gast, gnureadline, html5lib, idna-ssl, MarkupSafe, nltk, pexpect, pickleshare, ptyprocess, pyzmq, terminado, tornado\n","  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for absl-py: filename=absl_py-0.2.0-cp36-none-any.whl size=98179 sha256=72867b97a9009a3f0696d129b1373dedb44d8c2c13a1128327b41045be6ee25c\n","  Stored in directory: /root/.cache/pip/wheels/23/35/1d/48c0a173ca38690dd8dfccfa47ffc750db48f8989ed898455c\n","  Building wheel for aiocoap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for aiocoap: filename=aiocoap-0.3-cp36-none-any.whl size=81742 sha256=eb541680688281b678e1eccf043eefb0fb0460fb3e94b87d6c5bcfddce37d226\n","  Stored in directory: /root/.cache/pip/wheels/25/95/c4/a4ea61adaba87ac1805f3d3586f9db2bb718231a0f6c054d43\n","  Building wheel for backports.ssl-match-hostname (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for backports.ssl-match-hostname: filename=backports.ssl_match_hostname-3.5.0.1-cp36-none-any.whl size=5208 sha256=25cf8831212506f3d72c89f2d049e7c4de20efdcb768c503bee5b7d979fae927\n","  Stored in directory: /root/.cache/pip/wheels/99/7e/f7/a88a9bcf7a3bd6b12cf6a74eee8c89746aaa02f71ab7b33939\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.0-cp36-none-any.whl size=6665 sha256=ab6db4a01529175bff11c6764f8a57475bfe90dfe6271b05181924e843230a9a\n","  Stored in directory: /root/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n","  Building wheel for gnureadline (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gnureadline: filename=gnureadline-6.3.3-cp36-cp36m-linux_x86_64.whl size=379235 sha256=a710162cf3c187dfe54fb1e62173c17b1650651ff21dd95d2c6b5dc7442fb7f9\n","  Stored in directory: /root/.cache/pip/wheels/04/3e/18/247f1453fb466979e1b72100dfce2c62d103d7c0bb35ce4fc8\n","  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107221 sha256=df7fb62bdf9b6a51ca49ca23ad8a56ff59d01e93b6fefdf977049c6938ff15b9\n","  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n","  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for idna-ssl: filename=idna_ssl-1.0.1-cp36-none-any.whl size=3130 sha256=3ec3d67e75bccfd97d13cf98f31d0fd04567ac4fd4e462dc777f7b4e5cf79067\n","  Stored in directory: /root/.cache/pip/wheels/67/97/96/5745320e00648bb29ebaa62a555b613f34a94ccefeb8fc7d45\n","  Building wheel for MarkupSafe (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for MarkupSafe: filename=MarkupSafe-0.23-cp36-cp36m-linux_x86_64.whl size=31340 sha256=f023808d813471350840e7d4bb930af3c3c46322f6dfbdbf1e8dd6ae17a0d030\n","  Stored in directory: /root/.cache/pip/wheels/28/de/65/f28b426d990edb591113e1549c8a0f09034e5958e440629306\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.2.2-cp36-none-any.whl size=1353251 sha256=87277cdf802264aceca050a59e9e1a9e4527336b0ec744d140a209412c7d485c\n","  Stored in directory: /root/.cache/pip/wheels/64/db/e2/39e07b414a807d7aa0350c58417f61fd8654eca1fb5daf20b8\n","  Building wheel for pexpect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pexpect: filename=pexpect-4.0.1-cp36-none-any.whl size=53288 sha256=b562f7a205289a496d23ab46cf9154fd6e32daa9f5ab010759f967900d594e3a\n","  Stored in directory: /root/.cache/pip/wheels/a7/8b/61/1c5ec2a1ac08abd0b837d81e0ccbb48cee3e25cca8d9029b5b\n","  Building wheel for pickleshare (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickleshare: filename=pickleshare-0.5-cp36-none-any.whl size=5140 sha256=adaf26446d3b09c7c46eee9eb23543f786de61da486b10b4e21934463e96cd7a\n","  Stored in directory: /root/.cache/pip/wheels/47/b9/1a/2f2036ca0235dd24ffe8fad661aefc055a65eea2565c1ff077\n","  Building wheel for ptyprocess (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ptyprocess: filename=ptyprocess-0.5-cp36-none-any.whl size=12980 sha256=d9d2036b984c0c28b0ab3cf5c90bf41d335e00feb4b047fc175e4af085393047\n","  Stored in directory: /root/.cache/pip/wheels/e9/34/b9/d89e62d3beb7df530e1ed9e999a03fc1a041ab1f3a80e6d654\n","  Building wheel for pyzmq (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyzmq: filename=pyzmq-15.1.0-cp36-cp36m-linux_x86_64.whl size=878303 sha256=a3d0ca811e5399807de42712727bc42b5267ba960d327541661af24538866877\n","  Stored in directory: /root/.cache/pip/wheels/84/65/1e/06b11fa357456f53815ab0e09979053933f614c0ab12e7dbb8\n","  Building wheel for terminado (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for terminado: filename=terminado-0.5-cp36-none-any.whl size=12712 sha256=75b7b3ab5851593d6f4500fc31aade7ffc21fb141d150d05338c323f0ab03a44\n","  Stored in directory: /root/.cache/pip/wheels/f4/29/26/1164c12cab668785b8cd72b3e207fbe434116568f68cce08ea\n","  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tornado: filename=tornado-4.3-cp36-cp36m-linux_x86_64.whl size=404153 sha256=9c4fba87317a38605d7eb3d394a9fccec4578e00d4c7bef316efdb420e3bc17f\n","  Stored in directory: /root/.cache/pip/wheels/00/be/0e/5bd46d9f24cf214e295555ff6832782701c836695145413e70\n","Successfully built absl-py aiocoap backports.ssl-match-hostname gast gnureadline html5lib idna-ssl MarkupSafe nltk pexpect pickleshare ptyprocess pyzmq terminado tornado\n","\u001b[31mERROR: yellowbrick 0.9.1 has requirement scipy>=1.0.0, but you'll have scipy 0.19.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: xarray 0.14.1 has requirement numpy>=1.14, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: widgetsnbextension 3.5.1 has requirement notebook>=4.4.1, but you'll have notebook 4.0.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 1.15.0 has requirement absl-py>=0.7.0, but you'll have absl-py 0.2.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 1.15.0 has requirement gast==0.2.2, but you'll have gast 0.2.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 1.15.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 1.15.0 has requirement protobuf>=3.6.1, but you'll have protobuf 3.5.2.post1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-metadata 0.15.2 has requirement protobuf<4,>=3.7, but you'll have protobuf 3.5.2.post1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-datasets 1.3.2 has requirement protobuf>=3.6.1, but you'll have protobuf 3.5.2.post1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorboard 1.15.0 has requirement absl-py>=0.4, but you'll have absl-py 0.2.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorboard 1.15.0 has requirement protobuf>=3.6.0, but you'll have protobuf 3.5.2.post1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: spacy 2.1.9 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: requests 2.21.0 has requirement certifi>=2017.4.17, but you'll have certifi 2015.11.20.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pydot 1.3.0 has requirement pyparsing>=2.1.4, but you'll have pyparsing 2.0.7 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pyarrow 0.14.1 has requirement numpy>=1.14, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 2.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 0.19.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pandas 0.25.3 has requirement python-dateutil>=2.6.1, but you'll have python-dateutil 2.4.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pandas 0.25.3 has requirement pytz>=2017.2, but you'll have pytz 2015.7 which is incompatible.\u001b[0m\n","\u001b[31mERROR: networkx 2.4 has requirement decorator>=4.3.0, but you'll have decorator 4.0.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 2.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: mir-eval 0.5 has requirement scipy>=1.0.0, but you'll have scipy 0.19.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: librosa 0.6.3 has requirement scipy>=1.0.0, but you'll have scipy 0.19.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: jaxlib 0.1.36 has requirement protobuf>=3.6.0, but you'll have protobuf 3.5.2.post1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: googleapis-common-protos 1.6.0 has requirement protobuf>=3.6.0, but you'll have protobuf 3.5.2.post1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.6.0, but you'll have ipykernel 4.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 4.0.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement notebook~=5.2.0, but you'll have notebook 4.0.6 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.10.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=4.5.0, but you'll have tornado 4.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-cloud-bigquery 1.21.0 has requirement protobuf>=3.6.0, but you'll have protobuf 3.5.2.post1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: flask 1.1.1 has requirement Jinja2>=2.10.1, but you'll have jinja2 2.8 which is incompatible.\u001b[0m\n","\u001b[31mERROR: flask 1.1.1 has requirement Werkzeug>=0.15, but you'll have werkzeug 0.14.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.60 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: dopamine-rl 1.0.5 has requirement absl-py>=0.2.2, but you'll have absl-py 0.2.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: distributed 1.25.3 has requirement tornado>=4.5.1, but you'll have tornado 4.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: cvxpy 1.0.25 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: cvxpy 1.0.25 has requirement scipy>=1.1.0, but you'll have scipy 0.19.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: cufflinks 0.17.0 has requirement ipython>=5.3.0, but you'll have ipython 4.0.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: cufflinks 0.17.0 has requirement ipywidgets>=7.0.0, but you'll have ipywidgets 4.1.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: astropy 4.0 has requirement numpy>=1.16, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: six, absl-py, aiocoap, multidict, idna, yarl, async-timeout, idna-ssl, attrs, aiohttp, appnope, astor, backports-abc, backports.ssl-match-hostname, html5lib, bleach, certifi, Cython, decorator, gast, gnureadline, protobuf, grpcio, numpy, h5py, path.py, pickleshare, ptyprocess, pexpect, ipython-genutils, traitlets, ipython, jupyter-core, pyzmq, jupyter-client, ipykernel, MarkupSafe, Jinja2, tornado, terminado, Pygments, mistune, jsonschema, nbformat, nbconvert, notebook, ipywidgets, jupyter-console, Markdown, python-dateutil, pytz, pyparsing, matplotlib, nltk, numexpr, Pillow, qtconsole, scipy, singledispatch, Werkzeug, sites\n","  Found existing installation: six 1.12.0\n","    Uninstalling six-1.12.0:\n","      Successfully uninstalled six-1.12.0\n","  Found existing installation: absl-py 0.9.0\n","    Uninstalling absl-py-0.9.0:\n","      Successfully uninstalled absl-py-0.9.0\n","  Found existing installation: idna 2.8\n","    Uninstalling idna-2.8:\n","      Successfully uninstalled idna-2.8\n","  Found existing installation: attrs 19.3.0\n","    Uninstalling attrs-19.3.0:\n","      Successfully uninstalled attrs-19.3.0\n","  Found existing installation: astor 0.8.1\n","    Uninstalling astor-0.8.1:\n","      Successfully uninstalled astor-0.8.1\n","  Found existing installation: html5lib 1.0.1\n","    Uninstalling html5lib-1.0.1:\n","      Successfully uninstalled html5lib-1.0.1\n","  Found existing installation: bleach 3.1.0\n","    Uninstalling bleach-3.1.0:\n","      Successfully uninstalled bleach-3.1.0\n","  Found existing installation: certifi 2019.11.28\n","    Uninstalling certifi-2019.11.28:\n","      Successfully uninstalled certifi-2019.11.28\n","  Found existing installation: Cython 0.29.14\n","    Uninstalling Cython-0.29.14:\n","      Successfully uninstalled Cython-0.29.14\n","  Found existing installation: decorator 4.4.1\n","    Uninstalling decorator-4.4.1:\n","      Successfully uninstalled decorator-4.4.1\n","  Found existing installation: gast 0.2.2\n","    Uninstalling gast-0.2.2:\n","      Successfully uninstalled gast-0.2.2\n","  Found existing installation: protobuf 3.10.0\n","    Uninstalling protobuf-3.10.0:\n","      Successfully uninstalled protobuf-3.10.0\n","  Found existing installation: grpcio 1.15.0\n","    Uninstalling grpcio-1.15.0:\n","      Successfully uninstalled grpcio-1.15.0\n","  Found existing installation: numpy 1.17.5\n","    Uninstalling numpy-1.17.5:\n","      Successfully uninstalled numpy-1.17.5\n","  Found existing installation: h5py 2.8.0\n","    Uninstalling h5py-2.8.0:\n","      Successfully uninstalled h5py-2.8.0\n","  Found existing installation: pickleshare 0.7.5\n","    Uninstalling pickleshare-0.7.5:\n","      Successfully uninstalled pickleshare-0.7.5\n","  Found existing installation: ptyprocess 0.6.0\n","    Uninstalling ptyprocess-0.6.0:\n","      Successfully uninstalled ptyprocess-0.6.0\n","  Found existing installation: pexpect 4.7.0\n","    Uninstalling pexpect-4.7.0:\n","      Successfully uninstalled pexpect-4.7.0\n","  Found existing installation: ipython-genutils 0.2.0\n","    Uninstalling ipython-genutils-0.2.0:\n","      Successfully uninstalled ipython-genutils-0.2.0\n","  Found existing installation: traitlets 4.3.3\n","    Uninstalling traitlets-4.3.3:\n","      Successfully uninstalled traitlets-4.3.3\n","  Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Found existing installation: jupyter-core 4.6.1\n","    Uninstalling jupyter-core-4.6.1:\n","      Successfully uninstalled jupyter-core-4.6.1\n","  Found existing installation: pyzmq 17.0.0\n","    Uninstalling pyzmq-17.0.0:\n","      Successfully uninstalled pyzmq-17.0.0\n","  Found existing installation: jupyter-client 5.3.4\n","    Uninstalling jupyter-client-5.3.4:\n","      Successfully uninstalled jupyter-client-5.3.4\n","  Found existing installation: ipykernel 4.6.1\n","    Uninstalling ipykernel-4.6.1:\n","      Successfully uninstalled ipykernel-4.6.1\n","  Found existing installation: MarkupSafe 1.1.1\n","    Uninstalling MarkupSafe-1.1.1:\n","      Successfully uninstalled MarkupSafe-1.1.1\n","  Found existing installation: Jinja2 2.10.3\n","    Uninstalling Jinja2-2.10.3:\n","      Successfully uninstalled Jinja2-2.10.3\n","  Found existing installation: tornado 4.5.3\n","    Uninstalling tornado-4.5.3:\n","      Successfully uninstalled tornado-4.5.3\n","  Found existing installation: terminado 0.8.3\n","    Uninstalling terminado-0.8.3:\n","      Successfully uninstalled terminado-0.8.3\n","  Found existing installation: Pygments 2.1.3\n","    Uninstalling Pygments-2.1.3:\n","      Successfully uninstalled Pygments-2.1.3\n","  Found existing installation: mistune 0.8.4\n","    Uninstalling mistune-0.8.4:\n","      Successfully uninstalled mistune-0.8.4\n","  Found existing installation: jsonschema 2.6.0\n","    Uninstalling jsonschema-2.6.0:\n","      Successfully uninstalled jsonschema-2.6.0\n","  Found existing installation: nbformat 5.0.3\n","    Uninstalling nbformat-5.0.3:\n","      Successfully uninstalled nbformat-5.0.3\n","  Found existing installation: nbconvert 5.6.1\n","    Uninstalling nbconvert-5.6.1:\n","      Successfully uninstalled nbconvert-5.6.1\n","  Found existing installation: notebook 5.2.2\n","    Uninstalling notebook-5.2.2:\n","      Successfully uninstalled notebook-5.2.2\n","  Found existing installation: ipywidgets 7.5.1\n","    Uninstalling ipywidgets-7.5.1:\n","      Successfully uninstalled ipywidgets-7.5.1\n","  Found existing installation: jupyter-console 5.2.0\n","    Uninstalling jupyter-console-5.2.0:\n","      Successfully uninstalled jupyter-console-5.2.0\n","  Found existing installation: Markdown 3.1.1\n","    Uninstalling Markdown-3.1.1:\n","      Successfully uninstalled Markdown-3.1.1\n","  Found existing installation: python-dateutil 2.6.1\n","    Uninstalling python-dateutil-2.6.1:\n","      Successfully uninstalled python-dateutil-2.6.1\n","  Found existing installation: pytz 2018.9\n","    Uninstalling pytz-2018.9:\n","      Successfully uninstalled pytz-2018.9\n","  Found existing installation: pyparsing 2.4.6\n","    Uninstalling pyparsing-2.4.6:\n","      Successfully uninstalled pyparsing-2.4.6\n","  Found existing installation: matplotlib 3.1.2\n","    Uninstalling matplotlib-3.1.2:\n","      Successfully uninstalled matplotlib-3.1.2\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","  Found existing installation: numexpr 2.7.1\n","    Uninstalling numexpr-2.7.1:\n","      Successfully uninstalled numexpr-2.7.1\n","  Found existing installation: Pillow 6.2.2\n","    Uninstalling Pillow-6.2.2:\n","      Successfully uninstalled Pillow-6.2.2\n","  Found existing installation: qtconsole 4.6.0\n","    Uninstalling qtconsole-4.6.0:\n","      Successfully uninstalled qtconsole-4.6.0\n","  Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","  Found existing installation: Werkzeug 0.16.0\n","    Uninstalling Werkzeug-0.16.0:\n","      Successfully uninstalled Werkzeug-0.16.0\n","Successfully installed Cython-0.25.2 Jinja2-2.8 Markdown-2.6.11 MarkupSafe-0.23 Pillow-5.1.0 Pygments-2.0.2 Werkzeug-0.14.1 absl-py-0.2.0 aiocoap-0.3 aiohttp-3.2.0 appnope-0.1.0 astor-0.6.2 async-timeout-3.0.0 attrs-18.1.0 backports-abc-0.4 backports.ssl-match-hostname-3.5.0.1 bleach-1.5.0 certifi-2015.11.20.1 decorator-4.0.6 gast-0.2.0 gnureadline-6.3.3 grpcio-1.11.0 h5py-2.7.0 html5lib-0.9999999 idna-2.6 idna-ssl-1.0.1 ipykernel-4.2.2 ipython-4.0.1 ipython-genutils-0.1.0 ipywidgets-4.1.1 jsonschema-2.5.1 jupyter-client-4.1.1 jupyter-console-4.0.3 jupyter-core-4.0.6 matplotlib-2.0.0 mistune-0.7.1 multidict-4.3.1 nbconvert-4.1.0 nbformat-4.0.1 nltk-3.2.2 notebook-4.0.6 numexpr-2.6.5 numpy-1.13.3 path.py-8.1.2 pexpect-4.0.1 pickleshare-0.5 protobuf-3.5.2.post1 ptyprocess-0.5 pyparsing-2.0.7 python-dateutil-2.4.2 pytz-2015.7 pyzmq-15.1.0 qtconsole-4.1.1 scipy-0.19.0 singledispatch-3.4.0.3 sites-0.0.1 six-1.10.0 terminado-0.5 tornado-4.3 traitlets-4.0.0 yarl-1.2.4\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","PIL","certifi","dateutil","decorator","google","grpc","idna","ipykernel","ipython_genutils","ipywidgets","jupyter_client","jupyter_core","matplotlib","mpl_toolkits","numpy","pexpect","pickleshare","ptyprocess","pygments","pyparsing","pytz","six","tornado","traitlets","zmq"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"HGFOUt2o_fX5","colab_type":"code","outputId":"254891f6-16bf-4620-90d5-304a892a272f","executionInfo":{"status":"ok","timestamp":1579609258965,"user_tz":-540,"elapsed":554,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd cs231n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/cs231n/assignment3/cs231n\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i02_SroqDEPD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"f5ebcee4-14a3-4a29-ea73-32f6023fff14","executionInfo":{"status":"ok","timestamp":1579609262625,"user_tz":-540,"elapsed":3176,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}}},"source":["!python setup.py"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Warning: Extension name 'im2col_cython' does not match fully qualified name 'cs231n.im2col_cython' of 'im2col_cython.pyx'\n","Compiling im2col_cython.pyx because it depends on /usr/local/lib/python3.6/dist-packages/Cython/Includes/libc/string.pxd.\n","[1/1] Cythonizing im2col_cython.pyx\n","usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n","   or: setup.py --help [cmd1 cmd2 ...]\n","   or: setup.py --help-commands\n","   or: setup.py cmd --help\n","\n","error: no commands supplied\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"trgqNOVRC_8l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6c8092ed-0694-4595-a8ae-1d3ada9c5b91","executionInfo":{"status":"ok","timestamp":1579609417026,"user_tz":-540,"elapsed":682,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}}},"source":["cd .."],"execution_count":10,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/cs231n/assignment3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"tags":["pdf-ignore"],"id":"-KsYdd_Q-Id1","colab_type":"code","outputId":"6ee15ab0-72a2-4ddb-e392-fe35dc4e88c1","executionInfo":{"status":"ok","timestamp":1579609422288,"user_tz":-540,"elapsed":4105,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# As usual, a bit of setup\n","import time, os, json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n","from cs231n.rnn_layers import *\n","from cs231n.captioning_solver import CaptioningSolver\n","from cs231n.classifiers.rnn import CaptioningRNN\n","from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n","from cs231n.image_utils import image_from_url\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n","  from ._conv import register_converters as _register_converters\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"bGFx49vp-Id3","colab_type":"text"},"source":["## Install h5py\n","The COCO dataset we will be using is stored in HDF5 format. To load HDF5 files, we will need to install the `h5py` Python package. From the command line, run: <br/>\n","`pip install h5py`  <br/>\n","If you receive a permissions error, you may need to run the command as root: <br/>\n","```sudo pip install h5py```\n","\n","You can also run commands directly from the Jupyter notebook by prefixing the command with the \"!\" character:"]},{"cell_type":"code","metadata":{"id":"b_dRV6UO-Id4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"f08731ae-dbd9-44aa-d0e0-b64ce005b97b","executionInfo":{"status":"ok","timestamp":1579609446850,"user_tz":-540,"elapsed":3477,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}}},"source":["!pip install h5py"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.7.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.10.0)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.13.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"tags":["pdf-ignore"],"id":"Jzi1qKMk-Id6","colab_type":"text"},"source":["# Microsoft COCO\n","For this exercise we will use the 2014 release of the [Microsoft COCO dataset](http://mscoco.org/) which has become the standard testbed for image captioning. The dataset consists of 80,000 training images and 40,000 validation images, each annotated with 5 captions written by workers on Amazon Mechanical Turk.\n","\n","You should have already downloaded the data by changing to the `cs231n/datasets` directory and running the script `get_assignment3_data.sh`. If you haven't yet done so, run that script now. Warning: the COCO data download is ~1GB.\n","\n","We have preprocessed the data and extracted features for you already. For all images we have extracted features from the fc7 layer of the VGG-16 network pretrained on ImageNet; these features are stored in the files `train2014_vgg16_fc7.h5` and `val2014_vgg16_fc7.h5` respectively. To cut down on processing time and memory requirements, we have reduced the dimensionality of the features from 4096 to 512; these features can be found in the files `train2014_vgg16_fc7_pca.h5` and `val2014_vgg16_fc7_pca.h5`.\n","\n","The raw images take up a lot of space (nearly 20GB) so we have not included them in the download. However all images are taken from Flickr, and URLs of the training and validation images are stored in the files `train2014_urls.txt` and `val2014_urls.txt` respectively. This allows you to download images on the fly for visualization. Since images are downloaded on-the-fly, **you must be connected to the internet to view images**.\n","\n","Dealing with strings is inefficient, so we will work with an encoded version of the captions. Each word is assigned an integer ID, allowing us to represent a caption by a sequence of integers. The mapping between integer IDs and words is in the file `coco2014_vocab.json`, and you can use the function `decode_captions` from the file `cs231n/coco_utils.py` to convert numpy arrays of integer IDs back into strings.\n","\n","There are a couple special tokens that we add to the vocabulary. We prepend a special `<START>` token and append an `<END>` token to the beginning and end of each caption respectively. Rare words are replaced with a special `<UNK>` token (for \"unknown\"). In addition, since we want to train with minibatches containing captions of different lengths, we pad short captions with a special `<NULL>` token after the `<END>` token and don't compute loss or gradient for `<NULL>` tokens. Since they are a bit of a pain, we have taken care of all implementation details around special tokens for you.\n","\n","You can load all of the MS-COCO data (captions, features, URLs, and vocabulary) using the `load_coco_data` function from the file `cs231n/coco_utils.py`. Run the following cell to do so:"]},{"cell_type":"code","metadata":{"tags":["pdf-ignore"],"id":"YXHQEvSm-Id6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"5a9200b1-3e34-4dcd-81a5-531ba289e6bf","executionInfo":{"status":"ok","timestamp":1579610636942,"user_tz":-540,"elapsed":6666,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}}},"source":["# Load COCO data from disk; this returns a dictionary\n","# We'll work with dimensionality-reduced features for this notebook, but feel\n","# free to experiment with the original features by changing the flag below.\n","data = load_coco_data(pca_features=True)\n","\n","# Print out all the keys and values from the data dictionary\n","for k, v in data.items():\n","    if type(v) == np.ndarray:\n","        print(k, type(v), v.shape, v.dtype)\n","    else:\n","        print(k, type(v), len(v))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["train_captions <class 'numpy.ndarray'> (400135, 17) int32\n","train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n","val_captions <class 'numpy.ndarray'> (195954, 17) int32\n","val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n","train_features <class 'numpy.ndarray'> (82783, 512) float32\n","val_features <class 'numpy.ndarray'> (40504, 512) float32\n","idx_to_word <class 'list'> 1004\n","word_to_idx <class 'dict'> 1004\n","train_urls <class 'numpy.ndarray'> (82783,) <U63\n","val_urls <class 'numpy.ndarray'> (40504,) <U63\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GN8tDb2O-Id8","colab_type":"text"},"source":["## Look at the data\n","It is always a good idea to look at examples from the dataset before working with it.\n","\n","You can use the `sample_coco_minibatch` function from the file `cs231n/coco_utils.py` to sample minibatches of data from the data structure returned from `load_coco_data`. Run the following to sample a small minibatch of training data and show the images and their captions. Running it multiple times and looking at the results helps you to get a sense of the dataset.\n","\n","Note that we decode the captions using the `decode_captions` function and that we download the images on-the-fly using their Flickr URL, so **you must be connected to the internet to view images**."]},{"cell_type":"code","metadata":{"id":"JBunhs9s-Id9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":375},"outputId":"6bf41ad4-8ba4-48f0-ea76-b5ee108d81b3","executionInfo":{"status":"error","timestamp":1579610799230,"user_tz":-540,"elapsed":1199,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}}},"source":["# Sample a minibatch and show the images and captions\n","batch_size = 3\n","\n","captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n","for i, (caption, url) in enumerate(zip(captions, urls)):\n","    plt.imshow(image_from_url(url))\n","    plt.axis('off')\n","    caption_str = decode_captions(caption, data['idx_to_word'])\n","    plt.title(caption_str)\n","    plt.show()"],"execution_count":16,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-49cc05547c0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_coco_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcaption_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_captions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx_to_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/cs231n/assignment3/cs231n/image_utils.py\u001b[0m in \u001b[0;36mimage_from_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(name, flatten, mode)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \"\"\"\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2804\u001b[0;31m \u001b[0;31m# --------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2805\u001b[0m \u001b[0;31m# Simple display support.  User code may override this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_open_core\u001b[0;34m(fp, filename, prefix)\u001b[0m\n\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2790\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mregister_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2791\u001b[0m     \"\"\"\n\u001b[1;32m   2792\u001b[0m     \u001b[0mRegisters\u001b[0m \u001b[0man\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mshould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36mjpeg_factory\u001b[0;34m(fp, filename)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;31m# Factory for making JPEG and MPO instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjpeg_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJpegImageFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mmpheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         except (IndexError,  # end of data\n\u001b[1;32m    104\u001b[0m                 \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# end of data (ord)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;31m# print(hex(i), name, description)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                     \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xFFDA\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# start of scan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                     \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36mSOF\u001b[0;34m(self, marker)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: can't set attribute"]}]},{"cell_type":"markdown","metadata":{"id":"_L3_rhqn-Id_","colab_type":"text"},"source":["# Recurrent Neural Networks\n","As discussed in lecture, we will use recurrent neural network (RNN) language models for image captioning. The file `cs231n/rnn_layers.py` contains implementations of different layer types that are needed for recurrent neural networks, and the file `cs231n/classifiers/rnn.py` uses these layers to implement an image captioning model.\n","\n","We will first implement different types of RNN layers in `cs231n/rnn_layers.py`."]},{"cell_type":"markdown","metadata":{"id":"M77AAR4J-IeA","colab_type":"text"},"source":["# Vanilla RNN: step forward\n","Open the file `cs231n/rnn_layers.py`. This file implements the forward and backward passes for different types of layers that are commonly used in recurrent neural networks.\n","\n","First implement the function `rnn_step_forward` which implements the forward pass for a single timestep of a vanilla recurrent neural network. After doing so run the following to check your implementation. You should see errors on the order of e-8 or less."]},{"cell_type":"code","metadata":{"id":"Pfrrkdqt-IeA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cc062d31-5395-40f8-81b7-b06dbf59376f","executionInfo":{"status":"ok","timestamp":1579612674316,"user_tz":-540,"elapsed":1199,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}}},"source":["N, D, H = 3, 10, 4\n","\n","x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n","prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n","Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n","Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n","b = np.linspace(-0.2, 0.4, num=H)\n","\n","next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n","expected_next_h = np.asarray([\n","  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n","  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n","  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n","\n","print('next_h error: ', rel_error(expected_next_h, next_h))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["next_h error:  6.292421426471037e-09\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2JQ6lraa-IeC","colab_type":"text"},"source":["# Vanilla RNN: step backward\n","In the file `cs231n/rnn_layers.py` implement the `rnn_step_backward` function. After doing so run the following to numerically gradient check your implementation. You should see errors on the order of `e-8` or less."]},{"cell_type":"code","metadata":{"id":"yr1ZeS7T-IeD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"b0e7fb59-0e4f-4ad2-e0ee-e49c0096af1a","executionInfo":{"status":"ok","timestamp":1579612974135,"user_tz":-540,"elapsed":780,"user":{"displayName":"Yoshihiro Kumazawa","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOadckYBPAEKocSXLtXI7iGlL0LxaIAJ4boabP=s64","userId":"03803515963893297686"}}},"source":["from cs231n.rnn_layers import rnn_step_forward, rnn_step_backward\n","np.random.seed(231)\n","N, D, H = 4, 5, 6\n","x = np.random.randn(N, D)\n","h = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b = np.random.randn(H)\n","\n","out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n","\n","dnext_h = np.random.randn(*out.shape)\n","\n","fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n","dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n","dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n","dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n","db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n","\n","dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n","print('dWx error: ', rel_error(dWx_num, dWx))\n","print('dWh error: ', rel_error(dWh_num, dWh))\n","print('db error: ', rel_error(db_num, db))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["dx error:  4.0192769090159184e-10\n","dprev_h error:  2.5632975303201374e-10\n","dWx error:  8.820222259148609e-10\n","dWh error:  4.703287554560559e-10\n","db error:  7.30162216654e-11\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"awjRe9oU-IeE","colab_type":"text"},"source":["# Vanilla RNN: forward\n","Now that you have implemented the forward and backward passes for a single timestep of a vanilla RNN, you will combine these pieces to implement a RNN that processes an entire sequence of data.\n","\n","In the file `cs231n/rnn_layers.py`, implement the function `rnn_forward`. This should be implemented using the `rnn_step_forward` function that you defined above. After doing so run the following to check your implementation. You should see errors on the order of `e-7` or less."]},{"cell_type":"code","metadata":{"id":"dXP49c10-IeF","colab_type":"code","colab":{}},"source":["N, T, D, H = 2, 3, 4, 5\n","\n","x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n","h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n","Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n","Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n","b = np.linspace(-0.7, 0.1, num=H)\n","\n","h, _ = rnn_forward(x, h0, Wx, Wh, b)\n","expected_h = np.asarray([\n","  [\n","    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n","    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n","    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n","  ],\n","  [\n","    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n","    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n","    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n","print('h error: ', rel_error(expected_h, h))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5E43Zxln-IeG","colab_type":"text"},"source":["# Vanilla RNN: backward\n","In the file `cs231n/rnn_layers.py`, implement the backward pass for a vanilla RNN in the function `rnn_backward`. This should run back-propagation over the entire sequence, making calls to the `rnn_step_backward` function that you defined earlier. You should see errors on the order of e-6 or less."]},{"cell_type":"code","metadata":{"id":"qOTClvPL-IeH","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","\n","N, D, T, H = 2, 3, 10, 5\n","\n","x = np.random.randn(N, T, D)\n","h0 = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b = np.random.randn(H)\n","\n","out, cache = rnn_forward(x, h0, Wx, Wh, b)\n","\n","dout = np.random.randn(*out.shape)\n","\n","dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n","\n","fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dout)\n","dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n","dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n","dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n","db_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dh0 error: ', rel_error(dh0_num, dh0))\n","print('dWx error: ', rel_error(dWx_num, dWx))\n","print('dWh error: ', rel_error(dWh_num, dWh))\n","print('db error: ', rel_error(db_num, db))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eZg1LruJ-IeJ","colab_type":"text"},"source":["# Word embedding: forward\n","In deep learning systems, we commonly represent words using vectors. Each word of the vocabulary will be associated with a vector, and these vectors will be learned jointly with the rest of the system.\n","\n","In the file `cs231n/rnn_layers.py`, implement the function `word_embedding_forward` to convert words (represented by integers) into vectors. Run the following to check your implementation. You should see an error on the order of `e-8` or less."]},{"cell_type":"code","metadata":{"id":"aF4F4Yfv-IeJ","colab_type":"code","colab":{}},"source":["N, T, V, D = 2, 4, 5, 3\n","\n","x = np.asarray([[0, 3, 1, 2], [2, 1, 0, 3]])\n","W = np.linspace(0, 1, num=V*D).reshape(V, D)\n","\n","out, _ = word_embedding_forward(x, W)\n","expected_out = np.asarray([\n"," [[ 0.,          0.07142857,  0.14285714],\n","  [ 0.64285714,  0.71428571,  0.78571429],\n","  [ 0.21428571,  0.28571429,  0.35714286],\n","  [ 0.42857143,  0.5,         0.57142857]],\n"," [[ 0.42857143,  0.5,         0.57142857],\n","  [ 0.21428571,  0.28571429,  0.35714286],\n","  [ 0.,          0.07142857,  0.14285714],\n","  [ 0.64285714,  0.71428571,  0.78571429]]])\n","\n","print('out error: ', rel_error(expected_out, out))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AaAgcsAq-IeL","colab_type":"text"},"source":["# Word embedding: backward\n","Implement the backward pass for the word embedding function in the function `word_embedding_backward`. After doing so run the following to numerically gradient check your implementation. You should see an error on the order of `e-11` or less."]},{"cell_type":"code","metadata":{"id":"dRvolkC1-IeM","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","\n","N, T, V, D = 50, 3, 5, 6\n","x = np.random.randint(V, size=(N, T))\n","W = np.random.randn(V, D)\n","\n","out, cache = word_embedding_forward(x, W)\n","dout = np.random.randn(*out.shape)\n","dW = word_embedding_backward(dout, cache)\n","\n","f = lambda W: word_embedding_forward(x, W)[0]\n","dW_num = eval_numerical_gradient_array(f, W, dout)\n","\n","print('dW error: ', rel_error(dW, dW_num))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"k3z0x8T9-IeN","colab_type":"text"},"source":["# Temporal Affine layer\n","At every timestep we use an affine function to transform the RNN hidden vector at that timestep into scores for each word in the vocabulary. Because this is very similar to the affine layer that you implemented in assignment 2, we have provided this function for you in the `temporal_affine_forward` and `temporal_affine_backward` functions in the file `cs231n/rnn_layers.py`. Run the following to perform numeric gradient checking on the implementation. You should see errors on the order of e-9 or less."]},{"cell_type":"code","metadata":{"tags":[],"id":"EJRMUn4x-IeO","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","\n","# Gradient check for temporal affine layer\n","N, T, D, M = 2, 3, 4, 5\n","x = np.random.randn(N, T, D)\n","w = np.random.randn(D, M)\n","b = np.random.randn(M)\n","\n","out, cache = temporal_affine_forward(x, w, b)\n","\n","dout = np.random.randn(*out.shape)\n","\n","fx = lambda x: temporal_affine_forward(x, w, b)[0]\n","fw = lambda w: temporal_affine_forward(x, w, b)[0]\n","fb = lambda b: temporal_affine_forward(x, w, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dout)\n","dw_num = eval_numerical_gradient_array(fw, w, dout)\n","db_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","dx, dw, db = temporal_affine_backward(dout, cache)\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dw error: ', rel_error(dw_num, dw))\n","print('db error: ', rel_error(db_num, db))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":[],"id":"2ekNpGCF-IeR","colab_type":"text"},"source":["# Temporal Softmax loss\n","In an RNN language model, at every timestep we produce a score for each word in the vocabulary. We know the ground-truth word at each timestep, so we use a softmax loss function to compute loss and gradient at each timestep. We sum the losses over time and average them over the minibatch.\n","\n","However there is one wrinkle: since we operate over minibatches and different captions may have different lengths, we append `<NULL>` tokens to the end of each caption so they all have the same length. We don't want these `<NULL>` tokens to count toward the loss or gradient, so in addition to scores and ground-truth labels our loss function also accepts a `mask` array that tells it which elements of the scores count towards the loss.\n","\n","Since this is very similar to the softmax loss function you implemented in assignment 1, we have implemented this loss function for you; look at the `temporal_softmax_loss` function in the file `cs231n/rnn_layers.py`.\n","\n","Run the following cell to sanity check the loss and perform numeric gradient checking on the function. You should see an error for dx on the order of e-7 or less."]},{"cell_type":"code","metadata":{"tags":[],"id":"SB1bS5ls-IeS","colab_type":"code","colab":{}},"source":["# Sanity check for temporal softmax loss\n","from cs231n.rnn_layers import temporal_softmax_loss\n","\n","N, T, V = 100, 1, 10\n","\n","def check_loss(N, T, V, p):\n","    x = 0.001 * np.random.randn(N, T, V)\n","    y = np.random.randint(V, size=(N, T))\n","    mask = np.random.rand(N, T) <= p\n","    print(temporal_softmax_loss(x, y, mask)[0])\n","  \n","check_loss(100, 1, 10, 1.0)   # Should be about 2.3\n","check_loss(100, 10, 10, 1.0)  # Should be about 23\n","check_loss(5000, 10, 10, 0.1) # Should be about 2.3\n","\n","# Gradient check for temporal softmax loss\n","N, T, V = 7, 8, 9\n","\n","x = np.random.randn(N, T, V)\n","y = np.random.randint(V, size=(N, T))\n","mask = (np.random.rand(N, T) > 0.5)\n","\n","loss, dx = temporal_softmax_loss(x, y, mask, verbose=False)\n","\n","dx_num = eval_numerical_gradient(lambda x: temporal_softmax_loss(x, y, mask)[0], x, verbose=False)\n","\n","print('dx error: ', rel_error(dx, dx_num))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzFN0fJ1-IeT","colab_type":"text"},"source":["# RNN for image captioning\n","Now that you have implemented the necessary layers, you can combine them to build an image captioning model. Open the file `cs231n/classifiers/rnn.py` and look at the `CaptioningRNN` class.\n","\n","Implement the forward and backward pass of the model in the `loss` function. For now you only need to implement the case where `cell_type='rnn'` for vanialla RNNs; you will implement the LSTM case later. After doing so, run the following to check your forward pass using a small test case; you should see error on the order of `e-10` or less."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"wjzgho3L-IeU","colab_type":"code","colab":{}},"source":["N, D, W, H = 10, 20, 30, 40\n","word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n","V = len(word_to_idx)\n","T = 13\n","\n","model = CaptioningRNN(word_to_idx,\n","          input_dim=D,\n","          wordvec_dim=W,\n","          hidden_dim=H,\n","          cell_type='rnn',\n","          dtype=np.float64)\n","\n","# Set all model parameters to fixed values\n","for k, v in model.params.items():\n","    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n","\n","features = np.linspace(-1.5, 0.3, num=(N * D)).reshape(N, D)\n","captions = (np.arange(N * T) % V).reshape(N, T)\n","\n","loss, grads = model.loss(features, captions)\n","expected_loss = 9.83235591003\n","\n","print('loss: ', loss)\n","print('expected loss: ', expected_loss)\n","print('difference: ', abs(loss - expected_loss))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RgwMaybB-IeW","colab_type":"text"},"source":["Run the following cell to perform numeric gradient checking on the `CaptioningRNN` class; you should see errors around the order of `e-6` or less."]},{"cell_type":"code","metadata":{"id":"bOtp71KF-IeW","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","\n","batch_size = 2\n","timesteps = 3\n","input_dim = 4\n","wordvec_dim = 5\n","hidden_dim = 6\n","word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n","vocab_size = len(word_to_idx)\n","\n","captions = np.random.randint(vocab_size, size=(batch_size, timesteps))\n","features = np.random.randn(batch_size, input_dim)\n","\n","model = CaptioningRNN(word_to_idx,\n","          input_dim=input_dim,\n","          wordvec_dim=wordvec_dim,\n","          hidden_dim=hidden_dim,\n","          cell_type='rnn',\n","          dtype=np.float64,\n","        )\n","\n","loss, grads = model.loss(features, captions)\n","\n","for param_name in sorted(grads):\n","    f = lambda _: model.loss(features, captions)[0]\n","    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n","    e = rel_error(param_grad_num, grads[param_name])\n","    print('%s relative error: %e' % (param_name, e))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eZ-bdIAR-IeY","colab_type":"text"},"source":["# Overfit small data\n","Similar to the `Solver` class that we used to train image classification models on the previous assignment, on this assignment we use a `CaptioningSolver` class to train image captioning models. Open the file `cs231n/captioning_solver.py` and read through the `CaptioningSolver` class; it should look very familiar.\n","\n","Once you have familiarized yourself with the API, run the following to make sure your model overfits a small sample of 100 training examples. You should see a final loss of less than 0.1."]},{"cell_type":"code","metadata":{"id":"INgOR8Gi-IeZ","colab_type":"code","colab":{}},"source":["np.random.seed(231)\n","\n","small_data = load_coco_data(max_train=50)\n","\n","small_rnn_model = CaptioningRNN(\n","          cell_type='rnn',\n","          word_to_idx=data['word_to_idx'],\n","          input_dim=data['train_features'].shape[1],\n","          hidden_dim=512,\n","          wordvec_dim=256,\n","        )\n","\n","small_rnn_solver = CaptioningSolver(small_rnn_model, small_data,\n","           update_rule='adam',\n","           num_epochs=50,\n","           batch_size=25,\n","           optim_config={\n","             'learning_rate': 5e-3,\n","           },\n","           lr_decay=0.95,\n","           verbose=True, print_every=10,\n","         )\n","\n","small_rnn_solver.train()\n","\n","# Plot the training losses\n","plt.plot(small_rnn_solver.loss_history)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Training loss history')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GG8eaKZm-Iea","colab_type":"text"},"source":["# Test-time sampling\n","Unlike classification models, image captioning models behave very differently at training time and at test time. At training time, we have access to the ground-truth caption, so we feed ground-truth words as input to the RNN at each timestep. At test time, we sample from the distribution over the vocabulary at each timestep, and feed the sample as input to the RNN at the next timestep.\n","\n","In the file `cs231n/classifiers/rnn.py`, implement the `sample` method for test-time sampling. After doing so, run the following to sample from your overfitted model on both training and validation data. The samples on training data should be very good; the samples on validation data probably won't make sense."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"E9WDSt7a-Ieb","colab_type":"code","colab":{}},"source":["for split in ['train', 'val']:\n","    minibatch = sample_coco_minibatch(small_data, split=split, batch_size=2)\n","    gt_captions, features, urls = minibatch\n","    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n","\n","    sample_captions = small_rnn_model.sample(features)\n","    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n","\n","    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n","        plt.imshow(image_from_url(url))\n","        plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n","        plt.axis('off')\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"tags":["pdf-inline"],"id":"9voLioRI-Ief","colab_type":"text"},"source":["# INLINE QUESTION 1\n","\n","In our current image captioning setup, our RNN language model produces a word at every timestep as its output. However, an alternate way to pose the problem is to train the network to operate over _characters_ (e.g. 'a', 'b', etc.) as opposed to words, so that at it every timestep, it receives the previous character as input and tries to predict the next character in the sequence. For example, the network might generate a caption like\n","\n","'A', ' ', 'c', 'a', 't', ' ', 'o', 'n', ' ', 'a', ' ', 'b', 'e', 'd'\n","\n","Can you describe one advantage of an image-captioning model that uses a character-level RNN? Can you also describe one disadvantage? HINT: there are several valid answers, but it might be useful to compare the parameter space of word-level and character-level models.\n","\n","**Your Answer:** \n","\n"]}]}