{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Batch Normalization\n",
    "One way to make deep networks easier to train is to use more sophisticated optimization procedures such as SGD+momentum, RMSProp, or Adam. Another strategy is to change the architecture of the network to make it easier to train. \n",
    "One idea along these lines is batch normalization which was proposed by [1] in 2015.\n",
    "\n",
    "The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However, even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.\n",
    "\n",
    "The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, [1] proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.\n",
    "\n",
    "It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.\n",
    "\n",
    "[1] [Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "Internal Covariate Shift\", ICML 2015.](https://arxiv.org/abs/1502.03167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def print_mean_std(x,axis=0):\n",
    "    print('  means: ', x.mean(axis=axis))\n",
    "    print('  stds:  ', x.std(axis=axis))\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization: forward\n",
    "In the file `cs231n/layers.py`, implement the batch normalization forward pass in the function `batchnorm_forward`. Once you have done so, run the following to test your implementation.\n",
    "\n",
    "Referencing the paper linked to above in [1] may be helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [ -2.3814598  -13.18038246   1.91780462]\n",
      "  stds:   [27.18502186 34.21455511 37.68611762]\n",
      "\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  means:  [ 1.55431223e-17  7.88258347e-17 -3.40005801e-18]\n",
      "  stds:   [0.99749686 0.99749686 0.99749686]\n",
      "\n",
      "After batch normalization (gamma= [1. 2. 3.] , beta= [11. 12. 13.] )\n",
      "  means:  [11. 12. 13.]\n",
      "  stds:   [0.99749686 1.99499373 2.99249059]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization   \n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print_mean_std(a,axis=0)\n",
    "\n",
    "gamma = np.ones((D3,))\n",
    "beta = np.zeros((D3,))\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)\n",
    "\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "print('After batch normalization (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [-0.03917523 -0.04338266 -0.10426524]\n",
      "  stds:   [1.01277281 1.0098496  0.97575131]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "\n",
    "for t in range(50):\n",
    "  X = np.random.randn(N, D1)\n",
    "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "  batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization: backward\n",
    "Now implement the backward pass for batch normalization in the function `batchnorm_backward`.\n",
    "\n",
    "To derive the backward pass you should write out the computation graph for batch normalization and backprop through each of the intermediate nodes. Some intermediates may have multiple outgoing branches; make sure to sum gradients across these branches in the backward pass.\n",
    "\n",
    "Once you have finished, run the following to numerically check your backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  1.6934271864958244e-09\n",
      "dgamma error:  1.1188362943000848e-12\n",
      "dbeta error:  2.379446949959628e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, a, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, b, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "#You should expect to see relative errors between 1e-13 and 1e-8\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization: alternative backward\n",
    "In class we talked about two different implementations for the sigmoid backward pass. One strategy is to write out a computation graph composed of simple operations and backprop through all intermediate values. Another strategy is to work out the derivatives on paper. For example, you can derive a very simple formula for the sigmoid function's backward pass by simplifying gradients on paper.\n",
    "\n",
    "Surprisingly, it turns out that you can do a similar simplification for the batch normalization backward pass too!  \n",
    "\n",
    "In the forward pass, given a set of inputs $X=\\begin{bmatrix}x_1\\\\x_2\\\\...\\\\x_N\\end{bmatrix}$, \n",
    "\n",
    "we first calculate the mean $\\mu$ and variance $v$.\n",
    "With $\\mu$ and $v$ calculated, we can calculate the standard deviation $\\sigma$  and normalized data $Y$.\n",
    "The equations and graph illustration below describe the computation ($y_i$ is the i-th element of the vector $Y$).\n",
    "\n",
    "\\begin{align}\n",
    "& \\mu=\\frac{1}{N}\\sum_{k=1}^N x_k  &  v=\\frac{1}{N}\\sum_{k=1}^N (x_k-\\mu)^2 \\\\\n",
    "& \\sigma=\\sqrt{v+\\epsilon}         &  y_i=\\frac{x_i-\\mu}{\\sigma}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"notebook_images/batchnorm_graph.png\" width=691 height=202>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "The meat of our problem during backpropagation is to compute $\\frac{\\partial L}{\\partial X}$, given the upstream gradient we receive, $\\frac{\\partial L}{\\partial Y}.$ To do this, recall the chain rule in calculus gives us $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot \\frac{\\partial Y}{\\partial X}$.\n",
    "\n",
    "The unknown/hart part is $\\frac{\\partial Y}{\\partial X}$. We can find this by first deriving step-by-step our local gradients at \n",
    "$\\frac{\\partial v}{\\partial X}$, $\\frac{\\partial \\mu}{\\partial X}$,\n",
    "$\\frac{\\partial \\sigma}{\\partial v}$, \n",
    "$\\frac{\\partial Y}{\\partial \\sigma}$, and $\\frac{\\partial Y}{\\partial \\mu}$,\n",
    "and then use the chain rule to compose these gradients (which appear in the form of vectors!) appropriately to compute $\\frac{\\partial Y}{\\partial X}$.\n",
    "\n",
    "If it's challenging to directly reason about the gradients over $X$ and $Y$ which require matrix multiplication, try reasoning about the gradients in terms of individual elements $x_i$ and $y_i$ first: in that case, you will need to come up with the derivations for $\\frac{\\partial L}{\\partial x_i}$, by relying on the Chain Rule to first calculate the intermediate $\\frac{\\partial \\mu}{\\partial x_i}, \\frac{\\partial v}{\\partial x_i}, \\frac{\\partial \\sigma}{\\partial x_i},$ then assemble these pieces to calculate $\\frac{\\partial y_i}{\\partial x_i}$. \n",
    "\n",
    "You should make sure each of the intermediary gradient derivations are all as simplified as possible, for ease of implementation. \n",
    "\n",
    "After doing so, implement the simplified batch normalization backward pass in the function `batchnorm_backward_alt` and compare the two implementations by running the following. Your two implementations should compute nearly identical results, but the alternative implementation should be a bit faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx difference:  2.263198472648062e-12\n",
      "dgamma difference:  0.0\n",
      "dbeta difference:  0.0\n",
      "speedup: 3.92x\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D = 100, 500\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "t1 = time.time()\n",
    "dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n",
    "t2 = time.time()\n",
    "dx2, dgamma2, dbeta2 = batchnorm_backward_alt(dout, cache)\n",
    "t3 = time.time()\n",
    "\n",
    "print('dx difference: ', rel_error(dx1, dx2))\n",
    "print('dgamma difference: ', rel_error(dgamma1, dgamma2))\n",
    "print('dbeta difference: ', rel_error(dbeta1, dbeta2))\n",
    "print('speedup: %.2fx' % ((t2 - t1) / (t3 - t2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Nets with Batch Normalization\n",
    "Now that you have a working implementation for batch normalization, go back to your `FullyConnectedNet` in the file `cs231n/classifiers/fc_net.py`. Modify your implementation to add batch normalization.\n",
    "\n",
    "Concretely, when the `normalization` flag is set to `\"batchnorm\"` in the constructor, you should insert a batch normalization layer before each ReLU nonlinearity. The outputs from the last layer of the network should not be normalized. Once you are done, run the following to gradient-check your implementation.\n",
    "\n",
    "HINT: You might find it useful to define an additional helper layer similar to those in the file `cs231n/layer_utils.py`. If you decide to do so, do it in the file `cs231n/classifiers/fc_net.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  2.2621133407844862\n",
      "(0, 0) 0.9960137235864862\n",
      "(0, 1) 0.0002697220891079155\n",
      "(0, 2) -0.0004630628769319855\n",
      "(0, 3) 2.1356116874926553e-05\n",
      "(0, 4) 4.598987857207248e-07\n",
      "(0, 5) 2.1698642882483906e-05\n",
      "(0, 6) -3.6962988225752724e-05\n",
      "(0, 7) 0.000147555478946515\n",
      "(0, 8) -6.046851908081407e-06\n",
      "(0, 9) 5.962097482381522e-06\n",
      "(0, 10) 1.3452883251829915e-05\n",
      "(0, 11) -3.376312562863859e-05\n",
      "(0, 12) -5.469180663908445e-06\n",
      "(0, 13) 7.0384142958346266e-06\n",
      "(0, 14) -3.461919639846655e-06\n",
      "(0, 15) 5.506461953075358e-06\n",
      "(0, 16) 3.2984770470534386e-05\n",
      "(0, 17) -0.4021242400842339\n",
      "(0, 18) -9.178378057583812e-05\n",
      "(0, 19) 9.337730588754312e-06\n",
      "(1, 0) -14.422934803515195\n",
      "(1, 1) -0.003905711487561802\n",
      "(1, 2) 0.006705383670535524\n",
      "(1, 3) -0.00030924696137191177\n",
      "(1, 4) -6.659917062279418e-06\n",
      "(1, 5) -0.00031420683832550367\n",
      "(1, 6) 0.000535242850041584\n",
      "(1, 7) -0.0021366772129383094\n",
      "(1, 8) 8.756129155074176e-05\n",
      "(1, 9) -8.633391779255815e-05\n",
      "(1, 10) -0.0001948048611311037\n",
      "(1, 11) 0.0004889070481794988\n",
      "(1, 12) 7.919680466983436e-05\n",
      "(1, 13) -0.00010191971711037694\n",
      "(1, 14) 5.0130588569174954e-05\n",
      "(1, 15) -7.97366395133281e-05\n",
      "(1, 16) -0.0004776357531710573\n",
      "(1, 17) 5.822988155124164\n",
      "(1, 18) 0.0013290754274919434\n",
      "(1, 19) -0.00013521517239212244\n",
      "(2, 0) 24.973133876637906\n",
      "(2, 1) 0.006762544257910007\n",
      "(2, 2) -0.011610039418741279\n",
      "(2, 3) 0.0005354454879480386\n",
      "(2, 4) 1.1531287036348202e-05\n",
      "(2, 5) 0.0005440333072925796\n",
      "(2, 6) -0.0009267459688544476\n",
      "(2, 7) 0.003699549000479862\n",
      "(2, 8) -0.00015160797062208076\n",
      "(2, 9) 0.0001494827372994223\n",
      "(2, 10) 0.00033729481430100344\n",
      "(2, 11) -0.0008465179002214994\n",
      "(2, 12) -0.00013712531110599002\n",
      "(2, 13) 0.00017646883954114398\n",
      "(2, 14) -8.679859053728477e-05\n",
      "(2, 15) 0.00013806000787042194\n",
      "(2, 16) 0.000827002244463415\n",
      "(2, 17) -10.082308139414131\n",
      "(2, 18) -0.0023012270977673666\n",
      "(2, 19) 0.00023411828031782986\n",
      "(3, 0) 7.777740280912048\n",
      "(3, 1) 0.0021062179333952713\n",
      "(3, 2) -0.003615986043925545\n",
      "(3, 3) 0.0001667664228932608\n",
      "(3, 4) 3.5915048712809035e-06\n",
      "(3, 5) 0.00016944114999972723\n",
      "(3, 6) -0.00028863835765946533\n",
      "(3, 7) 0.0011522377718620191\n",
      "(3, 8) -4.721885105141154e-05\n",
      "(3, 9) 4.655698049305101e-05\n",
      "(3, 10) 0.0001050516562628445\n",
      "(3, 11) -0.00026365103433789727\n",
      "(3, 12) -4.2708148129122485e-05\n",
      "(3, 13) 5.496187949205477e-05\n",
      "(3, 14) -2.703375301393862e-05\n",
      "(3, 15) 4.2999270810639694e-05\n",
      "(3, 16) 0.00025757282973160045\n",
      "(3, 17) -3.140129809509062\n",
      "(3, 18) -0.0007167254345574746\n",
      "(3, 19) 7.291698356226561e-05\n",
      "(4, 0) 9.948310596552545\n",
      "(4, 1) 0.0026940045749768156\n",
      "(4, 2) -0.00462510696408458\n",
      "(4, 3) 0.0002133062615428116\n",
      "(4, 4) 4.593725400070525e-06\n",
      "(4, 5) 0.00021672741379319402\n",
      "(4, 6) -0.00036918925694351396\n",
      "(4, 7) 0.0014737951303800398\n",
      "(4, 8) -6.0396287970831957e-05\n",
      "(4, 9) 5.954969850563429e-05\n",
      "(4, 10) 0.00013436864954030625\n",
      "(4, 11) -0.0003372286450087358\n",
      "(4, 12) -5.462683638768339e-05\n",
      "(4, 13) 7.030016568876363e-05\n",
      "(4, 14) -3.457809594209493e-05\n",
      "(4, 15) 5.499916078122168e-05\n",
      "(4, 16) 0.0003294541750520352\n",
      "(4, 17) -4.0164559254485255\n",
      "(4, 18) -0.0009167434589230082\n",
      "(4, 19) 9.326606154047566e-05\n",
      "(5, 0) 7.562023614515744\n",
      "(5, 1) 0.002047802039051305\n",
      "(5, 2) -0.003515696778144672\n",
      "(5, 3) 0.00016214116715929094\n",
      "(5, 4) 3.4918512525905494e-06\n",
      "(5, 5) 0.00016474168695879143\n",
      "(5, 6) -0.0002806329835181032\n",
      "(5, 7) 0.0011202804683918544\n",
      "(5, 8) -4.5909231971563706e-05\n",
      "(5, 9) 4.526570229756998e-05\n",
      "(5, 10) 0.0001021380979793207\n",
      "(5, 11) -0.00025633868361296663\n",
      "(5, 12) -4.1523651184149906e-05\n",
      "(5, 13) 5.343747666586295e-05\n",
      "(5, 14) -2.6283952792027773e-05\n",
      "(5, 15) 4.1806713646508335e-05\n",
      "(5, 16) 0.0002504290108618079\n",
      "(5, 17) -3.0530381888604903\n",
      "(5, 18) -0.0006968470467327846\n",
      "(5, 19) 7.089462350506892e-05\n",
      "(6, 0) 3.8443528225684527\n",
      "(6, 1) 0.0010410561301910093\n",
      "(6, 2) -0.0017873005075941248\n",
      "(6, 3) 8.242886373466263e-05\n",
      "(6, 4) 1.7751800029941476e-06\n",
      "(6, 5) 8.375093951684674e-05\n",
      "(6, 6) -0.00014266743342261634\n",
      "(6, 7) 0.0005695252269788398\n",
      "(6, 8) -2.3339197241512007e-05\n",
      "(6, 9) 2.301205892507596e-05\n",
      "(6, 10) 5.192466456804822e-05\n",
      "(6, 11) -0.0001303167573851738\n",
      "(6, 12) -2.1109691772380753e-05\n",
      "(6, 13) 2.7166402460920832e-05\n",
      "(6, 14) -1.336217803071804e-05\n",
      "(6, 15) 2.1253576676372173e-05\n",
      "(6, 16) 0.00012731244947161713\n",
      "(6, 17) -1.552093900869522\n",
      "(6, 18) -0.0003542612425633251\n",
      "(6, 19) 3.604119225286695e-05\n",
      "(7, 0) -16.04081525732326\n",
      "(7, 1) -0.004343820125818354\n",
      "(7, 2) 0.007457535367016987\n",
      "(7, 3) -0.0003439355911183383\n",
      "(7, 4) -7.4069639310891935e-06\n",
      "(7, 5) -0.0003494518008295699\n",
      "(7, 6) 0.0005952816684029472\n",
      "(7, 7) -0.002376350982657982\n",
      "(7, 8) 9.738314599161411e-05\n",
      "(7, 9) -9.60180823739165e-05\n",
      "(7, 10) -0.000216656359519618\n",
      "(7, 11) 0.0005437483796555398\n",
      "(7, 12) 8.808043183705648e-05\n",
      "(7, 13) -0.00011335217209307301\n",
      "(7, 14) 5.5753801575519894e-05\n",
      "(7, 15) -8.868081824431327e-05\n",
      "(7, 16) -0.0005312127626666552\n",
      "(7, 17) 6.476167446534119\n",
      "(7, 18) 0.0014781594392943018\n",
      "(7, 19) -0.00015038239542519705\n",
      "(8, 0) 21.716341621536014\n",
      "(8, 1) 0.005880675191249906\n",
      "(8, 2) -0.010096032321271764\n",
      "(8, 3) 0.0004656207863718009\n",
      "(8, 4) 1.0027578767335399e-05\n",
      "(8, 5) 0.000473088745955863\n",
      "(8, 6) -0.0008058937961408218\n",
      "(8, 7) 0.003217109867392764\n",
      "(8, 8) -0.00013183754088430533\n",
      "(8, 9) 0.00012998948584197478\n",
      "(8, 10) 0.0002933099096935621\n",
      "(8, 11) -0.0007361278697715078\n",
      "(8, 12) -0.00011924352616432542\n",
      "(8, 13) 0.00015345644754916066\n",
      "(8, 14) -7.547962255216589e-05\n",
      "(8, 15) 0.0001200563204406535\n",
      "(8, 16) 0.0007191571560483111\n",
      "(8, 17) -8.767495283446003\n",
      "(8, 18) -0.002001135945839394\n",
      "(8, 19) 0.00020358810193243923\n",
      "(9, 0) 5.6770429445407435\n",
      "(9, 1) 0.001537349803015786\n",
      "(9, 2) -0.0026393447649653012\n",
      "(9, 3) 0.00012172449714853427\n",
      "(9, 4) 2.6214364012844267e-06\n",
      "(9, 5) 0.00012367680213287713\n",
      "(9, 6) -0.00021068005118252128\n",
      "(9, 7) 0.0008410300234018563\n",
      "(9, 8) -3.446549712293745e-05\n",
      "(9, 9) 3.398239467600206e-05\n",
      "(9, 10) 7.66782637384722e-05\n",
      "(9, 11) -0.00019244155158304463\n",
      "(9, 12) -3.117315294787204e-05\n",
      "(9, 13) 4.0117220656554764e-05\n",
      "(9, 14) -1.973219365680734e-05\n",
      "(9, 15) 3.138560522586431e-05\n",
      "(9, 16) 0.00018800498935434004\n",
      "(9, 17) -2.2920110558821705\n",
      "(9, 18) -0.000523145127218072\n",
      "(9, 19) 5.322284835074242e-05\n",
      "(10, 0) -4.246774682492571\n",
      "(10, 1) -0.0011500323582325223\n",
      "(10, 2) 0.0019743924495330134\n",
      "(10, 3) -9.105742826420736e-05\n",
      "(10, 4) -1.9610091328559065e-06\n",
      "(10, 5) -9.251786003972028e-05\n",
      "(10, 6) 0.0001576016872562036\n",
      "(10, 7) -0.0006291422716131478\n",
      "(10, 8) 2.5782331825041634e-05\n",
      "(10, 9) -2.5420954230526146e-05\n",
      "(10, 10) -5.736007224754757e-05\n",
      "(10, 11) 0.00014395813430212456\n",
      "(10, 12) 2.331943527167368e-05\n",
      "(10, 13) -3.00101499206562e-05\n",
      "(10, 14) 1.4760881406061797e-05\n",
      "(10, 15) -2.3478352595418525e-05\n",
      "(10, 16) -0.0001406393224101521\n",
      "(10, 17) 1.7145649088945445\n",
      "(10, 18) 0.0003913447788050916\n",
      "(10, 19) -3.981397433960865e-05\n",
      "(11, 0) -1.6324794922972783\n",
      "(11, 1) -0.0004420780186364936\n",
      "(11, 2) 0.0007589660233620065\n",
      "(11, 3) -3.50029116802375e-05\n",
      "(11, 4) -7.537970247994962e-07\n",
      "(11, 5) -3.556430705486946e-05\n",
      "(11, 6) 6.058287205235046e-05\n",
      "(11, 7) -0.00024184532154691849\n",
      "(11, 8) 9.91084991852631e-06\n",
      "(11, 9) -9.771916609224718e-06\n",
      "(11, 10) -2.2049473358265456e-05\n",
      "(11, 11) 5.533817848402122e-05\n",
      "(11, 12) 8.964118336507454e-06\n",
      "(11, 13) -1.153606099535409e-05\n",
      "(11, 14) 5.674150038714742e-06\n",
      "(11, 15) -9.025225011782823e-06\n",
      "(11, 16) -5.4062443410884946e-05\n",
      "(11, 17) 0.6590868450562226\n",
      "(11, 18) 0.00015043486456534083\n",
      "(11, 19) -1.5304690847983693e-05\n",
      "(12, 0) 1.6850309143778828\n",
      "(12, 1) 0.0004563090350018228\n",
      "(12, 2) -0.000783398013126657\n",
      "(12, 3) 3.612969923239007e-05\n",
      "(12, 4) 7.780887045782946e-07\n",
      "(12, 5) 3.670916903786292e-05\n",
      "(12, 6) -6.253308981740702e-05\n",
      "(12, 7) 0.00024963062728033947\n",
      "(12, 8) -1.0229905811343087e-05\n",
      "(12, 9) 1.008648720102201e-05\n",
      "(12, 10) 2.2759305551289796e-05\n",
      "(12, 11) -5.711959794041376e-05\n",
      "(12, 12) -9.252665300607532e-06\n",
      "(12, 13) 1.1907408392630712e-05\n",
      "(12, 14) -5.856826135186565e-06\n",
      "(12, 15) 9.315703763945749e-06\n",
      "(12, 16) 5.5802762410905864e-05\n",
      "(12, 17) -0.6803036170843767\n",
      "(12, 18) -0.00015527750196753232\n",
      "(12, 19) 1.5797363417391352e-05\n",
      "(13, 0) -6.375203527420047\n",
      "(13, 1) -0.0017264117246895692\n",
      "(13, 2) 0.0029639291021510412\n",
      "(13, 3) -0.00013669407827876512\n",
      "(13, 4) -2.9438229631750797e-06\n",
      "(13, 5) -0.0001388864800944134\n",
      "(13, 6) 0.00023658932590819856\n",
      "(13, 7) -0.0009444591331231321\n",
      "(13, 8) 3.8704039972969895e-05\n",
      "(13, 9) -3.8161518389756566e-05\n",
      "(13, 10) -8.610812063380989e-05\n",
      "(13, 11) 0.0002161078427320717\n",
      "(13, 12) 3.50067752563632e-05\n",
      "(13, 13) -4.5050807528923535e-05\n",
      "(13, 14) 2.215887473511202e-05\n",
      "(13, 15) -3.524540659327613e-05\n",
      "(13, 16) -0.00021112569470460582\n",
      "(13, 17) 2.573881062151173\n",
      "(13, 18) 0.0005874810860007074\n",
      "(13, 19) -5.976816819242003e-05\n",
      "(14, 0) -9.293356236739214\n",
      "(14, 1) -0.002516644448391503\n",
      "(14, 2) 0.0043206124678007995\n",
      "(14, 3) -0.00019926322814001193\n",
      "(14, 4) -4.291322852623125e-06\n",
      "(14, 5) -0.0002024591161386979\n",
      "(14, 6) 0.00034488361055906575\n",
      "(14, 7) -0.0013767676554365946\n",
      "(14, 8) 5.642011302597893e-05\n",
      "(14, 9) -5.562923455215695e-05\n",
      "(14, 10) -0.00012552250350239547\n",
      "(14, 11) 0.0003150272043228597\n",
      "(14, 12) 5.103046873955463e-05\n",
      "(14, 13) -6.567195676154824e-05\n",
      "(14, 14) 3.230165024348253e-05\n",
      "(14, 15) -5.137830161316969e-05\n",
      "(14, 16) -0.0003077645249760508\n",
      "(14, 17) 3.7520309908112277\n",
      "(14, 18) 0.000856389537062796\n",
      "(14, 19) -8.712586208048377e-05\n",
      "W1 relative error: 2.62e-05\n",
      "(0, 0) 0.0024581791713274015\n",
      "(0, 1) 0.0013311299174034728\n",
      "(0, 2) 0.0009562692859788057\n",
      "(0, 3) -2.903552953625876e-05\n",
      "(0, 4) 7.191311990339955e-05\n",
      "(0, 5) 1.884183919997895e-05\n",
      "(0, 6) -3.681324134419128e-05\n",
      "(0, 7) -0.0007352124020698624\n",
      "(0, 8) -0.0002347709582295465\n",
      "(0, 9) 0.9678013398684014\n",
      "(0, 10) 2.8916424810176973e-06\n",
      "(0, 11) -1.0616219014991655e-05\n",
      "(0, 12) 2.98081559435559e-06\n",
      "(0, 13) -6.322387058332878e-06\n",
      "(0, 14) 0.05053256020293161\n",
      "(0, 15) 9.319660598805511e-05\n",
      "(0, 16) -0.0001703561514077023\n",
      "(0, 17) -0.002517853214811794\n",
      "(0, 18) 5.983629147721103e-05\n",
      "(0, 19) -1.3597656334241036e-05\n",
      "(0, 20) -0.0002544221056055562\n",
      "(0, 21) 6.220508552701176e-05\n",
      "(0, 22) -5.784475121117793e-05\n",
      "(0, 23) -0.2995534330807459\n",
      "(0, 24) -0.0002939980925376062\n",
      "(0, 25) -8.570100185067986e-06\n",
      "(0, 26) -3.8783798395058966e-05\n",
      "(0, 27) 2.922269093375007e-05\n",
      "(0, 28) -0.0001314686137732224\n",
      "(0, 29) -1.873265986773731e-05\n",
      "(1, 0) -0.0035450423041538666\n",
      "(1, 1) -0.00191967770568624\n",
      "(1, 2) -0.0013790756314691064\n",
      "(1, 3) 4.187334923244634e-05\n",
      "(1, 4) -0.00010370890812794185\n",
      "(1, 5) -2.717259750539824e-05\n",
      "(1, 6) 5.30899102457738e-05\n",
      "(1, 7) 0.0010602803079962086\n",
      "(1, 8) 0.0003385729696603334\n",
      "(1, 9) -1.3957068716763386\n",
      "(1, 10) -4.170130907255043e-06\n",
      "(1, 11) 1.531008653188337e-05\n",
      "(1, 12) -4.298739142427621e-06\n",
      "(1, 13) 9.11781761203656e-06\n",
      "(1, 14) -0.07287511771814081\n",
      "(1, 15) -0.00013440268897824126\n",
      "(1, 16) 0.0002456777226100826\n",
      "(1, 17) 0.0036311007756140152\n",
      "(1, 18) -8.629241765589767e-05\n",
      "(1, 19) 1.9609736057191185e-05\n",
      "(1, 20) 0.0003669127002225991\n",
      "(1, 21) -8.970857390266927e-05\n",
      "(1, 22) 8.342027069119239e-05\n",
      "(1, 23) 0.4319986040890455\n",
      "(1, 24) 0.0004239868900413057\n",
      "(1, 25) 1.2359291368113643e-05\n",
      "(1, 26) 5.593172591744632e-05\n",
      "(1, 27) -4.2143266654193205e-05\n",
      "(1, 28) 0.00018959640524229823\n",
      "(1, 29) 2.70151456760459e-05\n",
      "(2, 0) 0.003536706660867139\n",
      "(2, 1) 0.001915163894139482\n",
      "(2, 2) 0.0013758329586721627\n",
      "(2, 3) -4.1774872450162086e-05\n",
      "(2, 4) 0.00010346505874281318\n",
      "(2, 5) 2.71087374770218e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6) -5.296509897334544e-05\n",
      "(2, 7) -0.0010577872133765709\n",
      "(2, 8) -0.0003377768509338352\n",
      "(2, 9) 1.3924250674257619\n",
      "(2, 10) 4.160360944638342e-06\n",
      "(2, 11) -1.527407089696453e-05\n",
      "(2, 12) 4.288636112903532e-06\n",
      "(2, 13) -9.096368103200803e-06\n",
      "(2, 14) 0.07270376241041987\n",
      "(2, 15) 0.00013408665289205146\n",
      "(2, 16) -0.00024510000695698864\n",
      "(2, 17) -0.003622562760874359\n",
      "(2, 18) 8.608951329591717e-05\n",
      "(2, 19) -1.956363959720875e-05\n",
      "(2, 20) -0.00036604996811462337\n",
      "(2, 21) 8.949760932353e-05\n",
      "(2, 22) -8.32241386916621e-05\n",
      "(2, 23) -0.43098282103137814\n",
      "(2, 24) -0.0004229899541741133\n",
      "(2, 25) -1.2330225729328957e-05\n",
      "(2, 26) -5.5800208897949226e-05\n",
      "(2, 27) 4.2044168147015164e-05\n",
      "(2, 28) -0.00018915058408452976\n",
      "(2, 29) -2.695161871457685e-05\n",
      "(3, 0) 0.0035554310828800335\n",
      "(3, 1) 0.001925303361183239\n",
      "(3, 2) 0.0013831170431188864\n",
      "(3, 3) -4.199602887666742e-05\n",
      "(3, 4) 0.00010401284278316324\n",
      "(3, 5) 2.725224490518485e-05\n",
      "(3, 6) -5.324551910490526e-05\n",
      "(3, 7) -0.001063387489175227\n",
      "(3, 8) -0.0003395651315685199\n",
      "(3, 9) 1.3997970116363943\n",
      "(3, 10) 4.182365564986412e-06\n",
      "(3, 11) -1.535496174653872e-05\n",
      "(3, 12) 4.311351275987363e-06\n",
      "(3, 13) -9.144485169088057e-06\n",
      "(3, 14) 0.07308867928657037\n",
      "(3, 15) 0.00013479657390291777\n",
      "(3, 16) -0.000246397657832631\n",
      "(3, 17) -0.003641741752602456\n",
      "(3, 18) 8.654530425644678e-05\n",
      "(3, 19) -1.9667201200945783e-05\n",
      "(3, 20) -0.00036798795122194855\n",
      "(3, 21) 8.997147471490051e-05\n",
      "(3, 22) -8.366476400567534e-05\n",
      "(3, 23) -0.43326458298853504\n",
      "(3, 24) -0.0004252293628326242\n",
      "(3, 25) -1.2395484638716424e-05\n",
      "(3, 26) -5.609566144926247e-05\n",
      "(3, 27) 4.226676786345251e-05\n",
      "(3, 28) -0.00019015200525274165\n",
      "(3, 29) -2.7094282373241182e-05\n",
      "(4, 0) -0.003555907701624505\n",
      "(4, 1) -0.001925561465832004\n",
      "(4, 2) -0.0013833024281595383\n",
      "(4, 3) 4.2001691014093005e-05\n",
      "(4, 4) -0.00010402676497989204\n",
      "(4, 5) -2.725588643670562e-05\n",
      "(4, 6) 5.325264673672336e-05\n",
      "(4, 7) 0.0010635300196071285\n",
      "(4, 8) 0.00033961067291699004\n",
      "(4, 9) -1.3999846486090204\n",
      "(4, 10) -4.182920676498725e-06\n",
      "(4, 11) 1.535700455690403e-05\n",
      "(4, 12) -4.3119063874996755e-06\n",
      "(4, 13) 9.145728618875637e-06\n",
      "(4, 14) -0.07309847653846901\n",
      "(4, 15) -0.00013481460392483768\n",
      "(4, 16) 0.00024643069806984386\n",
      "(4, 17) 0.0036422298954619232\n",
      "(4, 18) -8.655687278036338e-05\n",
      "(4, 19) 1.966984353174439e-05\n",
      "(4, 20) 0.0003680372673287024\n",
      "(4, 21) -8.998350953248745e-05\n",
      "(4, 22) 8.367595505376356e-05\n",
      "(4, 23) 0.4333226603314699\n",
      "(4, 24) 0.00042528636168270845\n",
      "(4, 25) 1.2397149973253361e-05\n",
      "(4, 26) 5.6103144352448446e-05\n",
      "(4, 27) -4.2272430000878096e-05\n",
      "(4, 28) 0.00019017747376892655\n",
      "(4, 29) 2.7097923904761952e-05\n",
      "(5, 0) -0.0035550375754311854\n",
      "(5, 1) -0.0019250902649758925\n",
      "(5, 2) -0.001382963965568251\n",
      "(5, 3) 4.1991388144424484e-05\n",
      "(5, 4) -0.00010400131866816763\n",
      "(5, 5) -2.724922509855787e-05\n",
      "(5, 6) 5.3239590513953765e-05\n",
      "(5, 7) 0.0010632698055346168\n",
      "(5, 8) 0.0003395275616213666\n",
      "(5, 9) -1.3996420812345531\n",
      "(5, 10) -4.18189927131607e-06\n",
      "(5, 11) 1.535327420754129e-05\n",
      "(5, 12) -4.310840573396035e-06\n",
      "(5, 13) 9.143485968365894e-06\n",
      "(5, 14) -0.07308058982413712\n",
      "(5, 15) -0.0001347816525054668\n",
      "(5, 16) 0.0002463703907551462\n",
      "(5, 17) 0.0036413386750311356\n",
      "(5, 18) -8.653571192951402e-05\n",
      "(5, 19) 1.9664980754896533e-05\n",
      "(5, 20) 0.0003679471838324843\n",
      "(5, 21) -8.996148270767888e-05\n",
      "(5, 22) 8.365550474564996e-05\n",
      "(5, 23) 0.43321662899220564\n",
      "(5, 24) 0.0004251823115808406\n",
      "(5, 25) 1.2394130166626381e-05\n",
      "(5, 26) 5.608944420032457e-05\n",
      "(5, 27) -4.226210492674908e-05\n",
      "(5, 28) 0.00019013095542419475\n",
      "(5, 29) 2.7091284771074694e-05\n",
      "(6, 0) 0.00355524292228182\n",
      "(6, 1) 0.0019252014871184995\n",
      "(6, 2) 0.0013830438350126426\n",
      "(6, 3) -4.199383063507866e-05\n",
      "(6, 4) 0.0001040073138725006\n",
      "(6, 5) 2.7250801615252836e-05\n",
      "(6, 6) -5.3242699138422715e-05\n",
      "(6, 7) -0.001063331223072339\n",
      "(6, 8) -0.000339547190364442\n",
      "(6, 9) 1.3997229323603475\n",
      "(6, 10) 4.182143520381487e-06\n",
      "(6, 11) -1.5354117977040005e-05\n",
      "(6, 12) 4.311107026921945e-06\n",
      "(6, 13) -9.144018875417714e-06\n",
      "(6, 14) 0.07308481140277934\n",
      "(6, 15) 0.00013478942406663919\n",
      "(6, 16) -0.0002463846016098614\n",
      "(6, 17) -0.003641549017885381\n",
      "(6, 18) 8.654068572866434e-05\n",
      "(6, 19) -1.9666135386842143e-05\n",
      "(6, 20) -0.0003679684779100966\n",
      "(6, 21) 8.996670075589462e-05\n",
      "(6, 22) -8.366032311357684e-05\n",
      "(6, 23) -0.43324165401870113\n",
      "(6, 24) -0.0004252068697141453\n",
      "(6, 25) -1.2394840709362141e-05\n",
      "(6, 26) -5.6092663847095985e-05\n",
      "(6, 27) 4.226454741740326e-05\n",
      "(6, 28) -0.00019014194663213854\n",
      "(6, 29) -2.709286128776966e-05\n",
      "(7, 0) 0.0035533213704752593\n",
      "(7, 1) 0.0019241609194864393\n",
      "(7, 2) 0.001382296299645702\n",
      "(7, 3) -4.197113767645532e-05\n",
      "(7, 4) 0.00010395111438299408\n",
      "(7, 5) 2.7236057853485814e-05\n",
      "(7, 6) -5.321389995316394e-05\n",
      "(7, 7) -0.0010627564828169511\n",
      "(7, 8) -0.00033936364829401095\n",
      "(7, 9) 1.3989663933733352\n",
      "(7, 10) 4.179878665411252e-06\n",
      "(7, 11) -1.534581350881581e-05\n",
      "(7, 12) 4.3087755585702325e-06\n",
      "(7, 13) -9.139089485188379e-06\n",
      "(7, 14) 0.07304530960094979\n",
      "(7, 15) 0.00013471657123176328\n",
      "(7, 16) -0.0002462514414602879\n",
      "(7, 17) -0.0036395808145073256\n",
      "(7, 18) 8.649394533932762e-05\n",
      "(7, 19) -1.9655521654726726e-05\n",
      "(7, 20) -0.0003677695703530048\n",
      "(7, 21) 8.991802857849505e-05\n",
      "(7, 22) -8.36151148320141e-05\n",
      "(7, 23) -0.4330074902636482\n",
      "(7, 24) -0.0004249770535480479\n",
      "(7, 25) -1.2388157166753898e-05\n",
      "(7, 26) -5.606235475852372e-05\n",
      "(7, 27) 4.224167682309598e-05\n",
      "(7, 28) -0.00019003913998005825\n",
      "(7, 29) -2.707820634384461e-05\n",
      "(8, 0) -0.003554663896565557\n",
      "(8, 1) -0.0019248879379318848\n",
      "(8, 2) -0.0013828185929654067\n",
      "(8, 3) 4.1986969456786476e-05\n",
      "(8, 4) -0.00010399037186914482\n",
      "(8, 5) -2.7246338518693843e-05\n",
      "(8, 6) 5.323401719437015e-05\n",
      "(8, 7) 0.001063158050484958\n",
      "(8, 8) 0.00033949185684889466\n",
      "(8, 9) -1.3994949607631921\n",
      "(8, 10) -4.18145518210622e-06\n",
      "(8, 11) 1.5351608873004352e-05\n",
      "(8, 12) -4.310396484186185e-06\n",
      "(8, 13) 9.142553381025209e-06\n",
      "(8, 14) -0.0730729081022119\n",
      "(8, 15) -0.0001347674638552121\n",
      "(8, 16) 0.00024634450035421196\n",
      "(8, 17) 0.003640955914541166\n",
      "(8, 18) -8.65266081007121e-05\n",
      "(8, 19) 1.9662937944531222e-05\n",
      "(8, 20) 0.00036790854807122736\n",
      "(8, 21) -8.995204581196957e-05\n",
      "(8, 22) 8.364671177929493e-05\n",
      "(8, 23) 0.4331710923288767\n",
      "(8, 24) 0.00042513761400186917\n",
      "(8, 25) 1.239279789899683e-05\n",
      "(8, 26) 5.6083537813833566e-05\n",
      "(8, 27) -4.225764183019009e-05\n",
      "(8, 28) 0.0001901109714097515\n",
      "(8, 29) 2.7088442600131654e-05\n",
      "(9, 0) 0.003541356274894269\n",
      "(9, 1) 0.0019176817023236479\n",
      "(9, 2) 0.001377641734023882\n",
      "(9, 3) -4.182980628542054e-05\n",
      "(9, 4) 0.00010360106106332977\n",
      "(9, 5) 2.7144353431651776e-05\n",
      "(9, 6) -5.303473216144993e-05\n",
      "(9, 7) -0.0010591778787372164\n",
      "(9, 8) -0.00033822091793922476\n",
      "(9, 9) 1.394255654707521\n",
      "(9, 10) 4.165778832998512e-06\n",
      "(9, 11) -1.5294143729249754e-05\n",
      "(9, 12) 4.294276045868628e-06\n",
      "(9, 13) -9.10831410294577e-06\n",
      "(9, 14) 0.07279934428616741\n",
      "(9, 15) 0.00013426293410390144\n",
      "(9, 16) -0.00024542226029211633\n",
      "(9, 17) -0.003627325217969712\n",
      "(9, 18) 8.620266722658697e-05\n",
      "(9, 19) -1.9589307953538082e-05\n",
      "(9, 20) -0.0003665312053868774\n",
      "(9, 21) 8.961529296414027e-05\n",
      "(9, 22) -8.333356227296916e-05\n",
      "(9, 23) -0.4315494238626982\n",
      "(9, 24) -0.00042354599827376655\n",
      "(9, 25) -1.2346434985488484e-05\n",
      "(9, 26) -5.5873572435416456e-05\n",
      "(9, 27) 4.209947945810199e-05\n",
      "(9, 28) -0.0001893992518375853\n",
      "(9, 29) -2.6987034829062392e-05\n",
      "(10, 0) 0.0035540657750132705\n",
      "(10, 1) 0.0019245640636711412\n",
      "(10, 2) 0.0013825859124239057\n",
      "(10, 3) -4.197993064281035e-05\n",
      "(10, 4) 0.00010397287475427673\n",
      "(10, 5) 2.7241764399832388e-05\n",
      "(10, 6) -5.3225046592331175e-05\n",
      "(10, 7) -0.0010629791269423094\n",
      "(10, 8) -0.00033943476918096843\n",
      "(10, 9) 1.3992594781520038\n",
      "(10, 10) 4.1807446393704595e-06\n",
      "(10, 11) -1.5349055360047714e-05\n",
      "(10, 12) 4.309663736989933e-06\n",
      "(10, 13) -9.140999068790734e-06\n",
      "(10, 14) 0.07306061267087216\n",
      "(10, 15) 0.00013474481530550975\n",
      "(10, 16) -0.00024630304462647246\n",
      "(10, 17) -0.003640343271271717\n",
      "(10, 18) 8.651206417908951e-05\n",
      "(10, 19) -1.965962947991784e-05\n",
      "(10, 20) -0.00036784664203537426\n",
      "(10, 21) 8.993692457437417e-05\n",
      "(10, 22) -8.363263415134269e-05\n",
      "(10, 23) -0.4330982058098342\n",
      "(10, 24) -0.0004250660712301623\n",
      "(10, 25) -1.239075508863152e-05\n",
      "(10, 26) -5.607410091812425e-05\n",
      "(10, 27) 4.225055860729298e-05\n",
      "(10, 28) -0.0001900789747821818\n",
      "(10, 29) -2.708389068573069e-05\n",
      "(11, 0) -0.0035538917586563907\n",
      "(11, 1) -0.0019244698057363505\n",
      "(11, 2) -0.001382518211023864\n",
      "(11, 3) 4.197788783244504e-05\n",
      "(11, 4) -0.00010396781213728444\n",
      "(11, 5) -2.7240432132202837e-05\n",
      "(11, 6) 5.322244867045355e-05\n",
      "(11, 7) 0.001062927079686915\n",
      "(11, 8) 0.00033941811583559906\n",
      "(11, 9) -1.3991909689092805\n",
      "(11, 10) -4.180544799226027e-06\n",
      "(11, 11) 1.534830040839097e-05\n",
      "(11, 12) -4.3094638968455e-06\n",
      "(11, 13) 9.140554979580884e-06\n",
      "(11, 14) -0.07305703555449128\n",
      "(11, 15) -0.00013473819837628298\n",
      "(11, 16) 0.000246290987604425\n",
      "(11, 17) 0.0036401650138628834\n",
      "(11, 18) -8.650782312713544e-05\n",
      "(11, 19) 1.9658674688116662e-05\n",
      "(11, 20) 0.00036782861201345435\n",
      "(11, 21) -8.993250588673617e-05\n",
      "(11, 22) 8.362852632615157e-05\n",
      "(11, 23) 0.43307700083872186\n",
      "(11, 24) 0.00042504524344622036\n",
      "(11, 25) 1.239013336373773e-05\n",
      "(11, 26) 5.607134756502318e-05\n",
      "(11, 27) -4.2248471388006685e-05\n",
      "(11, 28) 0.00019006969331769594\n",
      "(11, 29) 2.708255841810114e-05\n",
      "(12, 0) -0.003555478755856711\n",
      "(12, 1) -0.0019253291627663314\n",
      "(12, 2) -0.0013831355838433976\n",
      "(12, 3) 4.199660619264022e-05\n",
      "(12, 4) -0.00010401421945971377\n",
      "(12, 5) -2.725260017655273e-05\n",
      "(12, 6) 5.324620744318053e-05\n",
      "(12, 7) 0.0010634017222344028\n",
      "(12, 8) 0.0003395696834829209\n",
      "(12, 9) -1.3998157719852242\n",
      "(12, 10) -4.182409973907397e-06\n",
      "(12, 11) 1.535513938222266e-05\n",
      "(12, 12) -4.311395684908348e-06\n",
      "(12, 13) 9.144640600311504e-06\n",
      "(12, 14) -0.07308965885854946\n",
      "(12, 15) -0.00013479835025975717\n",
      "(12, 16) 0.0002464009440927839\n",
      "(12, 17) 0.0036417905580066186\n",
      "(12, 18) -8.65464588883924e-05\n",
      "(12, 19) 1.9667467654471693e-05\n",
      "(12, 20) 0.0003679928806121779\n",
      "(12, 21) -8.997265155130661e-05\n",
      "(12, 22) 8.366589643316046e-05\n",
      "(12, 23) 0.4332703896992029\n",
      "(12, 24) 0.00042523509158343126\n",
      "(12, 25) 1.2395640069939871e-05\n",
      "(12, 26) 5.609637199199823e-05\n",
      "(12, 27) -4.226732297496482e-05\n",
      "(12, 28) 0.00019015455876569828\n",
      "(12, 29) 2.7094637644609062e-05\n",
      "(13, 0) -0.0035554515553926076\n",
      "(13, 1) -0.0019253144412090248\n",
      "(13, 2) -0.0013831249923157427\n",
      "(13, 3) 4.1996273125732835e-05\n",
      "(13, 4) -0.00010401342009913604\n",
      "(13, 5) -2.725242254086879e-05\n",
      "(13, 6) 5.3245807762891666e-05\n",
      "(13, 7) 0.0010633935954018625\n",
      "(13, 8) 0.00033956708556104326\n",
      "(13, 9) -1.3998050574226537\n",
      "(13, 10) -4.182387769446905e-06\n",
      "(13, 11) 1.5355028359920198e-05\n",
      "(13, 12) -4.311351275987363e-06\n",
      "(13, 13) 9.144573986930027e-06\n",
      "(13, 14) -0.07308909941716735\n",
      "(13, 15) -0.000134797351059035\n",
      "(13, 16) 0.00024639907891810253\n",
      "(13, 17) 0.00364176266920424\n",
      "(13, 18) -8.654577055011713e-05\n",
      "(13, 19) 1.9667290018787753e-05\n",
      "(13, 20) 0.00036799006064569534\n",
      "(13, 21) -8.997198541749184e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 22) 8.366523029934568e-05\n",
      "(13, 23) 0.43326707335200604\n",
      "(13, 24) 0.00042523180532327837\n",
      "(13, 25) 1.2395551252097901e-05\n",
      "(13, 26) 5.609597231170937e-05\n",
      "(13, 27) -4.2267012112517925e-05\n",
      "(13, 28) 0.00019015309327130578\n",
      "(13, 29) 2.7094460008925122e-05\n",
      "(14, 0) -0.0035554896138378918\n",
      "(14, 1) -0.0019253350691528224\n",
      "(14, 2) -0.0013831398026908912\n",
      "(14, 3) 4.1996717214942685e-05\n",
      "(14, 4) -0.00010401453032216067\n",
      "(14, 5) -2.72526889943947e-05\n",
      "(14, 6) 5.324638507886447e-05\n",
      "(14, 7) 0.0010634049862900952\n",
      "(14, 8) 0.00033957072709256403\n",
      "(14, 9) -1.3998200544262926\n",
      "(14, 10) -4.18243217836789e-06\n",
      "(14, 11) 1.5355205995604138e-05\n",
      "(14, 12) -4.31141788936884e-06\n",
      "(14, 13) 9.144662804771997e-06\n",
      "(14, 14) -0.07308988247967108\n",
      "(14, 15) -0.00013479879434896702\n",
      "(14, 16) 0.00024640174345336163\n",
      "(14, 17) 0.003641801660236865\n",
      "(14, 18) -8.65467253419183e-05\n",
      "(14, 19) 1.9667489858932186e-05\n",
      "(14, 20) 0.000367994013039663\n",
      "(14, 21) -8.997291800483252e-05\n",
      "(14, 22) 8.366609627330489e-05\n",
      "(14, 23) 0.43327171517226754\n",
      "(14, 24) 0.0004252363794421398\n",
      "(14, 25) 1.2395728887781841e-05\n",
      "(14, 26) 5.609654962768217e-05\n",
      "(14, 27) -4.226747840618827e-05\n",
      "(14, 28) 0.0001901551360816711\n",
      "(14, 29) 2.7094748666911524e-05\n",
      "(15, 0) -0.0035537947251640385\n",
      "(15, 1) -0.0019244172699828253\n",
      "(15, 2) -0.0013824804412365663\n",
      "(15, 3) 4.197673320049943e-05\n",
      "(15, 4) -0.0001039649699663414\n",
      "(15, 5) -2.7239699385006585e-05\n",
      "(15, 6) 5.322100538052154e-05\n",
      "(15, 7) 0.0010628980806615118\n",
      "(15, 8) 0.0003394088565755737\n",
      "(15, 9) -1.3991527547663194\n",
      "(15, 10) -4.1804337769235644e-06\n",
      "(15, 11) 1.534785631918112e-05\n",
      "(15, 12) -4.309352874543038e-06\n",
      "(15, 13) 9.140288526054974e-06\n",
      "(15, 14) -0.07305504023946696\n",
      "(15, 15) -0.00013473451243584122\n",
      "(15, 16) 0.0002462842596528958\n",
      "(15, 17) 0.0036400656044932585\n",
      "(15, 18) -8.650544724986274e-05\n",
      "(15, 19) 1.9658141781064842e-05\n",
      "(15, 20) 0.00036781857559731174\n",
      "(15, 21) -8.9930018987161e-05\n",
      "(15, 22) 8.362623926672085e-05\n",
      "(15, 23) 0.4330651728112755\n",
      "(15, 24) 0.0004250336305133828\n",
      "(15, 25) 1.2389800296830343e-05\n",
      "(15, 26) 5.60698154572492e-05\n",
      "(15, 27) -4.224733896052157e-05\n",
      "(15, 28) 0.0001900644530650197\n",
      "(15, 29) 2.708184787536538e-05\n",
      "(16, 0) -0.0035534569509110265\n",
      "(16, 1) -0.0019242343274328275\n",
      "(16, 2) -0.0013823490352393717\n",
      "(16, 3) 4.197273639761078e-05\n",
      "(16, 4) -0.00010395508898142224\n",
      "(16, 5) -2.7237101463128962e-05\n",
      "(16, 6) 5.321596496798974e-05\n",
      "(16, 7) 0.0010627970281618104\n",
      "(16, 8) 0.00033937661569893857\n",
      "(16, 9) -1.399019767389653\n",
      "(16, 10) -4.180034096634699e-06\n",
      "(16, 11) 1.53464352337096e-05\n",
      "(16, 12) -4.30893098979368e-06\n",
      "(16, 13) 9.139422552095766e-06\n",
      "(16, 14) -0.0730480964827862\n",
      "(16, 15) -0.00013472170046213705\n",
      "(16, 16) 0.0002462608339470762\n",
      "(16, 17) 0.0036397196367943248\n",
      "(16, 18) -8.649725380394101e-05\n",
      "(16, 19) 1.965627660638347e-05\n",
      "(16, 20) 0.00036778360357203605\n",
      "(16, 21) -8.992149247433189e-05\n",
      "(16, 22) 8.361829006986453e-05\n",
      "(16, 23) 0.4330240105820948\n",
      "(16, 24) 0.0004249932628042074\n",
      "(16, 25) 1.238862346042424e-05\n",
      "(16, 26) 5.6064486386731e-05\n",
      "(16, 27) -4.224329774871193e-05\n",
      "(16, 28) 0.0001900464230430998\n",
      "(16, 29) 2.707927215794825e-05\n",
      "(17, 0) -0.0021023704332989723\n",
      "(17, 1) -0.0011384557296878484\n",
      "(17, 2) -0.0008178542509895691\n",
      "(17, 3) 2.483278027654023e-05\n",
      "(17, 4) -6.150409070926344e-05\n",
      "(17, 5) -1.6114576339987252e-05\n",
      "(17, 6) 3.1484725937502844e-05\n",
      "(17, 7) 0.0006287942166949279\n",
      "(17, 8) 0.00020078907425613576\n",
      "(17, 9) -0.827717051343768\n",
      "(17, 10) -2.4730884007340137e-06\n",
      "(17, 11) 9.079559326607978e-06\n",
      "(17, 12) -2.549360722525762e-06\n",
      "(17, 13) 5.4072746280553466e-06\n",
      "(17, 14) -0.043218231571273684\n",
      "(17, 15) -7.970686333180765e-05\n",
      "(17, 16) 0.00014569798700847514\n",
      "(17, 17) 0.0021534069638562414\n",
      "(17, 18) -5.1175308435347226e-05\n",
      "(17, 19) 1.1629452956185558e-05\n",
      "(17, 20) 0.00021759583024305582\n",
      "(17, 21) -5.320122120622272e-05\n",
      "(17, 22) 4.947198206650682e-05\n",
      "(17, 23) 0.2561946008894367\n",
      "(17, 24) 0.0002514434216394079\n",
      "(17, 25) 7.329603590733313e-06\n",
      "(17, 26) 3.317004448888383e-05\n",
      "(17, 27) -2.4992852232230686e-05\n",
      "(17, 28) 0.00011243923569992374\n",
      "(17, 29) 1.602122878807677e-05\n",
      "(18, 0) 0.003554080052481367\n",
      "(18, 1) 0.0019245717908233926\n",
      "(18, 2) 0.0013825914857434893\n",
      "(18, 3) -4.198010827849429e-05\n",
      "(18, 4) 0.00010397331884348658\n",
      "(18, 5) 2.724187542213485e-05\n",
      "(18, 6) -5.322529084139659e-05\n",
      "(18, 7) -0.001062983390198724\n",
      "(18, 8) -0.0003394361236530585\n",
      "(18, 9) 1.399265107093761\n",
      "(18, 10) 4.1807890482914445e-06\n",
      "(18, 11) -1.53490997689687e-05\n",
      "(18, 12) 4.309685941450425e-06\n",
      "(18, 13) -9.141065682172211e-06\n",
      "(18, 14) 0.0730609065913157\n",
      "(18, 15) 0.00013474534821256157\n",
      "(18, 16) -0.0002463040438271946\n",
      "(18, 17) -0.0036403579262156423\n",
      "(18, 18) 8.65123972459969e-05\n",
      "(18, 19) -1.965971829775981e-05\n",
      "(18, 20) -0.00036784810752976677\n",
      "(18, 21) 8.993727984574206e-05\n",
      "(18, 22) -8.363296721825007e-05\n",
      "(18, 23) -0.4330999480606223\n",
      "(18, 24) -0.00042506780317808074\n",
      "(18, 25) -1.2390799497552505e-05\n",
      "(18, 26) -5.607434516718967e-05\n",
      "(18, 27) 4.225071403851643e-05\n",
      "(18, 28) -0.00019007972973383855\n",
      "(18, 29) -2.7084001708033153e-05\n",
      "(19, 0) 0.0035539468479228726\n",
      "(19, 1) 0.001924499626326792\n",
      "(19, 2) 0.0013825396383282393\n",
      "(19, 3) -4.197850955733883e-05\n",
      "(19, 4) 0.0001039694108584399\n",
      "(19, 5) 2.7240854016952195e-05\n",
      "(19, 6) -5.3223270235491775e-05\n",
      "(19, 7) -0.0010629435553966005\n",
      "(19, 8) -0.00033942340049719627\n",
      "(19, 9) 1.3992126452144757\n",
      "(19, 10) 4.1806114126075045e-06\n",
      "(19, 11) -1.5348544657456387e-05\n",
      "(19, 12) 4.309530510226978e-06\n",
      "(19, 13) -9.140688206343839e-06\n",
      "(19, 14) 0.0730581673602515\n",
      "(19, 15) 0.00013474028559556928\n",
      "(19, 16) -0.00024629478456716924\n",
      "(19, 17) -0.0036402214576014553\n",
      "(19, 18) 8.650915539476499e-05\n",
      "(19, 19) -1.9658963346103064e-05\n",
      "(19, 20) -0.00036783429635534043\n",
      "(19, 21) 8.99339047677472e-05\n",
      "(19, 22) -8.362983638932063e-05\n",
      "(19, 23) -0.43308371009409535\n",
      "(19, 24) -0.00042505183817098663\n",
      "(19, 25) -1.2390333203882163e-05\n",
      "(19, 26) -5.607221353898239e-05\n",
      "(19, 27) 4.224913752182146e-05\n",
      "(19, 28) -0.00019007262430648095\n",
      "(19, 29) -2.708300250731099e-05\n",
      "W2 relative error: 3.36e-06\n",
      "(0, 0) -0.31778223414935525\n",
      "(0, 1) 0.03556728809250842\n",
      "(0, 2) 0.033548364353208626\n",
      "(0, 3) 0.03466176607958005\n",
      "(0, 4) 0.03930750902547686\n",
      "(0, 5) 0.04110508915466937\n",
      "(0, 6) 0.035380049290267834\n",
      "(0, 7) 0.03675277029380197\n",
      "(0, 8) 0.03205540473061319\n",
      "(0, 9) 0.02940399317363784\n",
      "(1, 0) -0.3181819256958818\n",
      "(1, 1) 0.035612022952768996\n",
      "(1, 2) 0.03359055993357174\n",
      "(1, 3) 0.03470536200644858\n",
      "(1, 4) 0.0393569481671463\n",
      "(1, 5) 0.04115678917671062\n",
      "(1, 6) 0.03542454867222489\n",
      "(1, 7) 0.03679899618358462\n",
      "(1, 8) 0.032095722524161374\n",
      "(1, 9) 0.029440976145878036\n",
      "(2, 0) -0.3187430613227349\n",
      "(2, 1) 0.03567482718125348\n",
      "(2, 2) 0.03364979916931077\n",
      "(2, 3) 0.03476656731393746\n",
      "(2, 4) 0.03942635684595075\n",
      "(2, 5) 0.04122937198314247\n",
      "(2, 6) 0.035487022298497095\n",
      "(2, 7) 0.03686389373758203\n",
      "(2, 8) 0.03215232551312397\n",
      "(2, 9) 0.029492897302141326\n",
      "(3, 0) -0.31948765555345204\n",
      "(3, 1) 0.035758164762533795\n",
      "(3, 2) 0.03372840620130546\n",
      "(3, 3) 0.034847783147995415\n",
      "(3, 4) 0.03951845808369825\n",
      "(3, 5) 0.04132568516279633\n",
      "(3, 6) 0.03556992111963808\n",
      "(3, 7) 0.0369500089636432\n",
      "(3, 8) 0.03222743438779929\n",
      "(3, 9) 0.029561793701837754\n",
      "(4, 0) -0.3195088198904017\n",
      "(4, 1) 0.035760533556583596\n",
      "(4, 2) 0.03373064050293806\n",
      "(4, 3) 0.03485009161252606\n",
      "(4, 4) 0.039521075967385855\n",
      "(4, 5) 0.041328422772934914\n",
      "(4, 6) 0.035572277434781086\n",
      "(4, 7) 0.03695245671675451\n",
      "(4, 8) 0.032229569280062265\n",
      "(4, 9) 0.02956375197982197\n",
      "(5, 0) 0.04253243879315249\n",
      "(5, 1) 0.03262644125978653\n",
      "(5, 2) 0.028840505827965043\n",
      "(5, 3) 0.031005958689434007\n",
      "(5, 4) 0.03372894343822708\n",
      "(5, 5) 0.03763810272339185\n",
      "(5, 6) 0.03632016225552093\n",
      "(5, 7) -0.3135477854909752\n",
      "(5, 8) 0.029368972209731222\n",
      "(5, 9) 0.04148626033817493\n",
      "(6, 0) -0.31950765724264585\n",
      "(6, 1) 0.03576040341624065\n",
      "(6, 2) 0.033730517756680456\n",
      "(6, 3) 0.034849964780647724\n",
      "(6, 4) 0.039520932149095245\n",
      "(6, 5) 0.04132827235991954\n",
      "(6, 6) 0.035572147982776414\n",
      "(6, 7) 0.03695232226874623\n",
      "(6, 8) 0.032229452040510864\n",
      "(6, 9) 0.029563644421415344\n",
      "(7, 0) 0.0423592570797382\n",
      "(7, 1) 0.03249359434853716\n",
      "(7, 2) 0.028723074296799208\n",
      "(7, 3) 0.030879709966846743\n",
      "(7, 4) 0.03359160740679101\n",
      "(7, 5) 0.03748484951326958\n",
      "(7, 6) 0.03617227539720602\n",
      "(7, 7) -0.3122710952929708\n",
      "(7, 8) 0.02924938886650352\n",
      "(7, 9) 0.0413173383950749\n",
      "(8, 0) 0.042486200491076424\n",
      "(8, 1) 0.03259097205443595\n",
      "(8, 2) 0.028809152419206892\n",
      "(8, 3) 0.0309722511859789\n",
      "(8, 4) 0.03369227568050803\n",
      "(8, 5) 0.03759718520957023\n",
      "(8, 6) 0.03628067750671704\n",
      "(8, 7) -0.31320691822323\n",
      "(8, 8) 0.02933704430496675\n",
      "(8, 9) 0.04144115937076975\n",
      "(9, 0) 0.024712181923014494\n",
      "(9, 1) 0.01895660286521661\n",
      "(9, 2) 0.01675690006752717\n",
      "(9, 3) 0.018015070679666678\n",
      "(9, 4) 0.01959717828015073\n",
      "(9, 5) 0.02186847656293622\n",
      "(9, 6) 0.021102727298938362\n",
      "(9, 7) -0.18217741883930214\n",
      "(9, 8) 0.017063949386830757\n",
      "(9, 9) 0.024104331686203292\n",
      "(10, 0) 0.04253647547525218\n",
      "(10, 1) 0.032629537827233435\n",
      "(10, 2) 0.028843243038423335\n",
      "(10, 3) 0.031008901424378617\n",
      "(10, 4) 0.03373214461088736\n",
      "(10, 5) 0.037641674910382505\n",
      "(10, 6) 0.036323609364785625\n",
      "(10, 7) -0.31357754401994953\n",
      "(10, 8) 0.029371759580065767\n",
      "(10, 9) 0.04149019774413176\n",
      "(11, 0) 0.042529926957968194\n",
      "(11, 1) 0.032624514467727295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 2) 0.028838802612618505\n",
      "(11, 3) 0.031004127598599492\n",
      "(11, 4) 0.033726951542689676\n",
      "(11, 5) 0.03763587994587425\n",
      "(11, 6) 0.03631801730463735\n",
      "(11, 7) -0.31352926852523666\n",
      "(11, 8) 0.02936723777491323\n",
      "(11, 9) 0.04148381029800419\n",
      "(12, 0) 0.042532548238938254\n",
      "(12, 1) 0.032626525237056114\n",
      "(12, 2) 0.02884058003527201\n",
      "(12, 3) 0.031006038470060556\n",
      "(12, 4) 0.0337290302576676\n",
      "(12, 5) 0.0376381995348396\n",
      "(12, 6) 0.03632025569189068\n",
      "(12, 7) -0.31354859231225163\n",
      "(12, 8) 0.029369047771510278\n",
      "(12, 9) 0.04148636709722098\n",
      "(13, 0) 0.042536615429966666\n",
      "(13, 1) 0.032629645141390995\n",
      "(13, 2) 0.02884333791808302\n",
      "(13, 3) 0.03100900345387458\n",
      "(13, 4) 0.03373225561098536\n",
      "(13, 5) 0.03764179874465867\n",
      "(13, 6) 0.036323728846987535\n",
      "(13, 7) -0.313578575639184\n",
      "(13, 8) 0.02937185621387783\n",
      "(13, 9) 0.04149033423495041\n",
      "(14, 0) -0.3053597186220003\n",
      "(14, 1) 0.03417691711860016\n",
      "(14, 2) 0.03223691571463405\n",
      "(14, 3) 0.03330679314572649\n",
      "(14, 4) 0.03777092802970117\n",
      "(14, 5) 0.03949823841331579\n",
      "(14, 6) 0.0339969977503074\n",
      "(14, 7) 0.03531605732298715\n",
      "(14, 8) 0.030802317807498977\n",
      "(14, 9) 0.0282545533192291\n",
      "(15, 0) 0.04249054192939638\n",
      "(15, 1) 0.03259430241264738\n",
      "(15, 2) 0.02881209628657899\n",
      "(15, 3) 0.03097541607655074\n",
      "(15, 4) 0.03369571857092524\n",
      "(15, 5) 0.03760102706973356\n",
      "(15, 6) 0.03628438487446317\n",
      "(15, 7) -0.3132389233329036\n",
      "(15, 8) 0.029340042106973382\n",
      "(15, 9) 0.0414453940278392\n",
      "(16, 0) -0.3193980435689525\n",
      "(16, 1) 0.03574813505213825\n",
      "(16, 2) 0.033718945835481406\n",
      "(16, 3) 0.03483800883330446\n",
      "(16, 4) 0.039507373705838233\n",
      "(16, 5) 0.04131409385710327\n",
      "(16, 6) 0.03555994423365405\n",
      "(16, 7) 0.03693964500950386\n",
      "(16, 8) 0.032218395040750636\n",
      "(16, 9) 0.02955350200117834\n",
      "(17, 0) 0.04226290482023387\n",
      "(17, 1) 0.03241968298262066\n",
      "(17, 2) 0.0286577395369747\n",
      "(17, 3) 0.030809469597770086\n",
      "(17, 4) 0.033515198438749394\n",
      "(17, 5) 0.03739958480686312\n",
      "(17, 6) 0.0360899963469663\n",
      "(17, 7) -0.31156078961203093\n",
      "(17, 8) 0.029182856930987097\n",
      "(17, 9) 0.04122335612866124\n",
      "(18, 0) 0.04246328098034268\n",
      "(18, 1) 0.03257339065143583\n",
      "(18, 2) 0.0287936111176279\n",
      "(18, 3) 0.030955542973387647\n",
      "(18, 4) 0.03367410017496297\n",
      "(18, 5) 0.03757690312244932\n",
      "(18, 6) 0.03626110562926499\n",
      "(18, 7) -0.31303795648973676\n",
      "(18, 8) 0.02932121820897748\n",
      "(18, 9) 0.041418803631287915\n",
      "(19, 0) 0.04253187269043223\n",
      "(19, 1) 0.03262600700715268\n",
      "(19, 2) 0.02884012195725205\n",
      "(19, 3) 0.031005546019535753\n",
      "(19, 4) 0.0337284945306493\n",
      "(19, 5) 0.03763760176855868\n",
      "(19, 6) 0.036319678820007084\n",
      "(19, 7) -0.3135436122736479\n",
      "(19, 8) 0.02936858130020425\n",
      "(19, 9) 0.0414857081576514\n",
      "(20, 0) 0.04250967813312911\n",
      "(20, 1) 0.03260898169266113\n",
      "(20, 2) 0.02882507221801944\n",
      "(20, 3) 0.03098936631751314\n",
      "(20, 4) 0.03371089389858639\n",
      "(20, 5) 0.037617961190505866\n",
      "(20, 6) 0.036300726025118024\n",
      "(20, 7) -0.31337999486513723\n",
      "(20, 8) 0.029353255803776786\n",
      "(20, 9) 0.04146405956362287\n",
      "(21, 0) 0.04252461311970989\n",
      "(21, 1) 0.03262043826168792\n",
      "(21, 2) 0.028835199383792084\n",
      "(21, 3) 0.03100025385283089\n",
      "(21, 4) 0.03372273758017741\n",
      "(21, 5) 0.03763117759625345\n",
      "(21, 6) 0.036313479645500024\n",
      "(21, 7) -0.3134900951717867\n",
      "(21, 8) 0.029363568532225767\n",
      "(21, 9) 0.041478627199609264\n",
      "(22, 0) -0.319294931871994\n",
      "(22, 1) 0.0357365944392285\n",
      "(22, 2) 0.0337080602985651\n",
      "(22, 3) 0.03482676202981594\n",
      "(22, 4) 0.0394946194859358\n",
      "(22, 5) 0.04130075637043262\n",
      "(22, 6) 0.0355484643721482\n",
      "(22, 7) 0.036927719726520536\n",
      "(22, 8) 0.032207993960753356\n",
      "(22, 9) 0.029543961188593922\n",
      "(23, 0) -0.28759675194667267\n",
      "(23, 1) 0.03218882438371651\n",
      "(23, 2) 0.030361674085987996\n",
      "(23, 3) 0.03136931607805593\n",
      "(23, 4) 0.0355737693968905\n",
      "(23, 5) 0.037200601044062864\n",
      "(23, 6) 0.03201937102126351\n",
      "(23, 7) 0.033261700060904786\n",
      "(23, 8) 0.02901052762460665\n",
      "(23, 9) 0.026610968228979456\n",
      "(24, 0) -0.31924981822584186\n",
      "(24, 1) 0.03573154518932142\n",
      "(24, 2) 0.033703297641629604\n",
      "(24, 3) 0.034821841321530655\n",
      "(24, 4) 0.03948903919415159\n",
      "(24, 5) 0.04129492090498843\n",
      "(24, 6) 0.03554344170098034\n",
      "(24, 7) 0.03692250216680293\n",
      "(24, 8) 0.03220344324539326\n",
      "(24, 9) 0.029539786883248095\n",
      "(25, 0) -0.3195148771784062\n",
      "(25, 1) 0.03576121150317135\n",
      "(25, 2) 0.03373127999140024\n",
      "(25, 3) 0.03485075232845247\n",
      "(25, 4) 0.03952182521249625\n",
      "(25, 5) 0.04132920625732339\n",
      "(25, 6) 0.035572951828655164\n",
      "(25, 7) 0.03695315728968751\n",
      "(25, 8) 0.03223018032461056\n",
      "(25, 9) 0.02956431246481372\n",
      "(26, 0) 0.04252533700732641\n",
      "(26, 1) 0.03262099352863146\n",
      "(26, 2) 0.02883569021339127\n",
      "(26, 3) 0.031000781541834495\n",
      "(26, 4) 0.03372331160989006\n",
      "(26, 5) 0.03763181819493866\n",
      "(26, 6) 0.036314097751066754\n",
      "(26, 7) -0.3134954315031635\n",
      "(26, 8) 0.029364068376835913\n",
      "(26, 9) 0.041479333234839544\n",
      "(27, 0) 0.042522736420913525\n",
      "(27, 1) 0.03261899863549189\n",
      "(27, 2) 0.028833926823956798\n",
      "(27, 3) 0.030998885724997646\n",
      "(27, 4) 0.0337212493262129\n",
      "(27, 5) 0.037629516835835375\n",
      "(27, 6) 0.03631187701635952\n",
      "(27, 7) -0.313476260060952\n",
      "(27, 8) 0.029362272657706964\n",
      "(27, 9) 0.0414767966416818\n",
      "(28, 0) 0.04247119929079445\n",
      "(28, 1) 0.032579464726012475\n",
      "(28, 2) 0.028798980378219593\n",
      "(28, 3) 0.03096131537816404\n",
      "(28, 4) 0.03368037948536795\n",
      "(28, 5) 0.037583910250660324\n",
      "(28, 6) 0.03626786737598309\n",
      "(28, 7) -0.3130963299069478\n",
      "(28, 8) 0.029326685857533615\n",
      "(28, 9) 0.041426527164212246\n",
      "(29, 0) -0.3194624994984352\n",
      "(29, 1) 0.035755349214738885\n",
      "(29, 2) 0.033725750459012716\n",
      "(29, 3) 0.034845039276198975\n",
      "(29, 4) 0.03951534643942267\n",
      "(29, 5) 0.04132243125454238\n",
      "(29, 6) 0.03556712040442278\n",
      "(29, 7) 0.03694709960200271\n",
      "(29, 8) 0.03222489686205421\n",
      "(29, 9) 0.029559466030448785\n",
      "W3 relative error: 6.34e-10\n",
      "(0,) 0.0\n",
      "(1,) 0.0\n",
      "(2,) 0.0\n",
      "(3,) 0.0\n",
      "(4,) 0.0\n",
      "(5,) 0.0\n",
      "(6,) 2.2204460492503128e-11\n",
      "(7,) 0.0\n",
      "(8,) 0.0\n",
      "(9,) 0.0\n",
      "(10,) 0.0\n",
      "(11,) 0.0\n",
      "(12,) 0.0\n",
      "(13,) 0.0\n",
      "(14,) 0.0\n",
      "(15,) 0.0\n",
      "(16,) 0.0\n",
      "(17,) 0.0\n",
      "(18,) 0.0\n",
      "(19,) 0.0\n",
      "b1 relative error: 2.22e-03\n",
      "(0,) 0.0\n",
      "(1,) 0.0\n",
      "(2,) 0.0\n",
      "(3,) 0.0\n",
      "(4,) 0.0\n",
      "(5,) 0.0\n",
      "(6,) 0.0\n",
      "(7,) 0.0\n",
      "(8,) 0.0\n",
      "(9,) 0.0\n",
      "(10,) 0.0\n",
      "(11,) 0.0\n",
      "(12,) 0.0\n",
      "(13,) 0.0\n",
      "(14,) 0.0\n",
      "(15,) 0.0\n",
      "(16,) 0.0\n",
      "(17,) 0.0\n",
      "(18,) 0.0\n",
      "(19,) 0.0\n",
      "(20,) 0.0\n",
      "(21,) 0.0\n",
      "(22,) 0.0\n",
      "(23,) 0.0\n",
      "(24,) 0.0\n",
      "(25,) 0.0\n",
      "(26,) 0.0\n",
      "(27,) 0.0\n",
      "(28,) 0.0\n",
      "(29,) 0.0\n",
      "b2 relative error: 4.44e-08\n",
      "(0,) -0.39184611042575307\n",
      "(1,) 0.09674068572884663\n",
      "(2,) 0.08851376815943722\n",
      "(3,) 0.09316050151308984\n",
      "(4,) 0.10362021507503981\n",
      "(5,) 0.11170659779402568\n",
      "(6,) 0.10169915383162474\n",
      "(7,) -0.3912389216820244\n",
      "(8,) 0.08713772476909297\n",
      "(9,) 0.10050638519221165\n",
      "b3 relative error: 1.41e-10\n",
      "(0,) -0.15646114532863464\n",
      "(1,) 0.03296945789710293\n",
      "(2,) 0.02431736767505299\n",
      "(3,) -0.2031836977556267\n",
      "(4,) 0.04600534335708061\n",
      "(5,) 0.09708836530286645\n",
      "(6,) 0.23387366310245736\n",
      "(7,) -0.14691268070965435\n",
      "(8,) -0.01677553098655693\n",
      "(9,) -0.00047263624125548626\n",
      "(10,) -0.02167258801311078\n",
      "(11,) -0.047900559030367156\n",
      "(12,) -0.05888490086647379\n",
      "(13,) 0.07052406099106179\n",
      "(14,) -0.03839784559200865\n",
      "(15,) 0.007309750049699914\n",
      "(16,) 0.0354661660928457\n",
      "(17,) -0.045430769168852685\n",
      "(18,) 0.14948954318860785\n",
      "(19,) -0.013775368712387602\n",
      "beta1 relative error: 2.73e-08\n",
      "(0,) -0.02593905268355456\n",
      "(1,) -0.020280722501375692\n",
      "(2,) -0.030590145438935675\n",
      "(3,) 0.01607748401077913\n",
      "(4,) -0.051904740305808154\n",
      "(5,) 0.028872571156135503\n",
      "(6,) 0.02615307201914163\n",
      "(7,) -0.012057545317745165\n",
      "(8,) -0.023088110179259044\n",
      "(9,) 0.023219309719380018\n",
      "(10,) 0.011092437768311923\n",
      "(11,) -0.011171722480440847\n",
      "(12,) 0.004654012619731418\n",
      "(13,) -0.025357368871326\n",
      "(14,) -0.025377668566584074\n",
      "(15,) 0.010366108615933456\n",
      "(16,) 0.04310071226587552\n",
      "(17,) -0.021869282340603032\n",
      "(18,) 0.0034927336578505215\n",
      "(19,) -0.018980580263061597\n",
      "(20,) -0.05757230119307621\n",
      "(21,) 0.036773366507425465\n",
      "(22,) 0.00822894374863381\n",
      "(23,) 0.046675502796134076\n",
      "(24,) 0.03438689870627343\n",
      "(25,) 0.006736836333232076\n",
      "(26,) -0.024508352214347436\n",
      "(27,) 0.014718117458123457\n",
      "(28,) -0.009009843426888153\n",
      "(29,) 0.007965221260342048\n",
      "beta2 relative error: 1.34e-09\n",
      "(0,) -0.07647848045966299\n",
      "(1,) 0.023240872804031195\n",
      "(2,) 0.01710152370826279\n",
      "(3,) -0.14364824660884068\n",
      "(4,) 0.03252954283716747\n",
      "(5,) 0.06863262176359797\n",
      "(6,) 0.16533690483377939\n",
      "(7,) -0.10380373334228919\n",
      "(8,) -0.011857524317981925\n",
      "(9,) -0.0003328249453815601\n",
      "(10,) -0.015316355783667744\n",
      "(11,) -0.03385041402736988\n",
      "(12,) -0.04163142148794208\n",
      "(13,) 0.04985988582006939\n",
      "(14,) -0.02714722724395102\n",
      "(15,) 0.005165520233774146\n",
      "(16,) 0.02506019840531337\n",
      "(17,) -0.018992343164825343\n",
      "(18,) 0.10564701407034248\n",
      "(19,) -0.009734941874839365\n",
      "gamma1 relative error: 2.62e-09\n",
      "(0,) -0.01823631510244894\n",
      "(1,) -0.014276189097550683\n",
      "(2,) -0.02157126650637053\n",
      "(3,) 0.01136385103794879\n",
      "(4,) -0.03668962191838432\n",
      "(5,) 0.020411757573057798\n",
      "(6,) 0.018486612729518015\n",
      "(7,) -0.008489495706420769\n",
      "(8,) -0.016304629557595263\n",
      "(9,) 0.009537511003188115\n",
      "(10,) 0.007842655858070202\n",
      "(11,) -0.007897496345776744\n",
      "(12,) 0.0032902100155496323\n",
      "(13,) -0.017928409978829052\n",
      "(14,) -0.01714418447207322\n",
      "(15,) 0.0073212069295891516\n",
      "(16,) 0.03045580350136134\n",
      "(17,) -0.015362734728796566\n",
      "(18,) 0.0024652085928167367\n",
      "(19,) -0.013418335464976392\n",
      "(20,) -0.04067954264552753\n",
      "(21,) 0.025992521845985547\n",
      "(22,) 0.005812854886322326\n",
      "(23,) 0.029697941306139338\n",
      "(24,) 0.02428717658098378\n",
      "(25,) 0.004762121186274726\n",
      "(26,) -0.017323535628221975\n",
      "(27,) 0.010402749128779476\n",
      "(28,) -0.0063604292277474874\n",
      "(29,) 0.005629516475202933\n",
      "gamma2 relative error: 5.64e-09\n",
      "\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  7.00966245455163\n",
      "(0, 0) -0.04528060770780939\n",
      "(0, 1) -0.2632205831076817\n",
      "(0, 2) 0.23514754423636927\n",
      "(0, 3) 0.02636156679614032\n",
      "(0, 4) -0.028363814230658587\n",
      "(0, 5) -0.16130769417976865\n",
      "(0, 6) -0.005105843481345573\n",
      "(0, 7) -0.05535896878683388\n",
      "(0, 8) -0.14186483805467276\n",
      "(0, 9) 0.014368133260944658\n",
      "(0, 10) -0.21038448707599852\n",
      "(0, 11) 0.20292486646589222\n",
      "(0, 12) -0.04076397659424913\n",
      "(0, 13) -0.3408650536051993\n",
      "(0, 14) 0.08296994344902942\n",
      "(0, 15) 0.19131977930797459\n",
      "(0, 16) 0.2199111106993001\n",
      "(0, 17) 0.2906125165402784\n",
      "(0, 18) 0.04955088934721629\n",
      "(0, 19) 0.0442819611201628\n",
      "(1, 0) 0.14132053300919267\n",
      "(1, 1) -0.07423737784506557\n",
      "(1, 2) 0.1867195802862653\n",
      "(1, 3) -0.14809064965426444\n",
      "(1, 4) -0.15766865457678136\n",
      "(1, 5) -0.21525854130288732\n",
      "(1, 6) 0.2197713389051614\n",
      "(1, 7) 0.21438180395350057\n",
      "(1, 8) 0.08042620174286697\n",
      "(1, 9) -0.5409360884556236\n",
      "(1, 10) 0.02377615762050311\n",
      "(1, 11) -0.2032470230339811\n",
      "(1, 12) -0.06699546948318869\n",
      "(1, 13) -0.2639606146992435\n",
      "(1, 14) 0.2809462482655789\n",
      "(1, 15) 0.012611934119277633\n",
      "(1, 16) 0.029553825386940954\n",
      "(1, 17) 0.41441369162953373\n",
      "(1, 18) 0.24909939257966582\n",
      "(1, 19) -0.16657991457336152\n",
      "(2, 0) 0.06665088521096152\n",
      "(2, 1) 0.06387081503689274\n",
      "(2, 2) -0.12121976045165182\n",
      "(2, 3) -0.4586650026006111\n",
      "(2, 4) 0.034703104345723546\n",
      "(2, 5) -0.0066854146218986435\n",
      "(2, 6) 0.17344034861643817\n",
      "(2, 7) 0.028322336032005065\n",
      "(2, 8) -0.10232757059469576\n",
      "(2, 9) 0.11038920648687166\n",
      "(2, 10) 0.3074800591029714\n",
      "(2, 11) 0.07602858884681041\n",
      "(2, 12) -0.22457519048835192\n",
      "(2, 13) -0.22801058805477223\n",
      "(2, 14) -0.19150775996124023\n",
      "(2, 15) 0.07128907260067763\n",
      "(2, 16) 0.15692160486580065\n",
      "(2, 17) 0.032661719950866086\n",
      "(2, 18) 0.4163840382709338\n",
      "(2, 19) 0.33679638544725776\n",
      "(3, 0) -0.050165583953543084\n",
      "(3, 1) 0.17657067372667254\n",
      "(3, 2) -0.028119611172883193\n",
      "(3, 3) 0.1867838668179189\n",
      "(3, 4) -0.01503933586555206\n",
      "(3, 5) 0.07003550650530599\n",
      "(3, 6) 0.12078013091887384\n",
      "(3, 7) -0.26073856669306394\n",
      "(3, 8) -0.13042299062959728\n",
      "(3, 9) -0.22770330074806108\n",
      "(3, 10) -0.021430145924483664\n",
      "(3, 11) 0.04740169639028124\n",
      "(3, 12) -0.12592379619036365\n",
      "(3, 13) -0.059178512890056816\n",
      "(3, 14) 0.029405208046284766\n",
      "(3, 15) -0.057405509412689064\n",
      "(3, 16) 0.2402054647188123\n",
      "(3, 17) -0.14414277234031658\n",
      "(3, 18) -0.07112355242533397\n",
      "(3, 19) -0.2789371001288288\n",
      "(4, 0) 0.21284719928793547\n",
      "(4, 1) 0.08715499029143813\n",
      "(4, 2) 0.04919911122946984\n",
      "(4, 3) -0.1039078311393382\n",
      "(4, 4) -0.23369045543120134\n",
      "(4, 5) -0.1379851739535809\n",
      "(4, 6) 0.06438221418214596\n",
      "(4, 7) 0.07220802338281374\n",
      "(4, 8) -0.09954212694118779\n",
      "(4, 9) 0.30853811434639056\n",
      "(4, 10) 0.0021323575349896373\n",
      "(4, 11) 0.23802974560283016\n",
      "(4, 12) 0.08665037736221846\n",
      "(4, 13) 0.09729456857954231\n",
      "(4, 14) -0.0035963420685902743\n",
      "(4, 15) -0.07182619095047471\n",
      "(4, 16) -0.22231526473071025\n",
      "(4, 17) 0.22117093467777235\n",
      "(4, 18) -0.05304659409510747\n",
      "(4, 19) -0.15189658082448432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0) 0.13408893613231498\n",
      "(5, 1) 0.28750429179602577\n",
      "(5, 2) 0.00016054966245349078\n",
      "(5, 3) -0.0790175360254608\n",
      "(5, 4) 0.041514229964789706\n",
      "(5, 5) -0.057178973111859925\n",
      "(5, 6) -0.032097131930086675\n",
      "(5, 7) 0.07198379305961566\n",
      "(5, 8) 0.16668397204711027\n",
      "(5, 9) 0.06874178426485855\n",
      "(5, 10) -0.12161532993992806\n",
      "(5, 11) 0.2119884751738965\n",
      "(5, 12) 0.13384062769183913\n",
      "(5, 13) -0.04122594225997034\n",
      "(5, 14) 0.0432445222742217\n",
      "(5, 15) 0.017743838798267575\n",
      "(5, 16) -0.042245049636235876\n",
      "(5, 17) -0.04034029124078131\n",
      "(5, 18) -0.0115169481507138\n",
      "(5, 19) -0.14890296546887782\n",
      "(6, 0) -0.01998521792501151\n",
      "(6, 1) 0.4041804740673171\n",
      "(6, 2) 0.07874544012409501\n",
      "(6, 3) -0.1380171119613749\n",
      "(6, 4) -0.14432209383130612\n",
      "(6, 5) 0.07142083191524762\n",
      "(6, 6) 0.02638210747640812\n",
      "(6, 7) -0.37036453033323363\n",
      "(6, 8) 0.08443150822934342\n",
      "(6, 9) 0.006130548690919112\n",
      "(6, 10) 0.09832257599207138\n",
      "(6, 11) -0.27387383552124334\n",
      "(6, 12) -0.06846934930138104\n",
      "(6, 13) -0.022287104961549172\n",
      "(6, 14) -0.17740472526561743\n",
      "(6, 15) 0.15220728180054266\n",
      "(6, 16) -0.1764817339378055\n",
      "(6, 17) -0.2634787573008168\n",
      "(6, 18) -0.2030190414448185\n",
      "(6, 19) -0.3010554124216469\n",
      "(7, 0) -0.03108773469762127\n",
      "(7, 1) -0.06589627159492295\n",
      "(7, 2) 0.10752781016698519\n",
      "(7, 3) -0.14190809149994266\n",
      "(7, 4) -0.2689021923352186\n",
      "(7, 5) -0.02046203366745658\n",
      "(7, 6) -0.19127737105328887\n",
      "(7, 7) -0.018616417607475455\n",
      "(7, 8) -0.2822045674921725\n",
      "(7, 9) 0.021595801591445248\n",
      "(7, 10) 0.048791558215910406\n",
      "(7, 11) 0.0814866012444071\n",
      "(7, 12) -0.03458881709939021\n",
      "(7, 13) 0.14666813337171902\n",
      "(7, 14) -0.2367841142270066\n",
      "(7, 15) -0.08532665578542263\n",
      "(7, 16) -0.04800364497015152\n",
      "(7, 17) 0.2427292871587383\n",
      "(7, 18) -0.12459194702607589\n",
      "(7, 19) -0.15342491961334304\n",
      "(8, 0) -0.04729140665737929\n",
      "(8, 1) -0.1765150035026863\n",
      "(8, 2) -0.13792464521600323\n",
      "(8, 3) 0.08907998014784367\n",
      "(8, 4) -0.13739168442938876\n",
      "(8, 5) -0.2104158001170475\n",
      "(8, 6) -0.16134460385508476\n",
      "(8, 7) -0.05214417675425408\n",
      "(8, 8) -0.06974756368016699\n",
      "(8, 9) 0.06566974262156577\n",
      "(8, 10) 0.1827117881880724\n",
      "(8, 11) 0.1476956039248023\n",
      "(8, 12) -0.07406086766081899\n",
      "(8, 13) 0.19793767260800618\n",
      "(8, 14) -0.023841829754900342\n",
      "(8, 15) 0.13536191847407508\n",
      "(8, 16) -0.003147047955565085\n",
      "(8, 17) 0.2331446927961167\n",
      "(8, 18) -0.0535632304732303\n",
      "(8, 19) 0.11292620998304413\n",
      "(9, 0) -0.004885445159530377\n",
      "(9, 1) 0.25503687917094453\n",
      "(9, 2) 0.16108803571057706\n",
      "(9, 3) 0.13301350634264963\n",
      "(9, 4) -0.005994132701303555\n",
      "(9, 5) -0.11434985545122343\n",
      "(9, 6) 0.04782958025195682\n",
      "(9, 7) 0.12657355656919833\n",
      "(9, 8) -0.2562843859799102\n",
      "(9, 9) 0.21358262811688175\n",
      "(9, 10) 0.06257018658750724\n",
      "(9, 11) -0.02369621343589756\n",
      "(9, 12) 0.04380578824125791\n",
      "(9, 13) 0.018944720148184047\n",
      "(9, 14) -0.29138225432490117\n",
      "(9, 15) 0.19315819201892734\n",
      "(9, 16) 0.1565223485044953\n",
      "(9, 17) -0.07218723090396395\n",
      "(9, 18) 0.023948054028721796\n",
      "(9, 19) -0.03853381866925076\n",
      "(10, 0) 0.1958525858380966\n",
      "(10, 1) -0.06609588405481759\n",
      "(10, 2) 0.027906279376210815\n",
      "(10, 3) 0.037849355871699686\n",
      "(10, 4) 0.08161974403009253\n",
      "(10, 5) 0.10805810561897998\n",
      "(10, 6) -0.048498974658528475\n",
      "(10, 7) 0.2327547227398696\n",
      "(10, 8) -0.050845945054334145\n",
      "(10, 9) 0.06882306848865483\n",
      "(10, 10) 0.10354005017809696\n",
      "(10, 11) -0.2925707493162122\n",
      "(10, 12) -0.019628453973297155\n",
      "(10, 13) 0.18755657325009165\n",
      "(10, 14) 0.0530362185280353\n",
      "(10, 15) -0.03613056183127128\n",
      "(10, 16) -0.2654708584959309\n",
      "(10, 17) -0.018177855087841976\n",
      "(10, 18) 0.10950620383809449\n",
      "(10, 19) 0.1943245981905761\n",
      "(11, 0) 0.021338526057235182\n",
      "(11, 1) 0.03112697122276131\n",
      "(11, 2) -0.12899350587680658\n",
      "(11, 3) -0.007070634877592851\n",
      "(11, 4) -0.06046386671343384\n",
      "(11, 5) -0.454689424023158\n",
      "(11, 6) -0.11550426792794609\n",
      "(11, 7) -0.14334984168584697\n",
      "(11, 8) 0.06839968564875676\n",
      "(11, 9) -0.040810201440422134\n",
      "(11, 10) 0.3842089684624738\n",
      "(11, 11) -0.12026701270428929\n",
      "(11, 12) -0.06726299894843635\n",
      "(11, 13) 0.034423183414844516\n",
      "(11, 14) 0.3836815649194136\n",
      "(11, 15) 0.0200499160829537\n",
      "(11, 16) -0.08026883673295515\n",
      "(11, 17) 0.1182272882260804\n",
      "(11, 18) 0.14792368876470618\n",
      "(11, 19) -0.13529406373002928\n",
      "(12, 0) 0.2870207358895982\n",
      "(12, 1) 0.06054382075681985\n",
      "(12, 2) 0.21195738666435202\n",
      "(12, 3) 0.058318483953101456\n",
      "(12, 4) -0.12457990514747051\n",
      "(12, 5) -0.031463083338678643\n",
      "(12, 6) 0.11393628573941326\n",
      "(12, 7) 0.11272476005963482\n",
      "(12, 8) -0.05044982263058272\n",
      "(12, 9) 0.14717960348242798\n",
      "(12, 10) 0.17726337842027814\n",
      "(12, 11) -0.126879642525779\n",
      "(12, 12) 0.04103209301220545\n",
      "(12, 13) 0.16149766848272407\n",
      "(12, 14) 0.0013262348552700585\n",
      "(12, 15) -0.38879541279790425\n",
      "(12, 16) 0.01033033121267124\n",
      "(12, 17) 0.20149977171257658\n",
      "(12, 18) -0.013913225016537465\n",
      "(12, 19) -0.05133813454349933\n",
      "(13, 0) -0.20840920327280796\n",
      "(13, 1) 0.1119482070599531\n",
      "(13, 2) -0.1731287515571722\n",
      "(13, 3) 0.14954215581397534\n",
      "(13, 4) -0.0509801862058623\n",
      "(13, 5) -0.07790811942243181\n",
      "(13, 6) 0.03681977527314473\n",
      "(13, 7) 0.2983921207899698\n",
      "(13, 8) -0.09695353302419106\n",
      "(13, 9) 0.05114448344833988\n",
      "(13, 10) 0.15352598632389913\n",
      "(13, 11) 0.16705811956008176\n",
      "(13, 12) 0.05141772332351024\n",
      "(13, 13) -0.15396257766475685\n",
      "(13, 14) 0.13458611687333644\n",
      "(13, 15) -0.0013103827356530928\n",
      "(13, 16) -0.07031102051691107\n",
      "(13, 17) 0.12772229629476328\n",
      "(13, 18) -0.18258598446507787\n",
      "(13, 19) -0.03366552316919069\n",
      "(14, 0) 0.13741713895676355\n",
      "(14, 1) 0.02719588336752565\n",
      "(14, 2) 0.15056655500167437\n",
      "(14, 3) -0.33427561798582417\n",
      "(14, 4) -0.29886953005764383\n",
      "(14, 5) -0.008271587548236425\n",
      "(14, 6) 0.045708218188167386\n",
      "(14, 7) 0.07616621071626639\n",
      "(14, 8) 0.05975966259619269\n",
      "(14, 9) -0.07922932772075342\n",
      "(14, 10) 0.09262319071723367\n",
      "(14, 11) 0.05340855828706025\n",
      "(14, 12) -0.028241919736160522\n",
      "(14, 13) 0.285912924580245\n",
      "(14, 14) 0.06678715047669925\n",
      "(14, 15) -0.10924705788895038\n",
      "(14, 16) -0.020634543718145437\n",
      "(14, 17) 0.19098024646169162\n",
      "(14, 18) 0.04753672895319027\n",
      "(14, 19) 0.010721815968395275\n",
      "W1 relative error: 6.38e-07\n",
      "(0, 0) 0.048467416169373216\n",
      "(0, 1) 0.18562507975161677\n",
      "(0, 2) -0.002750794081407548\n",
      "(0, 3) -0.19700537863265308\n",
      "(0, 4) 0.006305826527608359\n",
      "(0, 5) -0.28217609751024497\n",
      "(0, 6) -0.0003642411261495226\n",
      "(0, 7) -0.14391600702268192\n",
      "(0, 8) 0.015528984675938771\n",
      "(0, 9) 0.07946445719220208\n",
      "(0, 10) 0.06658832134576187\n",
      "(0, 11) 0.16764656414203216\n",
      "(0, 12) -0.030191965461057176\n",
      "(0, 13) -0.01984606332605665\n",
      "(0, 14) 0.02194903512275914\n",
      "(0, 15) 0.09329910000133167\n",
      "(0, 16) 0.1318724345722444\n",
      "(0, 17) -0.2519576930204437\n",
      "(0, 18) -0.14730905948390216\n",
      "(0, 19) -0.09789161588003024\n",
      "(0, 20) -0.30875042802414043\n",
      "(0, 21) 0.03051386969765701\n",
      "(0, 22) -0.06345286274189732\n",
      "(0, 23) -0.12697788003279697\n",
      "(0, 24) -0.012668670867910235\n",
      "(0, 25) 0.2927101733884996\n",
      "(0, 26) 0.11357811922074744\n",
      "(0, 27) 0.23303070668667655\n",
      "(0, 28) -0.028369332927269394\n",
      "(0, 29) 0.019206232781954213\n",
      "(1, 0) -0.22584801975256139\n",
      "(1, 1) -0.05618803307072539\n",
      "(1, 2) -0.005651080137170083\n",
      "(1, 3) 0.16399387541277122\n",
      "(1, 4) 0.13400605523017362\n",
      "(1, 5) -0.10641102017316938\n",
      "(1, 6) -0.011387461196221691\n",
      "(1, 7) 0.024716916868783297\n",
      "(1, 8) -0.096160046769711\n",
      "(1, 9) 0.010324720634002915\n",
      "(1, 10) -0.12783957425988035\n",
      "(1, 11) -0.013570747814384275\n",
      "(1, 12) -0.14521779148068958\n",
      "(1, 13) -0.2604666918593068\n",
      "(1, 14) 0.23404499174262125\n",
      "(1, 15) 0.3089831190461467\n",
      "(1, 16) -0.05758450760673383\n",
      "(1, 17) -0.011282197931450353\n",
      "(1, 18) -0.09340276365676912\n",
      "(1, 19) -0.09695230165362999\n",
      "(1, 20) 0.036816723358867876\n",
      "(1, 21) 0.4275337278869528\n",
      "(1, 22) 0.09854611699999792\n",
      "(1, 23) -0.0936156626885065\n",
      "(1, 24) -0.18943317110498012\n",
      "(1, 25) 0.0034273924587324696\n",
      "(1, 26) -0.11357236768816391\n",
      "(1, 27) 0.3357755367972004\n",
      "(1, 28) 0.27484016915657605\n",
      "(1, 29) -0.2380295556214662\n",
      "(2, 0) -0.0856672954352433\n",
      "(2, 1) -0.0828928168772336\n",
      "(2, 2) 0.23160279551959204\n",
      "(2, 3) -0.013594147452167247\n",
      "(2, 4) -0.014008027893197548\n",
      "(2, 5) 0.1567772786703614\n",
      "(2, 6) 0.12302955507692558\n",
      "(2, 7) 0.1536286456271796\n",
      "(2, 8) -0.060101153920300014\n",
      "(2, 9) -0.07049427983574219\n",
      "(2, 10) -0.22525307041831863\n",
      "(2, 11) 0.1196380277068698\n",
      "(2, 12) 0.05774156903726179\n",
      "(2, 13) 0.02802883263264277\n",
      "(2, 14) 0.2708946139406976\n",
      "(2, 15) 0.15404275375274779\n",
      "(2, 16) 0.03489619513352693\n",
      "(2, 17) -0.06559995280319697\n",
      "(2, 18) -0.22734231110987932\n",
      "(2, 19) 0.07077890811224563\n",
      "(2, 20) -0.35164965694889355\n",
      "(2, 21) -0.20364100850045472\n",
      "(2, 22) 0.017638734961522573\n",
      "(2, 23) -0.20609569508778233\n",
      "(2, 24) -0.17044674955890568\n",
      "(2, 25) -0.04915559550511261\n",
      "(2, 26) -0.011233030772217488\n",
      "(2, 27) -0.2743917502101567\n",
      "(2, 28) 0.24859364513574175\n",
      "(2, 29) 0.2854099787619191\n",
      "(3, 0) 0.011490600382302317\n",
      "(3, 1) 0.05179993571857721\n",
      "(3, 2) 0.18273844819560733\n",
      "(3, 3) 0.2920660061800362\n",
      "(3, 4) 0.15280937448736154\n",
      "(3, 5) -0.14011646065625882\n",
      "(3, 6) 0.037307892286619904\n",
      "(3, 7) -0.07559104631127411\n",
      "(3, 8) 0.05903948800956015\n",
      "(3, 9) 0.040985194527110025\n",
      "(3, 10) 0.2591002509433338\n",
      "(3, 11) 0.1136467729700996\n",
      "(3, 12) -0.1618251392443426\n",
      "(3, 13) -0.315697925312719\n",
      "(3, 14) 0.12024142908018119\n",
      "(3, 15) 0.08617337314831273\n",
      "(3, 16) 0.22110147144260847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 17) -0.07928979752769294\n",
      "(3, 18) 0.021897114699243044\n",
      "(3, 19) 0.014068313936022035\n",
      "(3, 20) -0.04188588889064704\n",
      "(3, 21) 0.05251345056578315\n",
      "(3, 22) -0.09790609634130475\n",
      "(3, 23) 0.15677095026589427\n",
      "(3, 24) 0.12926569441518154\n",
      "(3, 25) -0.04476037918088593\n",
      "(3, 26) 0.06887959096424368\n",
      "(3, 27) -0.49048893702163804\n",
      "(3, 28) 0.08192963250053253\n",
      "(3, 29) -0.019315627453053708\n",
      "(4, 0) 0.023518248681853034\n",
      "(4, 1) 0.1198320807471731\n",
      "(4, 2) -0.12955629418165415\n",
      "(4, 3) -0.1445558588386575\n",
      "(4, 4) -0.03263567007749657\n",
      "(4, 5) 0.053126078825016514\n",
      "(4, 6) 0.004631461880677534\n",
      "(4, 7) 0.13943230769974946\n",
      "(4, 8) -0.033291538503732454\n",
      "(4, 9) -0.032870022970143964\n",
      "(4, 10) -0.22664703971386754\n",
      "(4, 11) 0.215459887042968\n",
      "(4, 12) -0.0786521382245553\n",
      "(4, 13) 0.0754194552143872\n",
      "(4, 14) 0.06940143744138538\n",
      "(4, 15) -0.06103008267643872\n",
      "(4, 16) 0.06715967226789132\n",
      "(4, 17) -0.14362447857685368\n",
      "(4, 18) 0.2164673697180319\n",
      "(4, 19) -0.15133710569159575\n",
      "(4, 20) 0.04660004511158888\n",
      "(4, 21) -0.11433179398778746\n",
      "(4, 22) 0.006132960894689176\n",
      "(4, 23) 0.04764505314724942\n",
      "(4, 24) 0.06734355313398055\n",
      "(4, 25) 0.23620315485750606\n",
      "(4, 26) 0.24215854899090059\n",
      "(4, 27) 0.38967462812067305\n",
      "(4, 28) 0.01022221853830274\n",
      "(4, 29) 0.017238366201155486\n",
      "(5, 0) -0.007751484831786114\n",
      "(5, 1) -0.15242627053879687\n",
      "(5, 2) 0.13423924491995365\n",
      "(5, 3) 0.17321209488940778\n",
      "(5, 4) 0.08301112552899781\n",
      "(5, 5) -0.2566780061385998\n",
      "(5, 6) -0.09380476106990442\n",
      "(5, 7) -0.07512332360626317\n",
      "(5, 8) 0.37397598608990273\n",
      "(5, 9) -0.13753873369104497\n",
      "(5, 10) 0.12054172819908614\n",
      "(5, 11) 0.06383125561448821\n",
      "(5, 12) -0.09985762119413265\n",
      "(5, 13) -0.042907844033379654\n",
      "(5, 14) -0.02596918511343915\n",
      "(5, 15) -0.15502553409874054\n",
      "(5, 16) 0.0005516987755527225\n",
      "(5, 17) 0.07492333646830218\n",
      "(5, 18) 0.3530322912226324\n",
      "(5, 19) -0.1908390760529954\n",
      "(5, 20) 0.20645228122617706\n",
      "(5, 21) 0.148117033349493\n",
      "(5, 22) -0.2327518912270676\n",
      "(5, 23) 0.08802772732963147\n",
      "(5, 24) 0.1344081429710542\n",
      "(5, 25) -0.22987996275425357\n",
      "(5, 26) -0.23919352836188065\n",
      "(5, 27) -0.4155406668981953\n",
      "(5, 28) -0.07845124554428651\n",
      "(5, 29) -0.04237744044566227\n",
      "(6, 0) -0.18242532737744452\n",
      "(6, 1) 0.024659282082950536\n",
      "(6, 2) -0.06698081822520408\n",
      "(6, 3) 0.31773115316546807\n",
      "(6, 4) -0.046537330522511404\n",
      "(6, 5) 0.16061568719472064\n",
      "(6, 6) 0.11926157181818552\n",
      "(6, 7) -0.017890443260171196\n",
      "(6, 8) 0.13193688270796144\n",
      "(6, 9) -0.002837732582250396\n",
      "(6, 10) -0.0008834490472509059\n",
      "(6, 11) 0.03670471993011404\n",
      "(6, 12) 0.23695515674049258\n",
      "(6, 13) 0.14063269304642745\n",
      "(6, 14) 0.061157312147841474\n",
      "(6, 15) -0.01930434549990423\n",
      "(6, 16) -0.0884136392098611\n",
      "(6, 17) 0.03806697428387906\n",
      "(6, 18) -0.2253778302652165\n",
      "(6, 19) 0.11329293423401053\n",
      "(6, 20) -0.1810469641760903\n",
      "(6, 21) 0.04080121001059922\n",
      "(6, 22) 0.05036751060671917\n",
      "(6, 23) -0.12220174085975087\n",
      "(6, 24) 0.051366160480625915\n",
      "(6, 25) -0.17580261761906965\n",
      "(6, 26) 0.018812159119363514\n",
      "(6, 27) 0.46075823205704575\n",
      "(6, 28) -0.1467291249390712\n",
      "(6, 29) 0.13369926397110987\n",
      "(7, 0) -0.3533184523618615\n",
      "(7, 1) -0.022429102131127362\n",
      "(7, 2) -0.07013730671801\n",
      "(7, 3) 0.14535411980709512\n",
      "(7, 4) 0.021518198156655895\n",
      "(7, 5) -0.215908285383648\n",
      "(7, 6) -0.10237480605468362\n",
      "(7, 7) 0.17220921706417866\n",
      "(7, 8) -0.05697718132857687\n",
      "(7, 9) -0.05374758935694501\n",
      "(7, 10) 0.08048695572249187\n",
      "(7, 11) 0.06333597868390939\n",
      "(7, 12) 0.016337555086565203\n",
      "(7, 13) 0.1332384176322421\n",
      "(7, 14) -0.45185134807113053\n",
      "(7, 15) 0.006256571216667338\n",
      "(7, 16) -0.016145651571264352\n",
      "(7, 17) 0.1440811198349934\n",
      "(7, 18) -0.36035368458087186\n",
      "(7, 19) -0.10783798076730021\n",
      "(7, 20) 0.12288131885362928\n",
      "(7, 21) 0.22338511800334968\n",
      "(7, 22) 0.03316697694266679\n",
      "(7, 23) 0.14821788130170432\n",
      "(7, 24) -0.0889296233097525\n",
      "(7, 25) -0.12844701595682295\n",
      "(7, 26) 0.11617581767708883\n",
      "(7, 27) -0.39093002488499445\n",
      "(7, 28) 0.1998298190919456\n",
      "(7, 29) 0.022303356050912267\n",
      "(8, 0) 0.1826574080432408\n",
      "(8, 1) 0.04297575171285927\n",
      "(8, 2) -0.0858179888041377\n",
      "(8, 3) -0.08254994736667243\n",
      "(8, 4) 0.012460948051185026\n",
      "(8, 5) 0.10812493038692138\n",
      "(8, 6) 0.20122489536866792\n",
      "(8, 7) 0.10117549615884512\n",
      "(8, 8) 0.06429309422628648\n",
      "(8, 9) 0.18980159945947148\n",
      "(8, 10) 0.151995473274269\n",
      "(8, 11) -0.13272135497288673\n",
      "(8, 12) 0.06290585718993214\n",
      "(8, 13) -0.05868537620301594\n",
      "(8, 14) -0.14010540305697816\n",
      "(8, 15) -0.03768286020999767\n",
      "(8, 16) 0.0437482902793107\n",
      "(8, 17) 0.04325720954767575\n",
      "(8, 18) -0.002481783933916404\n",
      "(8, 19) 0.2526462780316763\n",
      "(8, 20) -0.17881608469494378\n",
      "(8, 21) 0.5374259769652667\n",
      "(8, 22) 0.054687237760475675\n",
      "(8, 23) 0.13804865313105097\n",
      "(8, 24) 0.1340828272411443\n",
      "(8, 25) -0.014758639510503711\n",
      "(8, 26) -0.06508706276164844\n",
      "(8, 27) -0.11075018515604994\n",
      "(8, 28) 0.10775274312813109\n",
      "(8, 29) 0.026261547159123207\n",
      "(9, 0) 0.07079393844477977\n",
      "(9, 1) -0.05253517678660557\n",
      "(9, 2) 0.16831213480728024\n",
      "(9, 3) 0.09650842240915834\n",
      "(9, 4) 0.2592564379177986\n",
      "(9, 5) -0.1044122033277972\n",
      "(9, 6) -0.0967084245129257\n",
      "(9, 7) -0.04584082851089021\n",
      "(9, 8) 0.2834666573026112\n",
      "(9, 9) -0.1255777202224806\n",
      "(9, 10) 0.08614921225280624\n",
      "(9, 11) -0.1060362860627606\n",
      "(9, 12) -0.22033337336857525\n",
      "(9, 13) -0.18816180706870964\n",
      "(9, 14) -0.1583570250929256\n",
      "(9, 15) -0.003402719439549173\n",
      "(9, 16) 0.45547948408497513\n",
      "(9, 17) -0.06963011474070413\n",
      "(9, 18) 0.08315284070192774\n",
      "(9, 19) -0.06163017474136722\n",
      "(9, 20) -0.07607391365738181\n",
      "(9, 21) -0.12005534744119471\n",
      "(9, 22) 0.24708543806539748\n",
      "(9, 23) -0.16407172878096787\n",
      "(9, 24) -0.13561812473028567\n",
      "(9, 25) -0.15042797540942843\n",
      "(9, 26) -0.06137839263509192\n",
      "(9, 27) 0.22915419273239476\n",
      "(9, 28) 0.006035927757608305\n",
      "(9, 29) -0.1644522508392754\n",
      "(10, 0) 0.10068242359828615\n",
      "(10, 1) -0.047911661127386644\n",
      "(10, 2) 0.2813229545761686\n",
      "(10, 3) 0.20926443644597723\n",
      "(10, 4) -0.009894107932950647\n",
      "(10, 5) 0.2112005424770302\n",
      "(10, 6) -0.021591579102420152\n",
      "(10, 7) -0.1624700206814822\n",
      "(10, 8) 0.07740617271601025\n",
      "(10, 9) 0.13345198848213613\n",
      "(10, 10) -0.14546037974128012\n",
      "(10, 11) -0.1169477369167282\n",
      "(10, 12) 0.09247211867347004\n",
      "(10, 13) -0.03890380799731474\n",
      "(10, 14) 0.2400357297815958\n",
      "(10, 15) -0.2743614381017778\n",
      "(10, 16) -0.07225093598961507\n",
      "(10, 17) 0.23575962417865523\n",
      "(10, 18) 0.04678869767182902\n",
      "(10, 19) 0.010176525222505006\n",
      "(10, 20) 0.24081721221591576\n",
      "(10, 21) -0.011094042839943084\n",
      "(10, 22) 0.02738146820391307\n",
      "(10, 23) -0.16533602584800633\n",
      "(10, 24) 0.09765980819587126\n",
      "(10, 25) 0.10764200468749151\n",
      "(10, 26) -0.18639873071712995\n",
      "(10, 27) 0.5536847804687994\n",
      "(10, 28) -0.026967446853376483\n",
      "(10, 29) -0.05935736493434262\n",
      "(11, 0) 0.02778993390961659\n",
      "(11, 1) 0.12647454692427118\n",
      "(11, 2) -0.22578218321633867\n",
      "(11, 3) -0.09226159636277485\n",
      "(11, 4) 0.12201376220488667\n",
      "(11, 5) -0.06830700747251228\n",
      "(11, 6) -0.28036157555533237\n",
      "(11, 7) -0.046221134786250666\n",
      "(11, 8) 0.22493964744185743\n",
      "(11, 9) -0.013677353516428068\n",
      "(11, 10) 0.048501635552256055\n",
      "(11, 11) 0.13493872788039596\n",
      "(11, 12) -0.08296033908727907\n",
      "(11, 13) 0.0638666563634871\n",
      "(11, 14) -0.09026615952478777\n",
      "(11, 15) -0.06508020682360893\n",
      "(11, 16) 0.030345034440770743\n",
      "(11, 17) -0.07738978236027094\n",
      "(11, 18) -0.22144579165939146\n",
      "(11, 19) 0.207213693537156\n",
      "(11, 20) -0.1852943603442014\n",
      "(11, 21) 0.07769999670337313\n",
      "(11, 22) 0.21808925190214265\n",
      "(11, 23) -0.2546382656731794\n",
      "(11, 24) 0.21683817355011567\n",
      "(11, 25) 0.08773391586558431\n",
      "(11, 26) 0.3630810082810853\n",
      "(11, 27) 0.27810090692526046\n",
      "(11, 28) -0.003540948245728259\n",
      "(11, 29) 0.05881126523554769\n",
      "(12, 0) 0.1069842891610051\n",
      "(12, 1) 0.16273649441345128\n",
      "(12, 2) 0.14596261079802275\n",
      "(12, 3) -0.13227886048738924\n",
      "(12, 4) 0.44350843695362124\n",
      "(12, 5) 0.09703073198252808\n",
      "(12, 6) 0.10257124589685417\n",
      "(12, 7) -0.06557884759672561\n",
      "(12, 8) 0.21529550440213538\n",
      "(12, 9) 0.07487308444353857\n",
      "(12, 10) 0.010633773417367818\n",
      "(12, 11) 0.19609729573488718\n",
      "(12, 12) -0.18330421323042853\n",
      "(12, 13) 0.2998680468380144\n",
      "(12, 14) -0.02035924011600798\n",
      "(12, 15) 0.19547884373416477\n",
      "(12, 16) 0.09227786339494058\n",
      "(12, 17) 0.12580795534233857\n",
      "(12, 18) -0.07780380633093387\n",
      "(12, 19) 0.04387936360927824\n",
      "(12, 20) 0.04072231107876689\n",
      "(12, 21) -0.34237974766426754\n",
      "(12, 22) -0.1700934508797047\n",
      "(12, 23) 0.0094797349525777\n",
      "(12, 24) -0.15504927861620388\n",
      "(12, 25) 0.30262484176368787\n",
      "(12, 26) 0.037627233284709405\n",
      "(12, 27) -0.07947836326849256\n",
      "(12, 28) 0.19161091624475543\n",
      "(12, 29) 0.022858616643972592\n",
      "(13, 0) -0.2125033666811049\n",
      "(13, 1) 0.047687759163750336\n",
      "(13, 2) -0.03505152799121447\n",
      "(13, 3) -0.06238842802375188\n",
      "(13, 4) -0.25389456763669216\n",
      "(13, 5) -0.45015283784977095\n",
      "(13, 6) 0.021598014177115484\n",
      "(13, 7) -0.04153442354493109\n",
      "(13, 8) 0.027263787805154035\n",
      "(13, 9) -0.12418952270998317\n",
      "(13, 10) -0.05693183999788686\n",
      "(13, 11) -0.37171472557773194\n",
      "(13, 12) 0.2723539937132813\n",
      "(13, 13) -0.069794196555506\n",
      "(13, 14) 0.07416548553074165\n",
      "(13, 15) -0.16393497919153788\n",
      "(13, 16) 0.2827862571663786\n",
      "(13, 17) 0.08253070475916502\n",
      "(13, 18) -0.12596477678705753\n",
      "(13, 19) 0.1691937618897299\n",
      "(13, 20) 0.016088805843139653\n",
      "(13, 21) 0.09759409040910326\n",
      "(13, 22) 0.1325004480801084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 23) 0.08905955306559575\n",
      "(13, 24) -0.006037968613981092\n",
      "(13, 25) 0.5600826838758621\n",
      "(13, 26) -0.044457426096045076\n",
      "(13, 27) -0.406552796450299\n",
      "(13, 28) 0.0741146963356698\n",
      "(13, 29) -0.03502230021545927\n",
      "(14, 0) 0.1722243290647185\n",
      "(14, 1) -0.26282607823446824\n",
      "(14, 2) 0.1085879491213859\n",
      "(14, 3) 0.30462892421923016\n",
      "(14, 4) 0.043266973381861355\n",
      "(14, 5) 0.16425180611179258\n",
      "(14, 6) 0.18780280384156353\n",
      "(14, 7) -0.12588335156493713\n",
      "(14, 8) -0.043251040748870644\n",
      "(14, 9) 0.3977634909446692\n",
      "(14, 10) 0.05847043009765684\n",
      "(14, 11) 0.20290750457618853\n",
      "(14, 12) 0.04292281579054701\n",
      "(14, 13) -0.13684095971910892\n",
      "(14, 14) 0.2868103555986323\n",
      "(14, 15) -0.28771093489865507\n",
      "(14, 16) 0.14675308843692392\n",
      "(14, 17) -0.158349948886638\n",
      "(14, 18) -0.10725230423780373\n",
      "(14, 19) -0.10459800456885658\n",
      "(14, 20) -0.0134233727244748\n",
      "(14, 21) 0.15582845183104155\n",
      "(14, 22) 0.11685756553525549\n",
      "(14, 23) -0.013056073466799488\n",
      "(14, 24) -0.062492970309691025\n",
      "(14, 25) -0.10246953707770955\n",
      "(14, 26) -0.051957688462778144\n",
      "(14, 27) -0.5734733483642174\n",
      "(14, 28) 0.008770086123988108\n",
      "(14, 29) -0.19995009927775695\n",
      "(15, 0) 0.06776217356119218\n",
      "(15, 1) -0.04708202858694221\n",
      "(15, 2) 0.09766723572113277\n",
      "(15, 3) -0.21587930176814038\n",
      "(15, 4) 0.05069486936903899\n",
      "(15, 5) -0.21979151192397237\n",
      "(15, 6) -0.10656248621287999\n",
      "(15, 7) 0.3151604641260519\n",
      "(15, 8) -0.17350526029247246\n",
      "(15, 9) -0.2890047985371069\n",
      "(15, 10) -0.35788266270486696\n",
      "(15, 11) -0.1324581866857244\n",
      "(15, 12) 0.018153548264621122\n",
      "(15, 13) -0.0647960439081885\n",
      "(15, 14) -0.010329484689819424\n",
      "(15, 15) 0.11864043236720077\n",
      "(15, 16) 0.08849476760275364\n",
      "(15, 17) 0.018423779213350144\n",
      "(15, 18) 0.04175578927068102\n",
      "(15, 19) 0.026257973750887228\n",
      "(15, 20) 0.009722143756718538\n",
      "(15, 21) 0.023720995612208636\n",
      "(15, 22) -0.09086370531541375\n",
      "(15, 23) -0.19072656298746435\n",
      "(15, 24) 0.16767986710242155\n",
      "(15, 25) 0.03067693721270359\n",
      "(15, 26) -0.025376972390134252\n",
      "(15, 27) 0.17961750913286775\n",
      "(15, 28) 0.004516099050277944\n",
      "(15, 29) -0.012953908612445273\n",
      "(16, 0) 0.05122365935861239\n",
      "(16, 1) -0.013476031490711192\n",
      "(16, 2) 0.13267478018441636\n",
      "(16, 3) 0.0374055207696955\n",
      "(16, 4) -0.06489562753841938\n",
      "(16, 5) -0.06491026880439676\n",
      "(16, 6) 0.020226549057866805\n",
      "(16, 7) 0.032469548116864644\n",
      "(16, 8) -0.1370497700214912\n",
      "(16, 9) 0.09097695463644583\n",
      "(16, 10) 0.040098145248634864\n",
      "(16, 11) -0.014262922487517924\n",
      "(16, 12) -0.07593012663065224\n",
      "(16, 13) 0.04427681625784884\n",
      "(16, 14) 0.2233007188934266\n",
      "(16, 15) 0.15203482819359238\n",
      "(16, 16) 0.05326104943748077\n",
      "(16, 17) -0.12974915577501633\n",
      "(16, 18) -0.17834865273513853\n",
      "(16, 19) 0.13871741120219383\n",
      "(16, 20) 0.00745183181827258\n",
      "(16, 21) 0.10098689262605375\n",
      "(16, 22) 0.23281473544578543\n",
      "(16, 23) 0.1195523139596588\n",
      "(16, 24) 0.13171281967316872\n",
      "(16, 25) -0.11828193997587276\n",
      "(16, 26) 0.10208380167853191\n",
      "(16, 27) 0.041108289483560156\n",
      "(16, 28) 0.03375933674831799\n",
      "(16, 29) -0.16492814323143534\n",
      "(17, 0) -0.30832034521033336\n",
      "(17, 1) -0.19055673394774428\n",
      "(17, 2) 0.07057311597336025\n",
      "(17, 3) 0.1456008574152179\n",
      "(17, 4) -0.294096226749474\n",
      "(17, 5) 0.2233287867525746\n",
      "(17, 6) 0.28604225974504516\n",
      "(17, 7) 0.2711276380740202\n",
      "(17, 8) -0.252050562954409\n",
      "(17, 9) -0.03985127579042569\n",
      "(17, 10) -0.14443620579562833\n",
      "(17, 11) 0.19241958750804142\n",
      "(17, 12) 0.03599109108520793\n",
      "(17, 13) 0.13205447357833577\n",
      "(17, 14) -0.10679893471809974\n",
      "(17, 15) 0.049010667657611855\n",
      "(17, 16) 0.21560971350531585\n",
      "(17, 17) 0.2372286119545208\n",
      "(17, 18) 0.16574453436390968\n",
      "(17, 19) 0.1492348818743494\n",
      "(17, 20) 0.3452639650625144\n",
      "(17, 21) -0.1385511150697738\n",
      "(17, 22) -0.038491425424780346\n",
      "(17, 23) -0.11318886317113196\n",
      "(17, 24) 0.43643470553078595\n",
      "(17, 25) -0.06333802198277283\n",
      "(17, 26) -0.00014217844679365044\n",
      "(17, 27) -0.358429120916881\n",
      "(17, 28) 0.2114660184293626\n",
      "(17, 29) -0.2544372752932844\n",
      "(18, 0) 0.12514585248091237\n",
      "(18, 1) 0.07692604988562834\n",
      "(18, 2) 0.1688607393024455\n",
      "(18, 3) -0.12968089828646612\n",
      "(18, 4) -0.10324412000883852\n",
      "(18, 5) 0.24035601966509998\n",
      "(18, 6) -0.024594926006926695\n",
      "(18, 7) -0.23270111508821853\n",
      "(18, 8) 0.16659551267395045\n",
      "(18, 9) -0.14609348264471578\n",
      "(18, 10) 0.3410356033128891\n",
      "(18, 11) 0.10416015427949786\n",
      "(18, 12) -0.020165852188114286\n",
      "(18, 13) -0.174663965069044\n",
      "(18, 14) 0.019123669892096018\n",
      "(18, 15) -0.14289398997391345\n",
      "(18, 16) 0.16491350263159177\n",
      "(18, 17) -0.1683907810967611\n",
      "(18, 18) 0.1941037594654915\n",
      "(18, 19) -0.15549939758052744\n",
      "(18, 20) 0.03507635537580711\n",
      "(18, 21) -0.1815510938030229\n",
      "(18, 22) -0.057877097248137936\n",
      "(18, 23) 0.1704432921023624\n",
      "(18, 24) -0.2471414034754815\n",
      "(18, 25) -0.2733194647497328\n",
      "(18, 26) -0.2978594085600861\n",
      "(18, 27) 0.32340422144727654\n",
      "(18, 28) -0.024254524166877896\n",
      "(18, 29) -0.27447621802068056\n",
      "(19, 0) 0.1745107629069764\n",
      "(19, 1) 0.05265120961439606\n",
      "(19, 2) 0.16890117620071976\n",
      "(19, 3) -0.3603540281282846\n",
      "(19, 4) -0.2906076817410508\n",
      "(19, 5) 0.04175113659243834\n",
      "(19, 6) -0.2742848261849673\n",
      "(19, 7) 0.018126824397057817\n",
      "(19, 8) 0.04405404845364557\n",
      "(19, 9) 0.029916270172236633\n",
      "(19, 10) 0.03707136708719361\n",
      "(19, 11) 0.04652782674696709\n",
      "(19, 12) -0.07797254517605268\n",
      "(19, 13) -0.10817626900916365\n",
      "(19, 14) -0.23035463199150283\n",
      "(19, 15) 0.07766455487612234\n",
      "(19, 16) 0.22129557657457608\n",
      "(19, 17) 0.03357062978714964\n",
      "(19, 18) 0.3475049894063886\n",
      "(19, 19) -0.09489376280313876\n",
      "(19, 20) -0.12413212560069552\n",
      "(19, 21) -0.06385329611724444\n",
      "(19, 22) 0.08166670872888915\n",
      "(19, 23) -0.11252569196429361\n",
      "(19, 24) 0.22345546373259137\n",
      "(19, 25) 0.07259078231847127\n",
      "(19, 26) -0.013592834768871851\n",
      "(19, 27) 0.141584090229685\n",
      "(19, 28) -0.06133731806912123\n",
      "(19, 29) -0.3553545268264457\n",
      "W2 relative error: 3.00e-06\n",
      "(0, 0) -0.30009174194489674\n",
      "(0, 1) 0.05254156505429818\n",
      "(0, 2) 0.2737426225252193\n",
      "(0, 3) 0.027214781272988372\n",
      "(0, 4) 0.0053383203635348755\n",
      "(0, 5) -0.2494933275976052\n",
      "(0, 6) 0.008926303873835195\n",
      "(0, 7) -0.2827269070859728\n",
      "(0, 8) -0.12381609377776213\n",
      "(0, 9) -0.03165243764868819\n",
      "(1, 0) -0.29779535828389214\n",
      "(1, 1) 0.2854686618647406\n",
      "(1, 2) 0.16910351976484606\n",
      "(1, 3) 0.004904747585854352\n",
      "(1, 4) 0.08790789873813763\n",
      "(1, 5) -0.005126114821507599\n",
      "(1, 6) 0.02672744172649288\n",
      "(1, 7) 0.005415516257656349\n",
      "(1, 8) 0.1129569615621051\n",
      "(1, 9) 0.06101199785391031\n",
      "(2, 0) -0.3508555074915875\n",
      "(2, 1) -0.03317298529204038\n",
      "(2, 2) 0.16275407213051096\n",
      "(2, 3) -0.18432100863385867\n",
      "(2, 4) -0.17277966608553183\n",
      "(2, 5) 0.008511561766511022\n",
      "(2, 6) 0.2726598318236739\n",
      "(2, 7) 0.05603542865273425\n",
      "(2, 8) 0.07211757035996413\n",
      "(2, 9) 0.3207952872230635\n",
      "(3, 0) 0.11320537067760482\n",
      "(3, 1) -0.09605485726815742\n",
      "(3, 2) -0.045033021933704724\n",
      "(3, 3) -0.23431434774678903\n",
      "(3, 4) 0.0713219751258265\n",
      "(3, 5) -0.0020117043586509453\n",
      "(3, 6) 0.25028200356658203\n",
      "(3, 7) -0.2146295771776607\n",
      "(3, 8) 0.03657723550709591\n",
      "(3, 9) 0.005090224153292411\n",
      "(4, 0) -0.11103778074783575\n",
      "(4, 1) 0.23005132057107855\n",
      "(4, 2) -0.1469846576451772\n",
      "(4, 3) -0.08411301584132502\n",
      "(4, 4) -0.02882922731650694\n",
      "(4, 5) 0.11013646523672092\n",
      "(4, 6) 0.05428010014796313\n",
      "(4, 7) 0.112204233859714\n",
      "(4, 8) 0.0055623591510567385\n",
      "(4, 9) 0.13778926342133957\n",
      "(5, 0) -0.3172481544577721\n",
      "(5, 1) 0.13549539592538906\n",
      "(5, 2) 0.21073445322095094\n",
      "(5, 3) -0.031496001717812305\n",
      "(5, 4) 0.18010570093096587\n",
      "(5, 5) -0.20510194360667808\n",
      "(5, 6) -0.2880907666646948\n",
      "(5, 7) -0.07084717705829746\n",
      "(5, 8) 0.1256159845031135\n",
      "(5, 9) -0.1310954452105051\n",
      "(6, 0) 0.009755646601661283\n",
      "(6, 1) -0.059898157678262\n",
      "(6, 2) 0.22685504887931526\n",
      "(6, 3) -0.2861452918612173\n",
      "(6, 4) -0.05658456450063908\n",
      "(6, 5) 0.13493112600571067\n",
      "(6, 6) -0.2970191919082765\n",
      "(6, 7) -0.1970838490183979\n",
      "(6, 8) 0.26903125611710266\n",
      "(6, 9) 0.2351576243064812\n",
      "(7, 0) 0.07685272920276986\n",
      "(7, 1) 0.09691715554538403\n",
      "(7, 2) -0.05727395104848653\n",
      "(7, 3) -0.11889506756901368\n",
      "(7, 4) 0.11534641592980675\n",
      "(7, 5) -0.3573232814968463\n",
      "(7, 6) -0.16768798789534856\n",
      "(7, 7) -0.18189683030911394\n",
      "(7, 8) -0.07178520582407089\n",
      "(7, 9) 0.23694131741081034\n",
      "(8, 0) -0.22463360349611602\n",
      "(8, 1) -0.20525260864268088\n",
      "(8, 2) 0.11289326904417861\n",
      "(8, 3) 0.20349861880042394\n",
      "(8, 4) 0.11344082220254846\n",
      "(8, 5) -0.2640347649851549\n",
      "(8, 6) -0.2523699152323644\n",
      "(8, 7) 0.2409865703434377\n",
      "(8, 8) 0.07540361504076998\n",
      "(8, 9) 0.009381515697626241\n",
      "(9, 0) -0.18715413805381328\n",
      "(9, 1) 0.033855434988794286\n",
      "(9, 2) 0.09064913308876042\n",
      "(9, 3) -0.17936381326855153\n",
      "(9, 4) -0.13780252872130916\n",
      "(9, 5) 0.186725493600548\n",
      "(9, 6) -0.008821625829824598\n",
      "(9, 7) -0.08372502677822524\n",
      "(9, 8) -0.15318574781986172\n",
      "(9, 9) 0.37598124009718953\n",
      "(10, 0) 0.030547695262228555\n",
      "(10, 1) 0.15380729037062224\n",
      "(10, 2) -0.09982857420709478\n",
      "(10, 3) 0.24482318616847462\n",
      "(10, 4) -0.061988598165640944\n",
      "(10, 5) -0.12247566649747908\n",
      "(10, 6) -0.1483035604987748\n",
      "(10, 7) -0.3918719424511607\n",
      "(10, 8) -0.007879058916415715\n",
      "(10, 9) -0.05546661605571045\n",
      "(11, 0) -0.36510911147757946\n",
      "(11, 1) 0.023140951022071473\n",
      "(11, 2) 0.012716674735457898\n",
      "(11, 3) -0.05083874961009371\n",
      "(11, 4) 0.03655828928472715\n",
      "(11, 5) 0.13492212835863882\n",
      "(11, 6) 0.34458947739679496\n",
      "(11, 7) 0.08291726936171528\n",
      "(11, 8) -0.015060767299956977\n",
      "(11, 9) 0.052236316028242406\n",
      "(12, 0) -0.14109681725216205\n",
      "(12, 1) 0.1859787571500959\n",
      "(12, 2) 0.11715328152028802\n",
      "(12, 3) 0.10651631527913706\n",
      "(12, 4) 0.5076063612108328\n",
      "(12, 5) -0.10263835710233592\n",
      "(12, 6) -0.05005979595296139\n",
      "(12, 7) -0.07673639230709739\n",
      "(12, 8) 0.1434504198094544\n",
      "(12, 9) 0.13086829944164435\n",
      "(13, 0) -0.27469196131058027\n",
      "(13, 1) 0.034179588670468775\n",
      "(13, 2) 0.0697395840631998\n",
      "(13, 3) -0.19028396303077952\n",
      "(13, 4) -0.15211301094275598\n",
      "(13, 5) 0.027435608096482152\n",
      "(13, 6) 0.3802925704565751\n",
      "(13, 7) 0.25104282457277804\n",
      "(13, 8) -0.07415225136142567\n",
      "(13, 9) -0.06616730097164236\n",
      "(14, 0) -0.047363252786425385\n",
      "(14, 1) -0.07887134834128062\n",
      "(14, 2) 0.052751857371191584\n",
      "(14, 3) -0.09216365013386961\n",
      "(14, 4) 0.16473335291422586\n",
      "(14, 5) -0.29221172423987696\n",
      "(14, 6) 0.02138466954626494\n",
      "(14, 7) -0.009488931951295854\n",
      "(14, 8) 0.18193205630900874\n",
      "(14, 9) 0.2511287644946236\n",
      "(15, 0) -0.28271297631832226\n",
      "(15, 1) 0.04102038664299812\n",
      "(15, 2) 0.2026728335380312\n",
      "(15, 3) 0.10011934237219576\n",
      "(15, 4) -0.05568733936200942\n",
      "(15, 5) -0.09244105756422981\n",
      "(15, 6) 0.08014627717045641\n",
      "(15, 7) 0.23779118665245844\n",
      "(15, 8) 0.04713243360043861\n",
      "(15, 9) 0.1624653434006973\n",
      "(16, 0) -0.20776805476074853\n",
      "(16, 1) 0.018095519571659224\n",
      "(16, 2) 0.1961811664674684\n",
      "(16, 3) -0.18253168176940446\n",
      "(16, 4) 0.1215048166969268\n",
      "(16, 5) 0.14710799560724297\n",
      "(16, 6) 0.3898362036736102\n",
      "(16, 7) 0.26779461750336964\n",
      "(16, 8) -0.16045791504382123\n",
      "(16, 9) -0.01587365465738344\n",
      "(17, 0) 0.02537729413276679\n",
      "(17, 1) -0.1374271684895234\n",
      "(17, 2) 0.13669324521181636\n",
      "(17, 3) 0.3131119758581491\n",
      "(17, 4) 0.09043649993323298\n",
      "(17, 5) -0.3062979848422742\n",
      "(17, 6) -0.06759925130772615\n",
      "(17, 7) -0.6485335031669592\n",
      "(17, 8) 0.16814147403287905\n",
      "(17, 9) -0.1745882511006158\n",
      "(18, 0) -0.6093939722173758\n",
      "(18, 1) 0.13850203108844994\n",
      "(18, 2) -0.1600483361841043\n",
      "(18, 3) 0.0699384049784868\n",
      "(18, 4) 0.039477519386821314\n",
      "(18, 5) -0.1471135318897865\n",
      "(18, 6) -0.09641271629945435\n",
      "(18, 7) 0.045679360116679384\n",
      "(18, 8) 0.06025453362923371\n",
      "(18, 9) -0.2528545623547984\n",
      "(19, 0) -0.0036368152933619054\n",
      "(19, 1) -0.1738607840806594\n",
      "(19, 2) -0.023609932053858525\n",
      "(19, 3) -0.17125921005778363\n",
      "(19, 4) -0.0455394730369818\n",
      "(19, 5) 0.1894142617864247\n",
      "(19, 6) 0.284826269680849\n",
      "(19, 7) -0.2707693550263457\n",
      "(19, 8) -0.14080118724280055\n",
      "(19, 9) 0.045523752811860156\n",
      "(20, 0) 0.3386355326373547\n",
      "(20, 1) 0.11817106799760778\n",
      "(20, 2) 0.0043458578513622115\n",
      "(20, 3) -0.17346110281479807\n",
      "(20, 4) -0.06458326535785375\n",
      "(20, 5) -0.027014159798000034\n",
      "(20, 6) 0.12224871657195989\n",
      "(20, 7) -0.22811195576899476\n",
      "(20, 8) 0.014622480959403104\n",
      "(20, 9) 0.09664711422274762\n",
      "(21, 0) -0.051215041230179274\n",
      "(21, 1) -0.2094176721811891\n",
      "(21, 2) -0.12980489896285974\n",
      "(21, 3) 0.08395316237752581\n",
      "(21, 4) 0.045444034135755366\n",
      "(21, 5) 0.07278240254926516\n",
      "(21, 6) 0.16697936029430593\n",
      "(21, 7) -0.07029352397935895\n",
      "(21, 8) 0.05827757361132057\n",
      "(21, 9) 0.2660910426222074\n",
      "(22, 0) -0.42106726847457304\n",
      "(22, 1) 0.008855543232044738\n",
      "(22, 2) -0.05321709926064954\n",
      "(22, 3) 0.04065470582403918\n",
      "(22, 4) 0.259701635263454\n",
      "(22, 5) 0.047015598569899446\n",
      "(22, 6) 0.059209037095442334\n",
      "(22, 7) 0.14027546493267096\n",
      "(22, 8) 0.07568570912219741\n",
      "(22, 9) 0.08985647128589845\n",
      "(23, 0) 0.09193959860098742\n",
      "(23, 1) 0.04859955473612842\n",
      "(23, 2) -0.0009242917542451322\n",
      "(23, 3) -0.07701317903396898\n",
      "(23, 4) 0.1734474191827928\n",
      "(23, 5) -0.07953695351226031\n",
      "(23, 6) 0.030444706977661436\n",
      "(23, 7) -0.3778547685406863\n",
      "(23, 8) -0.06419863805007253\n",
      "(23, 9) 0.14359354043591566\n",
      "(24, 0) -0.0660801150687007\n",
      "(24, 1) 0.14381731876511594\n",
      "(24, 2) 0.09110846650450809\n",
      "(24, 3) 0.096647574876485\n",
      "(24, 4) 0.23131945487975256\n",
      "(24, 5) 0.06214932977144371\n",
      "(24, 6) 0.2867152451901944\n",
      "(24, 7) -0.39796456410279285\n",
      "(24, 8) 0.1661250786266777\n",
      "(24, 9) 0.27051471067629507\n",
      "(25, 0) -0.43010774337659535\n",
      "(25, 1) -0.006571021948786892\n",
      "(25, 2) 0.03511547790324698\n",
      "(25, 3) 0.0904521784139689\n",
      "(25, 4) 0.04432915505780954\n",
      "(25, 5) -0.06433319712506602\n",
      "(25, 6) -0.14975275810513722\n",
      "(25, 7) 0.15400861332892646\n",
      "(25, 8) 0.11681439096022926\n",
      "(25, 9) -0.19296445534777715\n",
      "(26, 0) -0.8191434805571162\n",
      "(26, 1) 0.14617106747216724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 2) -0.14690933527461425\n",
      "(26, 3) 0.28277775232510294\n",
      "(26, 4) -0.03654184403956151\n",
      "(26, 5) -0.061895250347276935\n",
      "(26, 6) -0.13565172167773198\n",
      "(26, 7) 0.06698785490755199\n",
      "(26, 8) 0.01554524589053585\n",
      "(26, 9) -0.02983942919421167\n",
      "(27, 0) -0.07337113983041377\n",
      "(27, 1) 0.4113577852571381\n",
      "(27, 2) 0.07378934929747061\n",
      "(27, 3) 0.06035955619765331\n",
      "(27, 4) 0.09919438674899082\n",
      "(27, 5) -0.11630255736250204\n",
      "(27, 6) -0.24843855297440595\n",
      "(27, 7) -0.10743301879223564\n",
      "(27, 8) 0.05401138953864403\n",
      "(27, 9) -0.17532816922738445\n",
      "(28, 0) -0.02328383397198763\n",
      "(28, 1) -0.07690665833059995\n",
      "(28, 2) -0.11777440827032136\n",
      "(28, 3) 0.02113822610283478\n",
      "(28, 4) -0.0626694377281467\n",
      "(28, 5) 0.23848483747812563\n",
      "(28, 6) 0.05149739035203992\n",
      "(28, 7) -0.37658260660933246\n",
      "(28, 8) -0.009545654977571871\n",
      "(28, 9) 0.2066579877446628\n",
      "(29, 0) -0.10592756556349057\n",
      "(29, 1) -0.041102341397092346\n",
      "(29, 2) 0.12541648350961054\n",
      "(29, 3) -0.2014640275049828\n",
      "(29, 4) 0.1507739507022876\n",
      "(29, 5) 0.013162544965084065\n",
      "(29, 6) 0.1057762699652187\n",
      "(29, 7) -0.3091672628574571\n",
      "(29, 8) 0.1533449202284487\n",
      "(29, 9) -0.13630556550126016\n",
      "W3 relative error: 1.71e-08\n",
      "(0,) 0.0\n",
      "(1,) 0.0\n",
      "(2,) 0.0\n",
      "(3,) 0.0\n",
      "(4,) 0.0\n",
      "(5,) 0.0\n",
      "(6,) 0.0\n",
      "(7,) 0.0\n",
      "(8,) 0.0\n",
      "(9,) 0.0\n",
      "(10,) 0.0\n",
      "(11,) 0.0\n",
      "(12,) 0.0\n",
      "(13,) 0.0\n",
      "(14,) 0.0\n",
      "(15,) 0.0\n",
      "(16,) 0.0\n",
      "(17,) 0.0\n",
      "(18,) 0.0\n",
      "(19,) 0.0\n",
      "b1 relative error: 2.78e-09\n",
      "(0,) 0.0\n",
      "(1,) 0.0\n",
      "(2,) 0.0\n",
      "(3,) 0.0\n",
      "(4,) 0.0\n",
      "(5,) 0.0\n",
      "(6,) 0.0\n",
      "(7,) 0.0\n",
      "(8,) 0.0\n",
      "(9,) 0.0\n",
      "(10,) 0.0\n",
      "(11,) 0.0\n",
      "(12,) 0.0\n",
      "(13,) 0.0\n",
      "(14,) 0.0\n",
      "(15,) 0.0\n",
      "(16,) 0.0\n",
      "(17,) 0.0\n",
      "(18,) 0.0\n",
      "(19,) 0.0\n",
      "(20,) 0.0\n",
      "(21,) 0.0\n",
      "(22,) 0.0\n",
      "(23,) 0.0\n",
      "(24,) 0.0\n",
      "(25,) 0.0\n",
      "(26,) 0.0\n",
      "(27,) 0.0\n",
      "(28,) 0.0\n",
      "(29,) 0.0\n",
      "b2 relative error: 2.22e-08\n",
      "(0,) -0.4072468673399498\n",
      "(1,) 0.10158929626413736\n",
      "(2,) 0.10398682555035065\n",
      "(3,) 0.08581693657916388\n",
      "(4,) 0.1071470380420436\n",
      "(5,) 0.08066826882036082\n",
      "(6,) 0.10047217560149589\n",
      "(7,) -0.38290323916001506\n",
      "(8,) 0.10142794093503936\n",
      "(9,) 0.10904162470737332\n",
      "b3 relative error: 1.36e-10\n",
      "(0,) -0.010990616416961528\n",
      "(1,) 0.005229057276068261\n",
      "(2,) -0.008784245419946046\n",
      "(3,) 0.023296011342210928\n",
      "(4,) 0.013164897172401878\n",
      "(5,) 0.013916316810025363\n",
      "(6,) 0.020989700466600425\n",
      "(7,) 0.014906275724868577\n",
      "(8,) -0.023791432735009718\n",
      "(9,) -0.011703480717883961\n",
      "(10,) 0.033920782405161276\n",
      "(11,) -0.004264490938155063\n",
      "(12,) -0.05468968504729332\n",
      "(13,) 0.011646915742957729\n",
      "(14,) 0.03784876305701346\n",
      "(15,) -0.015307110201590033\n",
      "(16,) -0.03587225636536573\n",
      "(17,) 0.0072226166381028625\n",
      "(18,) 0.002511268615279505\n",
      "(19,) -0.02580581996980413\n",
      "beta1 relative error: 1.17e-08\n",
      "(0,) -0.01269855745000825\n",
      "(1,) 0.004044052426266376\n",
      "(2,) 0.008629926773195962\n",
      "(3,) -0.016247835787908116\n",
      "(4,) -0.02902693942985479\n",
      "(5,) -0.006230597149325944\n",
      "(6,) -0.016999332341782747\n",
      "(7,) -0.02468280984047055\n",
      "(8,) -0.01792239601172696\n",
      "(9,) -0.02128996992034615\n",
      "(10,) 0.004608767145697357\n",
      "(11,) 0.011497314345021435\n",
      "(12,) -0.021746445888481954\n",
      "(13,) -0.0053690996537625315\n",
      "(14,) -0.0413937608634285\n",
      "(15,) 0.002030139656383767\n",
      "(16,) -0.0074061376587053465\n",
      "(17,) 0.04332220493452609\n",
      "(18,) 0.03202860114903672\n",
      "(19,) -0.011617394335416973\n",
      "(20,) -0.00971779545722029\n",
      "(21,) -0.03336355907102018\n",
      "(22,) 0.01989824403025864\n",
      "(23,) 0.009685360646827235\n",
      "(24,) 0.028524896533710372\n",
      "(25,) 0.01130526952053401\n",
      "(26,) 0.06846270226290585\n",
      "(27,) -0.026778320760811877\n",
      "(28,) 0.007419889858084615\n",
      "(29,) -0.003528567393829007\n",
      "beta2 relative error: 1.12e-08\n",
      "(0,) -0.007744596075553999\n",
      "(1,) 0.003696728523294723\n",
      "(2,) -0.00621067215433868\n",
      "(3,) 0.016455468809795093\n",
      "(4,) 0.009304780945740276\n",
      "(5,) 0.009822853286323152\n",
      "(6,) 0.014815989679561879\n",
      "(7,) 0.010538884254174263\n",
      "(8,) -0.01680366121803445\n",
      "(9,) -0.00827530048930214\n",
      "(10,) 0.023981817731666407\n",
      "(11,) -0.003015177929555079\n",
      "(12,) -0.038264003610066766\n",
      "(13,) 0.008221568981170435\n",
      "(14,) 0.026760629223687712\n",
      "(15,) -0.010821990148457415\n",
      "(16,) -0.025359236222044498\n",
      "(17,) 0.005106188494252706\n",
      "(18,) 0.0017752924463820816\n",
      "(19,) -0.01824554169971293\n",
      "gamma1 relative error: 8.69e-09\n",
      "(0,) -0.00897742542527169\n",
      "(1,) 0.0028591090384111335\n",
      "(2,) 0.006035202293475094\n",
      "(3,) -0.011487707229917985\n",
      "(4,) -0.020517201138048335\n",
      "(5,) -0.004399332276250334\n",
      "(6,) -0.012018779660394328\n",
      "(7,) -0.01744264883640767\n",
      "(8,) -0.012666263060623349\n",
      "(9,) -0.01469603136960984\n",
      "(10,) 0.003256004355023378\n",
      "(11,) 0.008104514215290237\n",
      "(12,) -0.01537223428549339\n",
      "(13,) -0.0037553680609647695\n",
      "(14,) -0.028847639699236535\n",
      "(15,) 0.001434988661230818\n",
      "(16,) -0.005205028186239247\n",
      "(17,) 0.03062024847011457\n",
      "(18,) 0.022552382272778\n",
      "(19,) -0.008203250345673041\n",
      "(20,) -0.006867246016284411\n",
      "(21,) -0.023586955100185488\n",
      "(22,) 0.01406232779110894\n",
      "(23,) 0.006847596933923227\n",
      "(24,) 0.02011151423175761\n",
      "(25,) 0.007987291361644111\n",
      "(26,) 0.04808801947753238\n",
      "(27,) -0.016729006624416343\n",
      "(28,) 0.005244008782767651\n",
      "(29,) -0.0024943078713590694\n",
      "gamma2 relative error: 1.31e-08\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "# You should expect losses between 1e-4~1e-10 for W, \n",
    "# losses between 1e-08~1e-10 for b,\n",
    "# and losses between 1e-08~1e-09 for beta and gammas.\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64,\n",
    "                            normalization='batchnorm')\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "  if reg == 0: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm for deep networks\n",
    "Run the following to train a six-layer network on a subset of 1000 training examples both with and without batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver with batch norm:\n",
      "(Iteration 1 / 200) loss: 2.340512\n",
      "(Epoch 0 / 10) train acc: 0.107000; val_acc: 0.116000\n",
      "(Epoch 1 / 10) train acc: 0.317000; val_acc: 0.260000\n",
      "(Iteration 21 / 200) loss: 2.002910\n",
      "(Epoch 2 / 10) train acc: 0.417000; val_acc: 0.285000\n",
      "(Iteration 41 / 200) loss: 2.030562\n",
      "(Epoch 3 / 10) train acc: 0.493000; val_acc: 0.294000\n",
      "(Iteration 61 / 200) loss: 1.776793\n",
      "(Epoch 4 / 10) train acc: 0.553000; val_acc: 0.311000\n",
      "(Iteration 81 / 200) loss: 1.298337\n",
      "(Epoch 5 / 10) train acc: 0.571000; val_acc: 0.307000\n",
      "(Iteration 101 / 200) loss: 1.265137\n",
      "(Epoch 6 / 10) train acc: 0.627000; val_acc: 0.329000\n",
      "(Iteration 121 / 200) loss: 1.060890\n",
      "(Epoch 7 / 10) train acc: 0.657000; val_acc: 0.318000\n",
      "(Iteration 141 / 200) loss: 1.124555\n",
      "(Epoch 8 / 10) train acc: 0.666000; val_acc: 0.292000\n",
      "(Iteration 161 / 200) loss: 0.770253\n",
      "(Epoch 9 / 10) train acc: 0.756000; val_acc: 0.322000\n",
      "(Iteration 181 / 200) loss: 0.888094\n",
      "(Epoch 10 / 10) train acc: 0.786000; val_acc: 0.309000\n",
      "\n",
      "Solver without batch norm:\n",
      "(Iteration 1 / 200) loss: 2.302332\n",
      "(Epoch 0 / 10) train acc: 0.129000; val_acc: 0.131000\n",
      "(Epoch 1 / 10) train acc: 0.283000; val_acc: 0.250000\n",
      "(Iteration 21 / 200) loss: 2.041970\n",
      "(Epoch 2 / 10) train acc: 0.316000; val_acc: 0.277000\n",
      "(Iteration 41 / 200) loss: 1.900473\n",
      "(Epoch 3 / 10) train acc: 0.373000; val_acc: 0.282000\n",
      "(Iteration 61 / 200) loss: 1.713156\n",
      "(Epoch 4 / 10) train acc: 0.390000; val_acc: 0.310000\n",
      "(Iteration 81 / 200) loss: 1.662209\n",
      "(Epoch 5 / 10) train acc: 0.434000; val_acc: 0.300000\n",
      "(Iteration 101 / 200) loss: 1.696062\n",
      "(Epoch 6 / 10) train acc: 0.536000; val_acc: 0.346000\n",
      "(Iteration 121 / 200) loss: 1.550785\n",
      "(Epoch 7 / 10) train acc: 0.530000; val_acc: 0.310000\n",
      "(Iteration 141 / 200) loss: 1.436308\n",
      "(Epoch 8 / 10) train acc: 0.622000; val_acc: 0.342000\n",
      "(Iteration 161 / 200) loss: 1.000868\n",
      "(Epoch 9 / 10) train acc: 0.654000; val_acc: 0.328000\n",
      "(Iteration 181 / 200) loss: 0.925455\n",
      "(Epoch 10 / 10) train acc: 0.726000; val_acc: 0.335000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [100, 100, 100, 100, 100]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 2e-2\n",
    "bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization='batchnorm')\n",
    "model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)\n",
    "\n",
    "print('Solver with batch norm:')\n",
    "bn_solver = Solver(bn_model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True,print_every=20)\n",
    "bn_solver.train()\n",
    "\n",
    "print('\\nSolver without batch norm:')\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to visualize the results from two networks trained above. You should find that using batch normalization helps the network to converge much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_training_history(title, label, baseline, bn_solvers, plot_fn, bl_marker='.', bn_marker='.', labels=None):\n",
    "    \"\"\"utility function for plotting training history\"\"\"\n",
    "    plt.title(title)\n",
    "    plt.xlabel(label)\n",
    "    bn_plots = [plot_fn(bn_solver) for bn_solver in bn_solvers]\n",
    "    bl_plot = plot_fn(baseline)\n",
    "    num_bn = len(bn_plots)\n",
    "    for i in range(num_bn):\n",
    "        label='with_norm'\n",
    "        if labels is not None:\n",
    "            label += str(labels[i])\n",
    "        plt.plot(bn_plots[i], bn_marker, label=label)\n",
    "    label='baseline'\n",
    "    if labels is not None:\n",
    "        label += str(labels[0])\n",
    "    plt.plot(bl_plot, bl_marker, label=label)\n",
    "    plt.legend(loc='lower center', ncol=num_bn+1) \n",
    "\n",
    "    \n",
    "plt.subplot(3, 1, 1)\n",
    "plot_training_history('Training loss','Iteration', solver, [bn_solver], \\\n",
    "                      lambda x: x.loss_history, bl_marker='o', bn_marker='o')\n",
    "plt.subplot(3, 1, 2)\n",
    "plot_training_history('Training accuracy','Epoch', solver, [bn_solver], \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-o', bn_marker='-o')\n",
    "plt.subplot(3, 1, 3)\n",
    "plot_training_history('Validation accuracy','Epoch', solver, [bn_solver], \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-o', bn_marker='-o')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization and initialization\n",
    "We will now run a small experiment to study the interaction of batch normalization and weight initialization.\n",
    "\n",
    "The first cell will train 8-layer networks both with and without batch normalization using different scales for weight initialization. The second layer will plot training accuracy, validation set accuracy, and training loss as a function of the weight initialization scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [50, 50, 50, 50, 50, 50, 50]\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "bn_solvers_ws = {}\n",
    "solvers_ws = {}\n",
    "weight_scales = np.logspace(-4, 0, num=20)\n",
    "for i, weight_scale in enumerate(weight_scales):\n",
    "  print('Running weight scale %d / %d' % (i + 1, len(weight_scales)))\n",
    "  bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization='batchnorm')\n",
    "  model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)\n",
    "\n",
    "  bn_solver = Solver(bn_model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  bn_solver.train()\n",
    "  bn_solvers_ws[weight_scale] = bn_solver\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  solver.train()\n",
    "  solvers_ws[weight_scale] = solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot results of weight scale experiment\n",
    "best_train_accs, bn_best_train_accs = [], []\n",
    "best_val_accs, bn_best_val_accs = [], []\n",
    "final_train_loss, bn_final_train_loss = [], []\n",
    "\n",
    "for ws in weight_scales:\n",
    "  best_train_accs.append(max(solvers_ws[ws].train_acc_history))\n",
    "  bn_best_train_accs.append(max(bn_solvers_ws[ws].train_acc_history))\n",
    "  \n",
    "  best_val_accs.append(max(solvers_ws[ws].val_acc_history))\n",
    "  bn_best_val_accs.append(max(bn_solvers_ws[ws].val_acc_history))\n",
    "  \n",
    "  final_train_loss.append(np.mean(solvers_ws[ws].loss_history[-100:]))\n",
    "  bn_final_train_loss.append(np.mean(bn_solvers_ws[ws].loss_history[-100:]))\n",
    "  \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Best val accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best val accuracy')\n",
    "plt.semilogx(weight_scales, best_val_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_val_accs, '-o', label='batchnorm')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Best train accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best training accuracy')\n",
    "plt.semilogx(weight_scales, best_train_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_train_accs, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Final training loss vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Final training loss')\n",
    "plt.semilogx(weight_scales, final_train_loss, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_final_train_loss, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "plt.gca().set_ylim(1.0, 3.5)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Inline Question 1:\n",
    "Describe the results of this experiment. How does the scale of weight initialization affect models with/without batch normalization differently, and why?\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization and batch size\n",
    "We will now run a small experiment to study the interaction of batch normalization and batch size.\n",
    "\n",
    "The first cell will train 6-layer networks both with and without batch normalization using different batch sizes. The second layer will plot training accuracy and validation set accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "def run_batchsize_experiments(normalization_mode):\n",
    "    np.random.seed(231)\n",
    "    # Try training a very deep net with batchnorm\n",
    "    hidden_dims = [100, 100, 100, 100, 100]\n",
    "    num_train = 1000\n",
    "    small_data = {\n",
    "      'X_train': data['X_train'][:num_train],\n",
    "      'y_train': data['y_train'][:num_train],\n",
    "      'X_val': data['X_val'],\n",
    "      'y_val': data['y_val'],\n",
    "    }\n",
    "    n_epochs=10\n",
    "    weight_scale = 2e-2\n",
    "    batch_sizes = [5,10,50]\n",
    "    lr = 10**(-3.5)\n",
    "    solver_bsize = batch_sizes[0]\n",
    "\n",
    "    print('No normalization: batch size = ',solver_bsize)\n",
    "    model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)\n",
    "    solver = Solver(model, small_data,\n",
    "                    num_epochs=n_epochs, batch_size=solver_bsize,\n",
    "                    update_rule='adam',\n",
    "                    optim_config={\n",
    "                      'learning_rate': lr,\n",
    "                    },\n",
    "                    verbose=False)\n",
    "    solver.train()\n",
    "    \n",
    "    bn_solvers = []\n",
    "    for i in range(len(batch_sizes)):\n",
    "        b_size=batch_sizes[i]\n",
    "        print('Normalization: batch size = ',b_size)\n",
    "        bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=normalization_mode)\n",
    "        bn_solver = Solver(bn_model, small_data,\n",
    "                        num_epochs=n_epochs, batch_size=b_size,\n",
    "                        update_rule='adam',\n",
    "                        optim_config={\n",
    "                          'learning_rate': lr,\n",
    "                        },\n",
    "                        verbose=False)\n",
    "        bn_solver.train()\n",
    "        bn_solvers.append(bn_solver)\n",
    "        \n",
    "    return bn_solvers, solver, batch_sizes\n",
    "\n",
    "batch_sizes = [5,10,50]\n",
    "bn_solvers_bsize, solver_bsize, batch_sizes = run_batchsize_experiments('batchnorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plot_training_history('Training accuracy (Batch Normalization)','Epoch', solver_bsize, bn_solvers_bsize, \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_training_history('Validation accuracy (Batch Normalization)','Epoch', solver_bsize, bn_solvers_bsize, \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Inline Question 2:\n",
    "Describe the results of this experiment. What does this imply about the relationship between batch normalization and batch size? Why is this relationship observed?\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization\n",
    "Batch normalization has proved to be effective in making networks easier to train, but the dependency on batch size makes it less useful in complex networks which have a cap on the input batch size due to hardware limitations. \n",
    "\n",
    "Several alternatives to batch normalization have been proposed to mitigate this problem; one such technique is Layer Normalization [2]. Instead of normalizing over the batch, we normalize over the features. In other words, when using Layer Normalization, each feature vector corresponding to a single datapoint is normalized based on the sum of all terms within that feature vector.\n",
    "\n",
    "[2] [Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer Normalization.\" stat 1050 (2016): 21.](https://arxiv.org/pdf/1607.06450.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Inline Question 3:\n",
    "Which of these data preprocessing steps is analogous to batch normalization, and which is analogous to layer normalization?\n",
    "\n",
    "1. Scaling each image in the dataset, so that the RGB channels for each row of pixels within an image sums up to 1.\n",
    "2. Scaling each image in the dataset, so that the RGB channels for all pixels within an image sums up to 1.  \n",
    "3. Subtracting the mean image of the dataset from each image in the dataset.\n",
    "4. Setting all RGB values to either 0 or 1 depending on a given threshold.\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization: Implementation\n",
    "\n",
    "Now you'll implement layer normalization. This step should be relatively straightforward, as conceptually the implementation is almost identical to that of batch normalization. One significant difference though is that for layer normalization, we do not keep track of the moving moments, and the testing phase is identical to the training phase, where the mean and variance are directly calculated per datapoint.\n",
    "\n",
    "Here's what you need to do:\n",
    "\n",
    "* In `cs231n/layers.py`, implement the forward pass for layer normalization in the function `layernorm_backward`. \n",
    "\n",
    "Run the cell below to check your results.\n",
    "* In `cs231n/layers.py`, implement the backward pass for layer normalization in the function `layernorm_backward`. \n",
    "\n",
    "Run the second cell below to check your results.\n",
    "* Modify `cs231n/classifiers/fc_net.py` to add layer normalization to the `FullyConnectedNet`. When the `normalization` flag is set to `\"layernorm\"` in the constructor, you should insert a layer normalization layer before each ReLU nonlinearity. \n",
    "\n",
    "Run the third cell below to run the batch size experiment on layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after layer normalization   \n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 =4, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before layer normalization:')\n",
    "print_mean_std(a,axis=1)\n",
    "\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After layer normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = layernorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=1)\n",
    "\n",
    "gamma = np.asarray([3.0,3.0,3.0])\n",
    "beta = np.asarray([5.0,5.0,5.0])\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "print('After layer normalization (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = layernorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "ln_param = {}\n",
    "fx = lambda x: layernorm_forward(x, gamma, beta, ln_param)[0]\n",
    "fg = lambda a: layernorm_forward(x, a, beta, ln_param)[0]\n",
    "fb = lambda b: layernorm_forward(x, gamma, b, ln_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = layernorm_forward(x, gamma, beta, ln_param)\n",
    "dx, dgamma, dbeta = layernorm_backward(dout, cache)\n",
    "\n",
    "#You should expect to see relative errors between 1e-12 and 1e-8\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization and batch size\n",
    "\n",
    "We will now run the previous batch size experiment with layer normalization instead of batch normalization. Compared to the previous experiment, you should see a markedly smaller influence of batch size on the training history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_solvers_bsize, solver_bsize, batch_sizes = run_batchsize_experiments('layernorm')\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_training_history('Training accuracy (Layer Normalization)','Epoch', solver_bsize, ln_solvers_bsize, \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_training_history('Validation accuracy (Layer Normalization)','Epoch', solver_bsize, ln_solvers_bsize, \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Inline Question 4:\n",
    "When is layer normalization likely to not work well, and why?\n",
    "\n",
    "1. Using it in a very deep network\n",
    "2. Having a very small dimension of features\n",
    "3. Having a high regularization term\n",
    "\n",
    "\n",
    "## Answer:\n",
    "[FILL THIS IN]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
